% !TeX spellcheck = en_US
\documentclass[11pt, fleqn, titlepage]{article}
%\usepackage{siunitx}
\usepackage{texfiles/SpeedyGonzales}
\usepackage{texfiles/MediocreMike}
\newcommand{\so}[2]{{#1}\mathrm{e}{#2}}
% \geometry{top=1cm}
%\usepackage{cleveref}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{array}
\usepackage{amsmath}
\usepackage{ragged2e}
\usepackage{booktabs}
\usepackage{lipsum}
\usepackage{csquotes}
\usepackage{longtable}
\usepackage{arydshln}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{pxfonts}
\usepackage{array}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{color}
\definecolor{green}{rgb}{0.67, 0.88, 0.69}
\definecolor{red}{rgb}{1.0, 0.44, 0.37}
\newcommand\MyBoxgreen[2]{
	\fcolorbox{black}{green}{\lower1cm
		\vbox to 2.4cm{\vfil
			\hbox to 2.4cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
			\vfil}%
	}%
}
\newcommand\MyBoxred[2]{
	\fcolorbox{black}{red}{\lower1cm
		\vbox to 2.4cm{\vfil
			\hbox to 2.4cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
			\vfil}%
	}%
}

\captionsetup{labelfont=bf}
\def\fix{\hskip\fontdimen2\font\relax}
\lstset{
	basicstyle=\ttfamily,
	keywordstyle=\bfseries,
	showstringspaces=false,
	morekeywords={def, for, return, if, else},
	xleftmargin=-3.0cm
}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
}
\usepackage{nccmath}
\usepackage{multicol}
%\usepackage{subfig}
\usepackage{graphicx}
%\usepackage{algorithmic}
%\usepackage{program}
\title{	Discrimination of Race \& Gender in the COMPAS Dataset}
\author{Oskar Eiler Wiese Christensen \& Anders Henriksen \\ \texttt{\{s183917, s183904\}@student.dtu.dk}}
\date{\today}

\pagestyle{plain}
\fancyhf{}
\rfoot{Page \thepage{} of \pageref{LastPage}}

\graphicspath{{Billeder/}}

\begin{document}
	%: Nævn GitHub linket til vores repo endnu tidligere
	
	\maketitle
	\begin{abstract}
	%TODO: Måske gøre abstractet endnu kortere
	\small
	\noindent
	Unfair treatment is deeply ingrained in many cultures and awareness of this is only just now starting to rise up. After the display of rage against the system and oppression seen in America during the spring and summer of 2020, it is obvious that discrimination has no place in a first world country. To that same extent, machine learning models that have been trained on biased datasets based on people's actions also need to have the discrimination corrected to avoid generally unfair treatment. One significant case is that of the COMPAS dataset and algorithm by Northpointe Inc., which has been commercialized to determine the recidivism risk scores (chance to re-offend) of defendants in the court of law. This algorithm was scrutinized by ProPublica for having a significant bias of applying higher risk scores to African-Americans than to Caucasians. As such, Northpointe has gotten away with using a biased algorithm in a highly important setting. To find and eliminate the bias in the COMPAS dataset for this project, a feed forward neural network was implemented to predict the recidivism score. Bias in the dataset was proven by separating the protected groups of the data and analyzing whether their false positive and true positive rates are equal and plotting the ROC-curves of each group of different classification thresholds of the model. The project finds that the model has inherited three biases, one of which is discussed and found by ProPublicas analysis, which is the racial bias against African-Americans. Furthermore, a sexist and age bias towards men and young individuals is also proven in this project. Equal opportunity and equalized odds from Hardt et al.'s paper was used to correct for any such bias found in the model \cite{equal_of_oppor}. Regarding the bias, it can be seen that Arfican-Americans are twice as often given an undeserved higher risk assessment and Caucasians are twice as often given an undeserved lower assenssment. Surprisingly, almost the same result can be seen for gender and age, where males and young individuals are the disadvantaged groups, which was never addressed in the ProPublica analysis. Regarding age, defendants under 25 are shown to get an undeserving lower score almost twice as often as those over 25, while the higher score remains similar. As such, race, gender and age are all discriminated against in the COMPAS dataset. 
	To conclude, the COMPAS data and algorithm was more biased than first hinted at by ProPublica, as race, sex and age have been discriminated against to a huge degree, both in the data and in the classifier. The bias was corrected using equal opportunity and lead with it a surprisingly low dwindling of accuracy, which shows that not correcting for bias is nonsensical. Bias correction should be enforced and used in the entire field of AI to ensure fairness and general application of ML models. More studies on the subject need to surface, and enforcement needs to be applied in the field to make sure that bias has in fact been removed or attempted to be removed. If the future of AI and ML keeps going like it has so far, bias will never be truly removed, and the ugly roots of racism and sexism found in many cultures will resurface. As seen all over the world as of the summer of 2020, discrimination will not be tolerated and unenforced models with poor or no bias correction will result in response from society, peaceful or otherwise.
	
%	UDKAST 2
%	Unfair treatment is deeply ingrained in many cultures and awareness of this is only just now starting to rise up. After the display of rage against the system and oppression seen in America during the spring and summer of 2020, it is obvious that discrimination has no place in a first world country. To that same extent, machine learning models that have been trained on biased datasets based on people's actions also need to have the discrimination corrected to avoid generally unfair treatment. One significant case is that of the COMPAS dataset and algorithm by Northpointe Inc., which has been used commercially to determine the recidivism risk scores (chance to re-offend) of defendants in the court of law. This algorithm was scrutinized by ProPublica for having a significant bias of applying higher risk scores to African-Americans than to Caucasians. As such, Northpointe has gotten away with using a biased algorithm in a highly important setting. To find and eliminate the bias in the COMPAS dataset, a feed forward neural network was implemented to predict the recidivism score. The classifier obtains an accuracy of $ 68.04 \% \pm 0.015 \% $. Plots of the data are used to estimate the existence of bias, but generally, bias is proven by separating the protected groups of the data and analyzing whether their false positive and true positive rates are equal and plotting the ROC-curves of each group of different classification thresholds of the model. The project finds that the model has inherited two biases, one of which is discussed and found by ProPublicas analysis, which is the racial bias against African-Americans. Furthermore, a sexist bias towards men is also proven in this project. Equal opportunity and equalized odds from Hardt et al.'s paper will be used to correct for any such bias found in the model \cite{equal_of_oppor}. Regarding the bias, it can be seen that Arfican-Americans are twice as often given an undeserved higher risk assessment and Caucasians are twice as often given an undeserved lower assenssment. Surprisingly, almost the same result can be seen for gender, where males are the disadvantaged group, which was never addressed in the ProPublica analysis. Regarding gender, defendants under 25 are shown to get an undeserving lower score almost twice as often as those over 25, while the higher score remains similar. As such, race gender and age are all discriminated against in the COMPAS dataset. When correcting bias using equal opportunity, the accuracy falls to around 67.5\% where the threshold for the protected groups are $ T_{African-American}=0.51  $ and $ T_{Caucasian}=0.56 $. When conditioning on gender the thresholds are $ T_{Male} = 0.45 $ and $ T_{Female}=0.68 $. Equalized odds gives a slightly lower accuracy 67\% and yields two different thresholds, one for the lower ROC-curve and a randomized threshold for the upper one, which makes the model unbiased. The threshold for African-Americans is, $ T_{\text{African-Americans}} = 0.47 $. As for Caucasian, the predictor has a randomized threshold, $ T_{\text{Caucasian}} $. The threshold assumes the value $\underline t_{\text{Caucasian}} = 1 $  with a probability $ \underline p_{\text{Caucasian}}= 0.76 $ and the value $ \bar t_{\text{Caucasian}} = 0.50 $ with the probability $\bar p_{\text{Caucasian}} = 0.24 $. For gender the threshold $T_{\text{Female}}$ is 0.45 and the male threshold is $\bar t_{\text{male}} = 0.61$ with probability $\bar p_{\text{Male}} = 0.09$ and $,\underline t_{\text{Male}} = 1.00$ with probability $\underline p_{\text{Male}} = 0.91$.
%	To conclude, the COMPAS data and algorithm was more biased than first hinted at by ProPublica, as race, sex and age have been discriminated against to a huge degree, both in the data and in the classifier. The bias was corrected using equal opportunity and lead with it a surprisingly low dwindling of accuracy, which shows that not correcting for bias is nonsensical. Bias correction should be enforced and used in the entire field of AI to ensure fairness and general application of ML models. More studies on the subject needs to surface, and enforcement needs to be applied in the field to make sure that bias has in fact been removed or attempted to be removed. If the future of AI and ML keeps going like it has so far, bias will never be truly removed, and the ugly roots of racism and sexism found in many cultures will resurface. As seen all over the world as of the summer of 2020, discrimination will not be tolerated and unenforced models with poor or no bias correction will result in response from society, peaceful or otherwise.
%	
	%UDKAST 1 
%		Unfair treatment is deeply ingrained in many cultures and awareness of this is only just now starting to rise up. After the display of rage against the system and oppression seen in America during the spring and summer of 2020, it is obvious that discrimination has no place in a first world country. To that same extent, machine learning models that have been trained on biased datasets based on people's actions also need to have the discrimination corrected to avoid generally unfair treatment. The discrimination could even be anything from treating the younger generation differently to genuine racist or sexist behavior, so care needs to be taken that all kinds of bias are considered. One significant case is that of the COMAS dataset and algorithm by Northpointe Inc., which has been used commercially to determine the recidivism risk scores (chance to re-offend) of defendants in the court of law. This algorithm was scrutinized by ProPublica for having a significant bias of applying higher risk scores to African-Americans than to Caucasians. As such, Northpointe has gotten away with using a biased algorithm in a highly important setting, so energy needs to be put into removing and correcting bias, which is what this paper will focus on. Meanwhile, this paper will take into account how to spot bias in data, how to quantify bias of an algorithm, using methods to correct bias and performing an ethical discussion of the need of bias correction generally in society.
%	
%	To find the and eliminate the bias, a classification model will first have to be implemented on the COMAS dataset, so a feed-forward neural network will be constructed and permutation tests as well as standard deviation of the optained accuracy from the model will be used to show significance of the result of this model. Plots of the data are used to estimate the existence of bias, but generally, bias is proven by separating the protected groups of the data and analyzing whether their false positive and true positive rates are equal and plotting the ROC-curves of each group of different classification thresholds of the model. In the case where the curves are not similar, the model is biased and bias correction will need to be implemented. In this project, equal opportunity and equalized odds from Hardt et al.'s paper will be used to correct for any bias found in the model \cite{equal_of_oppor}. It will also be shown how the results can be reproduced in the future.
%	
%	After analyzing the most important and often discriminated against variables of the dataset, it was realized that more than just a racial bias exists in the dataset, so the results of this report shows results from bias identification and correction on age, gender and race. From the results, it can be seen that the neural network gets a validation accuracy of around 68\% with a very low standard deviation. Meanwhile, the permutation test shows that no permuted accuracy is even close to the classification accuracy, so the results are significant. Regarding the bias, it can be seen that Arfican-Americans are twice as often given an undeserved higher risk assessment and Caucasians are twice as often given an undeserved lower assenssment. Surprisingly, almost the same result can be seen for gender, where males are the disadvantaged group, which was never addressed in the ProPublica analysis. Regarding gender, defendants under 25 are shown to get an undeserving lower score almost twice as often as those over 25, while the higher score remains similar. As such, race gender and age are all discriminated against in the COMAS dataset, which the ROC-curves of each of these groups also supports when comparing the false and true positive rate dependency on threshold. When correcting bias using equal opportunity, the accuracy falls to around 67.5\% and thresholds for the new classifiers can be found. Equalized odds gives a slightly lower accuracy 67\% and yields three different thresholds, one for the lower ROC-curve and two for the upper one, which makes the model unbiased.
%	
%	To conclude, the COMPAS data and algorithm was more biased than first hinted at by ProPublica, as race, sex and age have been discriminated against to a huge degree, both in the data and in the classifier. The bias was corrected using equal opportunity and lead with it a surprisingly low dwindling of accuracy, which shows that not correcting for bias is nonsensical. Bias correction should be enforced and used in the entire field of AI to ensure the best fairness and general application of ML models. More studies on the subject need to surface, and enforcement needs to be applied in the field to make sure that bias has in fact been removed or attempted to be removed. If the future of AI and ML keeps going like it has so far, bias will never be truly removed, and the ugly roots of racism and sexism found in many cultures will resurface. As seen all over the world as of the summer of 2020, discrimination will not be tolerated and unenforced models with poor or no bias correction will result in response from society, peaceful or otherwise.
		
	\end{abstract}
{	
	\hypersetup{linkcolor=black}
	\tableofcontents \newpage
}
	
	\section{Introduction} \label{indledning}
	
	\subsection{Motivation}
	%TODO et frækkere navn til motivationen?
	%	Artifical intelligence (AI) and machine learning (ML) methods are playing a bigger and bigger role in modern society. As the accuracies of AI and ML models increase, their applications become wider, allowing for these  models to be implemented either as ground truth or as a pointer in applications like autonomous vehicles, medical imaging, the American judicial system ect. These models are, for the general public, often seen as an objective decision maker.
	
	Riots, arson attacks, pepper sprays and tear gas have become the harsh reality for many during the summer of 2020. Not just in America but across the entire globe, people are standing up against the outright discrimination and oppression of African-Americans \cite{tv2, cnn1, cnn2, euro, guardian}. Enforcing change in societal habits, norms and rules can help to reduce or remove the bias of treating African-Americans unfairly. Bias also needs to be taken care of in artificial intelligence (AI) and machine learning (ML) models, as these inherit the bias seen in data built up from human-made decisions. As such, it becomes essential to avoid discrimination, since model discrimination could lead to reinforced societal discrimination. Bias in prediction models is most often corrected for by removing bias from the model pre- or post-training, thereby removing the risk of discrimination when using the model for classification tasks. This report aims to understand bias and how it affects datasets as well as classification models. Furthermore, bias correction methods will be put to use in order to explore the possibilites of keeping discrimination away from important fields. More specifically, the questions to be answered throughout the report are whether there is a bias in Northpointe's COMPAS recidivism algorithm and ProPublica's \texttt{compas-scores-two-years} dataset from their \href{https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv}{GitHub} page and which kind of bias there is, as well as what this bias means in the dataset. Meanwhile, it is also analyzed how bias affects a binary classification feed-forward neural network algorithm, how bias in an algorithm is detected and how to quantify the bias. Lastly, the bias correction methods equal opportunity and equalized odds from Hardt et al. \cite{equal_of_oppor} will be implemented and tested and an ethical discussion will be carried out to understand the applications of bias correction algorithms and AI models on society as well as how society can learn to trust these models. \\
	
	\noindent In this report, the rest of section \ref{indledning} will focus on notions of fairness, related work within the field of bias correction and the contributions from this report. Afterwards, section \ref{data} will present the necessary details of the COMPAS dataset including visualizations of possibly discriminatory variables like "race" and "sex", explanations of the variables of the dataset and how the data was collected and put together. Section \ref{methods} on methods sets up the necessary background knowledge for the topics of FFNNs, k-fold validation, permutation tests, bias identification and bias correction methods. The section will also contain a description of how to reproduce the results. Results will be illustrated in section \ref{results} through figures and tables, which will then be discussed and put into a greater perspective in section \ref{discussion}. The discussion also contains the ethical discussion of bias and bias correction as a whole. The report will be summed up in section \ref{conclusion}. Some lesser relevant figures as well as pseudo code for some of the used functions have been left out of the report, but can be found in section \ref{appendix} and the relevant sources are found in section \ref{bibliography}.
	
	\subsection{Equal Opportunity and Equalized Odds}\label{bias_def}
	%TODO: Give en intuitiv forklaring på betydning af equal opportunity og equalized odds.
	The two main fairness definitions used in this project are proposed by Hardt et. al \cite{equal_of_oppor}. The two definitions are equalized odds and equal opportunity. These are both defined below. Our explanation here will follow the exposition of Hardt et al. and the same notation will be used to ensure ease of understanding.   \\ \newline \textbf{Definition 1.1} (Equalized Odds)\textbf{:} A given predictor $ \hat Y $ satisfies equalized odds with respect to the protected attribute A and outcome $ Y $, if $ \hat Y $ and A are independent conditional on Y. The protected attribute for this project can assume the values 0 and 1, thus making it a binary attribute. The protected attributes for this project is: gender, race and age. If the target is binary, then equalized odds can be expressed mathematically as
	\begin{equation*}\label{key}
	\operatorname{Pr}\{\widehat{Y}=1 | A=0, Y=y\}=\operatorname{Pr}\{\hat{Y}=1 | A=1, Y=y\}, \quad y \in\{0,1\},
	\end{equation*}
	where $\operatorname{Pr}$ is the probability. Equalized odds states that if $ \hat Y $ has to be a fair predictor, then if $ y = 1 $, the predictor has to have equal true positive rates for both classes of A, i.e. $ A = 0 $  and $ A = 1 $. However, if $ y = 0 $ then the predictor $ \hat Y $ has to have equal false positive rates. Equalized odds ensures equal accuracy and bias in all classes, and punishes models that perform well only on the majority.
	\\\\
	\textbf{Definition 1.2} (Equal Opportunity)\textbf{:} A binary predictor $ \hat Y $ satisfies equal opportunity with respect to A if 
	\begin{equation*}
		\operatorname{Pr}\{\hat{Y}=1 | A=0, Y=1\}=\operatorname{Pr}\{\hat{Y}=1 | A=1, Y=1\}.
	\end{equation*}
	Equal opportunity ensures that the probability of predicting $ \hat Y = 1 $ is equal across the classes given the fact that $ Y = 1 $. Equal opportunity is a weaker notion of non-discrimination than Equalized Odds, but still allows for better utility than implementing no bias correction method at all. Equal opportunity follows from equalized odds.
	
	
	\subsection{Related Work}
	There are currently two main ways to ensure fairness in classification. One of the ways is by implementing steps during the training process of the classifier in order to ensure fairness definitions. The second method, which will be implemented in this project, involves post-processing steps. Post-processing methods are often desired because the method can be used on models that have been trained and are commercialized. With the vast increase of machine learning models in commercial use, society calls for ways to ensure that these models do not discriminate any minority or protected group. Many different suggestions of how to implement bias correction algorithms within the field of fair AI have been proposed in recent years. However, this is still a young field of science with many improvements to come.\\\\
	\noindent
	An approach proposed by Zafar et al. \cite{Zafar}, suggests that a way to achieve fair classifiers is by using linear constraints on the covariance between predicted labels and the value of features. This is a method that is implemented during the training process, which means that trained algorithms would have to be retrained. "Satisfying Real-world Goals with Dataset Constraints" uses the same strategy to remove bias inheritance from the dataset. The study proposes constraints in the training data, and by using the ramp penalty to quantify cost accurately, they succeed in developing an efficient algorithm to optimize the resulting non-convex constrained optimization problem. This state-of-the-art algorithm can be implemented in situations where one may require a classifier to make predictions with a certain rate of positive predictions in order to maintain fairness for a population. Equality of opportunity is one of such definitions which require the true positive rates of protected attributes to be equal \cite{g_goh}. \\
	
	\noindent The main method that will be demonstrated in this project is proposed by Hardt et al \cite{equal_of_oppor}. The suggested method can achieve a non-discriminatory classifier by a post-processing step. The main idea is to learn thresholds in order to deal with sensitive features or groups in the data in a discriminant classifier. The method requires that the classifier has information about the data at decision-time, which is the case in the experiment conducted in this project. The study proposes that with the learned thresholds, they can ensure fairness definitions (see section \ref{bias_def}) by using different thresholds for certain protected groups. However, another article, "Learning Non-Discriminatory Predictors", argues that by only using the post-hoc correction (post-training methods), the algorithm will still be unfair \cite{b_woodworth}. They show that for several loss functions such as the 0-1 or hinge loss, the method proposed by Hardt et al. \cite{equal_of_oppor} can fail. In the paper, a notion is defined to obtain an approximation of a non-discrimination algorithm. However, this notion is explored during the paper, and it turns out that creating a non-discrimination algorithm is computationally hard. Therefore, a relaxation definition of equalized odds is presented, which is based on a second-moment condition instead of full conditional independence. Throughout the paper, it is shown that under this condition it is possible to learn a nearly optimal non-discriminatory linear predictor with respect to a convex loss, without it being too computationally inefficient to train \cite{b_woodworth}.\\\\
	The scope of this project is to implement the post-hoc correction method presented by Hardt et al. on a binary classifier trained on the COMPAS dataset. The goal is to obtain a non-discriminatory classifier which still retains a respected accuracy. The motivation behind implementing the work of Moritz Hardt is due to his respectable success within the field of fairness in machine learning. Hardt has more than 8000 citations, is currently writing a book about fairness in ML and has several scientific studies, including "Equality of Opportunity in Supervised Learning". Moreover, the fairness definition of equalized odds and equality of opportunity are currently well accepted fairness definitions. Thus, the benefits and trade-offs of using Hardt et al.'s definitions will be discussed and examined thoroughly.
	
	\subsection{Contributions}
	%TODO: Er et contrubitions afsnit for meta?
	This report aims to give an approachable account of the effects of bias as well as the most promising methods of removing bias and avoiding discrimination from learned supervised learning models in datasets. As such, the reader can expect to get the following from this report:
	
	\begin{itemize}
		\item Illustrations which show an overview of the possible bias present in the modified COMPAS recidivism dataset. The effect of how this possibly biased data affects a classifier will be examined in this report. 
		\item A comprehensive table of all variables present in the modified COMPAS recidivism dataset will be provided. This table includes the name of the variable, a short description of the purpose of the variable as well as the type of variable in order to remove ambiguity of the definitions of the variables which was previously present.
		\item Two bias correction methods, equalized odds and equal opportunity, will be presented, both through text and mathematical definition, and implemented in order to show the effect of the bias corrections on a binary neural network classifier.
		\item To put the other contributions into a real-world context, an ethical discussion of the quantified bias and discrimination models will be carried out.
		\item Pseudo code for the implementation of equal opportunity and equalized odds as well as a section on reproducibility will be provided in the interest of making the results of this paper both as replicable and as reliable as possible. The section on reproducibility supplies the reader with the used seeds, packages, versions of these packages and a thorough explanation of the code used in the present project.
	\end{itemize}
	The general goal of the contributions given above is to avoid models that discriminate with respect to some protected attribute and to make these models reproducible for others in the future. In this report, race, age and gender plays the biggest role as protected attributes, but the findings of the discriminating models and bias correction can equally be transferred to other variables that are suspected to be discriminated. This could be variables such as nationality, income, sexual orientation etc.
	
	\section{Data} \label{data}
	
	\noindent To analyse the efficacy of using bias correction to remove discrimination among race, gender and age in the American justice system, the modified COMPAS recidivism dataset from ProPublica has been used. An important note on the dataset is that the output variable \texttt{score\_text} has been turned into a binary vector for this project to facilitate the use of equal opportunity and equalized odds. This method was also used by ProPublica during their analysis. Meanwhile, making the classifier's output one dimensional and thresholding at values to create predictions allows for equal opportunity and equalized odds to be implemented on the classifier, so turning the output into a binary vector was logical. The binarization was performed by merging the \texttt{Medium} and \texttt{High} risk of recidivism together into zeros and the \texttt{Low} risk of recidivism into ones. Furthermore, only the races African-American and Caucasian have been used in the analysis, see that there is not enough data to construct any meaningful analysis of other races, which is also shown by figure \ref{fig:races}.
	
	A general overview of the dataset and how it came to be, as well as the variables used for the classifier, is given in section \ref{dataDescription}. The data and variables have not been properly explained to any extent in examined literature, so section \ref{dataExamination} will cover the meaning of all 53 variables of the dataset to remove ambiguity and set a common ground from which to base future analyses. Lastly, section \ref{dataVisuals} will show important visualizations of features of the dataset to cover which of the variables are most likely to contain biases that will be explored and examined further in later sections. The ethicality of ProPublica's way of storing data is discussed in \ref{ethicality_of_data} and Supplementary plots of the data variables are also shown in section \ref{dist_categorical} \& \ref{dist_numerical}.
	
	\subsection{Description of Data} \label{dataDescription}
	%TODO: Skal vi fjerne det om class balancing her? Det giver en god anledning til at tale om plots i appendix.
	The data used in this project stems from an initial analysis of the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm by its developers, Northpointe Inc. After this analysis, ProPublica made a subsequent analysis of this data while adding their own queries of the offenders involved and data of the offenders who actually recidivated in order to examine possible discrimination in Northpointe's algorithm. This data is stored in the \texttt{compas-scores-two-year.csv} dataset from ProPublica's GitHub page, which can be found here: \url{https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv}. \\\\
	\noindent The data consists of 6150 rows, each of which is a unique defendant, and 53 different variables, 10 of which are used in the binary classification model. Four of the chosen variables are categorical, so these will have to be one-out-of-k encoded in order to convert them into a proper comprehensible format for the neural network. Class balancing methods could also have been used based on the illustrations of section \ref{dist_categorical} to ensure that the classifier will not have higher success on the larger class, though this was not implemented in this project. Meanwhile, the numerical variables, of which there are also four, will be normalized as to avoid the vanishing gradient and exploding gradient problem and to avoid having some variables be of more importance to the final prediction. Figure \ref{fig:boxplotnumericals} and \ref{fig:histogramnumerical} show both that this is necessary and that the sizes of the numerical variables vary quite wildly.
	The 10 chosen variables, of which one, \texttt{score\_text}, will be used as training ground truth and another, \texttt{is\_recid}, will be used as testing ground truth, are shown and explained below. To see an exhaustive explanation of all variables from the full dataset, see section \ref{dataExamination}.
	
	
	\begin{table}[H]\label{variable}
		\centering
		\begin{tabular}{l l l}
			\toprule
			\textbf{Variable} & \textbf{Description} & \textbf{Type} \\ \midrule
			age & The age of the offenders & Continuous ratio \\
			priors\_count & The number of previous offences & Discrete interval \\
			juv\_fel\_count & The number of previous juvenile felonies & Discrete interval \\
			juv\_misd\_count & The number of previous juvenile misdemeanor & Discrete interval \\
			c\_charge\_degree & The severity of the offence & Discrete nominal \\
			race & The race of the offender & Discrete nominal \\
			age\_cat & The age category of the offender & Discrete nominal \\
			sex & The sex of the offender & Discrete nominal \\
			\hdashline
			score\_text & The COMPAS prediction of chance of recidivism & Discrete interval \\
			is\_recid & Whether the offender truly recidivated after two years (0/1) & Discrete nominal \\ \bottomrule
		\end{tabular}
	\end{table} \noindent
	A vast majority of variables have not been included in the dataset for the binary classifier because a plethora of variables contain dates or categorical values, that are not often shared between defendants, so to one-of-of-k encode these values, a really large data matrix would be necessary, which would increase the running time significantly. Thus, only the variables that seem to be significant in order to predict the risk of recidivism, like age, race, sex, number of previous felonies ect., have been added to the model. The preliminary runs of the model also show that the other variables do not significantly enhance the accuracy of the binary classifier.
		
		
	\subsection{Explanation of Data Variables} \label{dataExamination}
	Using and researching the extended COMPAS dataset has shown that there is no documented meaning of each variable. This makes interpretation difficult and can make singular variables of the dataset next to impossible to understand. To remove ambiguity about the meaning of variables, an exhaustive description of every variable and variable type has been produced. This allows for the results of this report to be contained within this interpretation and sheds light on the true contents of the dataset. Some variables, mostly those with \textit{c\_}, \textit{r\_} or \textit{vr\_} prefixes,have been grouped together in the table, as these serve the same purpose in the dataset for \textit{custody}, \textit{recidivism} and \textit{violent recidivism} cases respectively. 
	
	\noindent Some variable names have the prefix \textit{v\_}, which has been disregarded as an error. As such, this interpretation of the dataset assumes that these variables are akin to those with the \textit{vr\_} prefix, representing the variables related to violent recidivism. Furthermore, in the case of a few of the variables in the dataset, it has simply not been possible to find a meaningful interpretation, since the entire column is either full of NaN's like in the case of \textit{violent\_recid} or every value of a variables matches one to one with another variable, which is the case between \textit{decile\_score} and \textit{decile\_score\_1} as well as \textit{priors\_count} and \textit{priors\_count\_2}. These variables are kept in the table for completeness, but seem to have no discernible purpose in the dataset.
	\newpage
	\setlength\LTleft{-9mm}
	\begin{longtable}{l l l} \label{allVars}
		\textbf{Variable} & \textbf{Description} & \textbf{Type} \\ \hline
		id & Index of the column & Discrete interval \\
		name & Full name of the offender & Discrete nominal \\
		first & First name of the offender & Discrete nominal \\
		last & Last name of the offender & Discrete nominal \\
		compas\_screening\_date & The date the COMAS score was given & Discrete nominal \\
		sex & The sex of the offender & Discrete nominal \\
		dob & The offender's date of birth & Discrete nominal \\
		age & The offender's age & Continuous ratio \\
		age\_cat & Which age category the offender belongs to & Discrete nominal \\
		race & The offender's race & Discrete nominal \\
		juv\_fel\_count & The number of previous juvenile felonies & Discrete interval \\
		juv\_misd\_count & The number of previous juvenile misdemeanor & Discrete interval \\
		juv\_other\_count & The other types of previous juvenile crimes & Discrete interval \\
		days\_b\_screening\_arrest & ----- & Discrete interval \\ \hdashline
		priors\_count & The number of previous offences & Discrete interval \\
		priors\_count\_2 & & \\ \hdashline
		c\_jail\_in & The date the offender was jailed & Discrete nominal \\
		r\_jail\_in & & \\ \hdashline
		c\_jail\_out & The date the offender was removed from jail & Discrete nominal \\
		r\_jail\_out & & \\ \hdashline
		c\_case\_number & The offense case number & Discrete nominal \\
		r\_case\_number & & \\
		vr\_case\_number & & \\ \hdashline
		c\_offense\_date & The date the offense was performed & Discrete nominal \\
		r\_offense\_date & & \\
		vr\_offense\_date & & \\ \hdashline
		c\_charge\_degree & The severity of the offense & Discrete nominal \\
		r\_charge\_degree & & \\
		vr\_charge\_degree & & \\ \hdashline
		c\_charge\_desc & The offense that was performed & Discrete nominal \\
		r\_charge\_desc & & \\
		vr\_charge\_desc & & \\ \hdashline
		c\_arrest\_date & The date the arrest was performed & Discrete nominal \\
		c\_days\_from\_compas & ------ & Discrete interval \\
		r\_days\_from\_arrest & Recidivism days until arrested for the crime & Discrete interval \\
		violent\_recid & Completely empty data column & NaN \\ \hdashline
		is\_recid & Whether the offender truly recidivated after two years (0/1) & Discrete nominal \\
		is\_violent\_recid & & \\
		two\_year\_recid & & \\ \hdashline
		type\_of\_assessment & What the COMPAS algorithm predicted for the offender & Discrete nominal \\
		v\_type\_of\_assessment & & \\ \hdashline
		decile\_score & Value from 1-10 representing risk of recidivism & Discrete ordinal \\
		decile\_score\_1 & & \\
		v\_decile\_score & & \\ \hdashline
		score\_text & decile score split into three categories & Discrete interval \\
		v\_score\_text & & \\ \hdashline
		screening\_date & When the offender was given the assessment & Discrete nominal \\
		v\_screening\_date & & \\ \hdashline
		in\_custody & When the offender was placed in custody & Discrete nominal \\
		out\_custody & When the offender was removed from custody & Discrete nominal \\
		start & The day the screening started compared to the start of the process & Discrete interval \\
		end & The day the screening ended compared to the start of the process & Discrete interval \\
		event & ------ & Discrete interval \\
	\end{longtable}
		
	\subsection{Visualization of Data} \label{dataVisuals}
	To better get an understanding of the data, a range of plots are shown below. Figure \ref{fig:predictedrecidrace}, \ref{fig:predictedrecidsex} and \ref{fig:predictedrecidage} aims to show whether there is a difference between the fraction of the protected group who are predicted by the COMPAS classifier as having low, medium and high risk of recidivism. The protected groups are race, sex and age. Figure \ref{fig:race_truerecid}, \ref{fig:sex_truerecid}, \ref{fig:age_truerecid} as well as \ref{fig:proirs}, \ref{fig:proirssex} and \ref{fig:proirsage} give some insight into the actual amount of crime done by each of the protected groups and whether the amounts seem to be similar. For plots of the categorical and numerical variables chosen for the focus of this report, see section \ref{dist_categorical} and \ref{dist_numerical}. \fix As for the binary classification a score of 1 means high/medium risk of recidivism and 0 is low risk of recidivism. 
	
	\begin{figure}[H]
		\centering
		\textbf{\textbf{Figure 1}}\par\medskip
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{"imgs/normalized_recid_race"}
			\begin{table}[H]
				\centering
				\begin{tabular}{|l|l|l|l|}
					\hline
					& Low   & Medium & High  \\ \hline
					Caucasian        & 0.652 & 0.236  & 0.112 \\ \hline
					African-American & 0.412 & 0.311  & 0.277 \\ \hline
				\end{tabular}
			\end{table}
			\caption{Race normalized predicted recidivism risk}\label{fig:predictedrecidrace}
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{"imgs/true_normalized_race"}
			\begin{table}[H]
				\centering
				\begin{tabular}{|l|l|l|}
					\hline
					& 0   & 1  \\ \hline
					Caucasian        & 0.582 & 0.418  \\ \hline
					African-American & 0.449 & 0.551  \\ \hline
				\end{tabular}
			\end{table}	
			\caption{Race normalized true recidivism risk}\label{fig:race_truerecid}
		\end{subfigure}
	\end{figure}

\noindent It is clear from illustration \ref{fig:predictedrecidrace} that there is a large discrepancy between the normalized distribution of Caucasians and African-Americans in the Low, Medium and High groups. From the matrix as well as the illustration itself, it can be seen that Caucasians are 1.5 times as often classified as low risk of recidivism than African-Americans, while African-Americans are about twice as often classified as high risk of recidivism than Caucasians. This could signify that the data is biased towards giving African-Americans a higher risk evaluation but does not serve as complete evidence. 

Illustration \ref{fig:race_truerecid} shows the true recidivism values (0 being no recidivism after two years and 1 being recidivism) for all offenders in the dataset seperated by the race of each offender and normalized by the size of each protected group. It is clear that the amounts of African-Americans and Caucasians that recidivated or did not recidivate based on the proportion matrix and illustration seems to be closer to equal than in figure \ref{fig:predictedrecidrace}. It also seems to be the case that the Caucasians slightly more often do not recidivate, while the African-Americans slightly more often recidivate. This makes it difficult to prove bias in the data based only on the data, as the bias that could exist in figure \ref{fig:predictedrecidrace} is somewhat explained by the true recidivism scores.

	\begin{figure}[H]
		\centering
		\textbf{\textbf{Figure 2}}\par\medskip		
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{"imgs/normalized_recid_sex"}
			\begin{table}[H]
				\centering
				\begin{tabular}{|l|l|l|l|}
					\hline
					& Low   & Medium & High  \\ \hline
					Male      & 0.500 & 0.274  & 0.226 \\ \hline
					Female    & 0.540 & 0.301  & 0.152 \\ \hline
				\end{tabular}
			\end{table}
			\caption{Sex normalized predicted recidivism risk}
			\label{fig:predictedrecidsex}			
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{"imgs/true_normalized_sex"}
			\begin{table}[H]
				\centering
				\begin{tabular}{|l|l|l|}
					\hline
					& 0   & 1  \\ \hline
					Male     & 0.475 & 0.525  \\ \hline
					Female   & 0.611 & 0.389  \\ \hline
				\end{tabular}
			\end{table}
			\caption{Sex normalized true recidivism risk}
			\label{fig:sex_truerecid}			
		\end{subfigure}
	\end{figure}


\noindent Illustration \ref{fig:predictedrecidsex} shows the relationship between sex and the COMPAS prediction of recidivism normalized by the sizes of the protected groups. No outstanding difference in the proportions of men or women being classified as belonging to each of the categories can be seen.\fix From the data alone, it would thus not seem as if any bias is present. This illustration shows the normalized true recidivism values separated by the sex of the offender. It is clear that there is a difference between the fraction of males who recidivate and the fraction of women who recidivate. 

The illustration \ref{fig:sex_truerecid} does not seem to match as well with figure \ref{fig:predictedrecidsex} as figure \ref{fig:race_truerecid} and \ref{fig:predictedrecidrace}, since a very slight amount of the differences in the true figure can be seen in the predicted values. Thus, a bias could be present, but specifically the figure \ref{fig:predictedrecidsex} shows only very slight sign of bias.

	\begin{figure}[H]
	\centering
	\textbf{\textbf{Figure 3}}\par\medskip	
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{"imgs/normalized_recid_age"}
		\begin{table}[H]
			\centering
			\begin{tabular}{|l|l|l|l|}
				\hline
				& Low   & Medium & High  \\ \hline
				Less than 25    & 0.326 & 0.365  & 0.309 \\ \hline
				Greater than 25 & 0.557 & 0.258  & 0.185 \\ \hline
			\end{tabular}
		\end{table}
		\caption{Age normalized predicted recidivism risk}	
		\label{fig:predictedrecidage}		
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{"imgs/true_normalized_age"}
		\begin{table}[H]
			\centering
			\begin{tabular}{|l|l|l|}
				\hline
				& 0   & 1  \\ \hline
				Less than 25      & 0.395 & 0.605  \\ \hline
				Greater than 25   & 0.531 & 0.469  \\ \hline
			\end{tabular}
		\end{table}
		\caption{Age normalized true recidivism risk}		
		\label{fig:age_truerecid}		
	\end{subfigure}
	\end{figure}
	
\noindent As in figure \ref{fig:predictedrecidrace}, it also holds true in illustration \ref{fig:predictedrecidage} that there is a large discrepancy between the normalized distribution defendants greater than and less than 25 years old in the Low, Medium and High groups. From the matrix as well as the illustration itself, it can be seen that defendants greater than 25 years old are 1.7 times as often classified as low risk of recidivism than those under 25 years old, while under 25 year olds are about twice as often classified as high risk of recidivism than over 25 year olds. This could signify that the data is biased towards giving older defendants a higher risk evaluation, but further analysis will have to be made.

Illustration \ref{fig:age_truerecid} shows the normalized true recidivism values for the protected group age. The amounts of defendants over and under 25 years old that recidivated or did not recidivate based on the confusion matrix and illustration are close to equal to the amounts seen in figure \ref{fig:predictedrecidage}. It also seems to be the case that the older group 1.3 times as often did not recidivate (got \texttt{is\_recid} value 0). This makes it difficult to prove bias in the data based only on the data, as the bias that could exist in figure \ref{fig:predictedrecidage} is somewhat explained by the true recidivism scores.

	\begin{figure}[H]
	\centering
	\textbf{\textbf{Figure 4}}\par\medskip
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{"imgs/proirs"}
		\caption{Normalized number of felonies by race}	
		\label{fig:proirs}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{"imgs/proirs_sex"}
		\caption{Normalized number of felonies by sex}		
		\label{fig:proirssex}		
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{"imgs/proirs_age"}
		\caption{Normalized number of felonies by race}		
		\label{fig:proirsage}
	\end{subfigure}
	\end{figure}	

\noindent Illustration \ref{fig:proirs} shows the relation between race (African-American or Caucasian) and the number of previous felonies prior to the study taking place. This plot, like figure \ref{fig:predictedrecidrace} gives an indication that a larger amount of Caucasians have committed a low number of previous felonies and African-Americans have committed a larger number of prior felonies. This is supported by the fact that the proportion of African-American to Caucasians becomes larger as the number of previous felonies becomes larger. This seems to indicate that African-Americans are more frequently engaged in criminal activity, which also weakens the indication of a bias from figure \ref{fig:predictedrecidrace}. Further analysis will still have to be made to check if an actual bias is present.

Illustration \ref{fig:proirssex} illustrates the relation between sex (Female and Male) and the number of previous felonies prior to the study taking place. This plot gives a clear indication that the number of females and males who have performed no prior felonies is very different. Otherwise, it can be seen that the proportion of females who have a low number of felonies previous is in general somewhat the same as that of men. The proportion of men becomes larger as the number of previous felonies becomes larger. This seems to indicate that men are more frequently engaged in criminal activity due to the fact that women seems to often have less felonies than men. Further analysis will still have to be made to check if an actual bias is present.
	
In \ref{fig:proirsage}, the relation between age (less than 25 and over) and the number of previous felonies prior to the study taking place has been shown. This plot gives a clear indication that the proportion of people younger than 25 who have performed no prior felonies is larger than the proportion of people older than 25. Otherwise, it is clear that the proportion of people older than 25 becomes larger as the number of previous felonies becomes larger. This seems to indicate that people over 25 are more frequently engaged in criminal activity, which is a logical deduction since older people have had more time to engage in crime. Further analysis will still have to be made to check if an actual bias is present.
	
	
	
	\subsection{Ethicality of Data}\label{ethicality_of_data}
	The COMPAS dataset as well as ProPublica's expanded data and analysis of this data is freely available from their public GitHub repository. As such, the data is not stored safely. This is also true for the variables of the data such as age, full name and number of prior felonies, which give ample opportunity to identify people who have recidivated, leading to compromise of their personal data. This is not a concern though, as the data is collected from Florida, where public records are subject to the broad legislated public right of inspection. According to chapter 119 section 1 of the law of the State of Florida, 
	\begin{displayquote}
		"It is the policy of this state that all state, county, and municipal records are open for personal inspection and copying by any person. Providing access to public records is a duty of each agency." \cite{floridaLaw}.
	\end{displayquote}
	
	\noindent As such, since ProPublica submitted a public records request for access to the data for use in research \cite{propublicaAnalysis}, which inherently is a type of inspection, that allows them, within the legal ramifications, to use the dataset as they please. Another debate entirely, which will in particular be discussed in section \ref{discussion}, is the more subjective question of the ethicality of open-record laws and public criminal data in their entirety being shared among whomever and what exactly should be allowed to be done with this data.
	
		
	\section{Methods} \label{methods}
	This next section will cover the methods used for this project. To see the implementation of these methods, go to the Jupyter Notebook \texttt{classifier\_notebook.ipynb} on GitHub here: \url{https://github.com/oskarwiese/fagprojekt/tree/master/src}. To get a better understanding of the implementation, the pseudo code in section \ref{pseudo_code} or the section on reproducibility, section \ref{repro}, can be used. Now, an overview of the section will be given. In section \ref{Feed-forward neural}, the basics of a neural network are explained and these basics are applied to the model implemented in this report. Furthermore, an illustration of the model architecture has been given and loss as well as hyperparameters will be looked at. In the next sections, section \ref{standard_deviation} \& \ref{permutation_test}, the methods for testing the significance of the accuracy found from the neural network will be given, and cross validation and permutation test will be used to show that the accuracy of the model is reliable and significantly better than random. The general theory behind confusion matrices and true and false positive rates as well as accuracy will be covered in section \ref{bias_id}. The connection between bias correction and these values will also be made in this section. Afterwards, in section \ref{biascorr}, the bias correction methods equal opportunity and equalized odds will be derived and explained in detail. Our explanation will again follow the exposition of Hardt et Al., which means we will use the same notation and explain the methods in the same order as the original paper "Equality of Opportunity in Supervised Learning \cite{equal_of_oppor}". Lastly, a section about reproducibility, see section \ref{repro}, will uncover the packages as well as their versions used for this report. After this, a thorough walkthrough of the implementation of the most essential code will be made.
	
	\subsection{Binary Classifier}\label{Feed-forward neural}
	%initialization schemes
	\subsubsection{Feed-forward Neural Network}
	Feed-forward neural networks (FFNN) are the simplest kinds of neural networks. The flow of information in a feed-forward neural network is one-directional, which means that the network has an input layer, from which information flows through the hidden layers unto the output layer. The main purpose of a FFNN is to approximate a function. In this project, the function $ \mathbf{y} = f^*(\mathbf{x}) $ maps an input $ \mathbf x $ to a category $ \mathbf y $. The FFNN is thereby a classifier, the goal of which is to predict the the recidivism value (0 or 1) of a defendant given the input $ \mathbf x $. The variable $ \mathbf x $ contains both categorical and continuous variables. The categorical variables that are fed to network are the following: \texttt{["c\_charge\_degree", "race", "age\_cat", "sex"]} and the continuous variables are \texttt{["age", "priors\_count", "juv\_fel\_count", "juv\_misd\_count"]}. Meanwhile, the ground truth variables for training and testing respectively are \texttt{["score\_text", "is\_recid"]}, which are also both categorical. Two different variables were chosen for train and test since the goal of the classifier is to show how well the biased variable \texttt{score\_text} describes the true recidivism values. These 10 variables were chosen from the data to train and test the model, since the other variables of the data contains dates or many unique categories, and would as such make it computationally inefficient to train the network, as one-out-of-k encoding dates takes a lot of rows. The possible values of $ \mathbf y $ are 0 or 1, which corresponds to the classifier classifying a person as either low risk or medium/high risk of recidivism respectively. Hence, this is a binary classifier which constructs a mapping $ \mathbf y = f(\mathbf x \ ; \mathbf w) $. The goal of the FFNN is to learn the weights that most efficiently approximate the function $ f^* $ through supervised learning. The binary classifier in this project has an input layer, five hidden layers and an output layer \cite{dl}.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{imgs/ffnn}
		\caption{Visualization of the binary classifier model. The input layer has dimension $ \mathbb R ^8$ and the output layer has dimension $ \mathbb R^1 $. The number of nodes in the hidden layers are hyperparameters. The number of nodes used in this project are, layer one through five respectively: $ \mathbb R ^{16} $, $ \mathbb R ^{32} $, $ \mathbb R ^{64} $, $ \mathbb R ^{128} $, $ \mathbb R ^{64} $}
		\label{fig:ffnn}
	\end{figure}
		
	\noindent 
	Optimal neural network hyperparamters for highest possible accuracy are often found by expert knowledge or exhaustive searches. Since the scope of this project is to demonstrate biased algorithms, the goal is to construct a classifier with an accuracy vastly better than random, so preliminary runs and grid search have been used to find the best performing hyperparameters, which can be seen in the table below.
	
	\begin{table}[H]
		\centering
		\begin{tabular}{l l l l l l l l l}
			\toprule
			& \textbf{Hyperparameter}           &&&&& & \textbf{Value}    & \\ \midrule
			& Train-test split         &&&&& & $0.8/0.2$& \\
			& Dropout $[p]$            &&&&& & $0.5$    & \\ 
			& Learning rate $[\alpha]$ &&&&& & $0.001$  & \\ 
			& Weight decay             &&&&& & $10^{-6}$& \\ 
			& Threshold                &&&&& & $0.5$    & \\ 
			& Epochs                   &&&&& & $400$    & \\ 
			& Input layer              &&&&& & $8$      & \\ 
			& Hidden layer 1           &&&&& & $16$     & \\ 
			& Hidden layer 2           &&&&& & $32$     & \\ 
			& Hidden layer 3           &&&&& & $64$     & \\ 
			& Hidden layer 4           &&&&& & $128$    & \\ 
			& Hidden layer 5           &&&&& & $64$     & \\ 
			& Output layer             &&&&& & $1$      & \\ \bottomrule
		\end{tabular}
	\end{table}
	\noindent
	The fully connected layers are trained on 80\% of the rows of the COMPAS dataset with a supervised learning strategy and are tested on the last 20\%. Other hyperparameters used are a drop-out value at $p=0.5$, which is used on each layer. The learning rate, $\alpha$, is initialized as $\alpha = 0.001$. However, the optimization algorithm used for this project is Adaptive Moment Estimation (Adam). Adam is a well known optimizer in the literature and has an adaptive learning rate and step size. The activation function used is rectified linear unit (ReLU), which is defined as $ y = \text{max}(0,x) $. Thus, the function returns x for positive values and zero otherwise. Sigmoid is used on the last layer to ensure that the output value is a vector between zero and one. The activation functions also ensure non-linearity, allowing for a more expressive model. Binary cross-entropy loss (BCL) is used as the cost function. The loss in the binary classifier is calculated using the expression below. 
	\begin{equation}\label{key}
	L\left(\boldsymbol{y}_{i}, \hat{\boldsymbol{y}}_{i}\right) = -(\mathbf y \log (\mathbf {\hat y})+(1-\mathbf  y) \log (1-\mathbf {\hat y}))
	\end{equation}
	BCL punishes the algorithm if the predicted value is far from the label. I.e. if the classifier predicts 0.09 and the label is 1, it would result in a very high loss. The ideal model would have a binary cross-entropy loss of zero \cite {dl}. The result is a FFNN with one output node where a threshold can be set to manipulate how the network will classify an input \textbf{x}. This will be elaborated upon further in section \ref{biascorr}.

	\subsection{Standard deviation of the obtained accuracy} \label{standard_deviation}
	To obtain a standard deviation of the model accuracy, ten different folds of the dataset were constructed. This was done by splitting the data into ten equal sized arrays and combining these into ten different train test splits. Below, an illustration of a 10-fold split can be seen: 
	
	%TODO: Søren synes dette er grimt, så måske skal det ændres
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\linewidth]{imgs/kfolds2}
		\caption{Illustrates how the data was split into folds. It is clear from the illustration that the models obtained are trained and tested on different data. On the right, it is shown that the accuracy, E, can be found by the mean of all accuracies from the ten different models. Image found at \href{https://www.researchgate.net/figure/Diagram-of-k-fold-cross-validation-with-k-10-Image-from-Karl-Rosaen-Log_fig1_332370436}{ResearchGate} \cite{researchgate}. }
		\label{fig:kfolds}
	\end{figure}
	\noindent
	This results in ten models with different accuracies and training/test data which allows for a value of the mean and standard deviation to be obtained.
	
	\subsection{Permutation Test}\label{permutation_test}

	When the model is trained on the COMPAS dataset with supervised learning and a validation accuracy has been obtained, it is important to know if the model performs significantly better than random. To examine this, a permutation test was conducted by comparing the NN accuracy to a distribution of random model accuracies. These accuracies are found by permuting the output variables \texttt{score\_text} and \texttt{is\_recid} in the dataset and then training and validating on the new dataset, as this will construct a model that approximately predicts randomly. This is done 500 times. The constructed models have the same dimension of features which will ensure that the training data of these models and the proposed classifier of this project are equal. Thus, allowing one to compare the accuracies of the models. 
	
	For the significance test itself, a null hypothesis $H_{0}$, stating that the NN accuracy comes from the distribution of accuracies from the random model, is set up and evaluated. The 500 accuracies form a distribution and the result from the neural network on the real data is statistically significant if the \textit{p}-value is under the desired level of significance at $ \alpha = 0.05 $, since this allows for the null hypothesis to be rejected. Intuitively, this is equivalent to the accuracy being significantly far away from this distribution. 
	This can be evaluated using the \textit{p}-value. For a permutation test, the \textit{p}-value is calculated using the following expression,
	\begin{equation}\label{pval}
	p=\frac{B+1}{M}
	\end{equation}
	where \textit{B} is the number of extreme permutations (permutations with accuracies greater than or equal to the observed accuracy) and \textit{M} is the total number of permutations \cite{p-value}.
	
%	As for feature selection every feature was randomly shuffled with re-sampling. If the outputs remained unchanged regardless of the permutation the feature was dropped. Every feature is permuted 1000 times.\\\\
	%As for identification of biases in the binary classifier the categorical variable race was used. Permutation was implemented by randomly shuffling the attribute with re-sampling. The permuted data were used as input for the model and several confusion matrices were constructed and compared. The matrices were compared to the original confusion matrix, which in turn results in a \textit{p}-value on the percentage of false negatives (FN) on whites compared to African-Americans in the ProPublica data.
	
	\subsection{Bias Identification}\label{bias_id}
	To identify if the constructed classifier trained on the COMPAS dataset is fair or not, the original dataset was conditioned on each protected group and the confusion matrices of these conditional populations were compared. A confusion matrix contains the amount of predictions which are true positive, false positive, true negative and false negative. These are arbitrarily ordered as,	
	
	\begin{center}
		\bgroup
		\def\arraystretch{2}%  1 is the default, change whatever you need
		\begin{tabular}{c | c c |}
			& Predicted positive & Predicted negative \\ 
			\hline 
			Actual positive & True positive [TP] & False positive [FP] \\
			Actual negative & False negative [FN] & True negative [TN] \\ 
			\hline
		\end{tabular}
		\egroup
	\end{center}
	
%	\begin{figure*}[h]	
%			\begin{ceqn}
%				\begin{equation*}
%				\begin{bmatrix}
%				\text{TP} & \text{FP}  \\
%				\text{FN} & \text{TN} 
%				\end{bmatrix} 
%				\end{equation*} 
%			\end{ceqn}
%	\end{figure*} \noindent
	\noindent
	For this project, the positive group belongs to those who have not recidivated and the negative group is for those who have recidivated. The true positive (TP) amounts to the classifier predicting true and the ground truth being true, while the true negative (TN) is the number of times the classifier predicted false, when the true label was false. True positive and true negative can thus be seen as the correct predictions. The incorrect predictions are false positive and false negative. False positive (FP) amounts to the model predicting true when the ground truth is false. Meanwhile, the false negative (FN) is a false prediction from the model, where the ground truth is true. From the confusion matrix the recall and selectivity also known as true positive rate (TPR) and false negative rate (FPR) can be calculated. Furthermore, the validation accuracy can be determined.
	
	\begin{align}\label{tpr_fpr_acc}
	\text{TPR} = \frac{\text{TP}}{\text{TP}+\text{FN}}\qquad \qquad \qquad \qquad &
	\text{FPR} = \frac{\text{FP}}{\text{FP}+\text{TN}} \qquad &&
	\text{ACC} = \frac{\text{TP}+\text{TN}}{N}
	\end{align} \noindent
	Where N is the total amount of predictions which is given as, 
	\[\text{N} = \text{TP}+\text{FN}+\text{FP}+\text{TN.}\] 
	The TPR and FPR values of the two conditional populations have to be equal in order for the classification to be fair according to equalized odds and just the TPR values have to be equal to satisfy equal opportunity. The split of the data allows for a different ROC-curves to be constructed for each protected group, showing differing values for TPR and FPR, which can be equated. This is a simplification of the process of the bias definitions used for bias correction methods, which have been explained in section \ref{bias_def}. Furthermore, in the next section, \ref{biascorr}, a detailed explanation of how to fulfill these fairness definitions will be given. 
	
	\subsection{Bias Correction Methods}\label{biascorr}
	
	The method that was implemented in this project is introduced by Hardt et al \cite{equal_of_oppor}. The proposed fairness implementations have been implemented after the training process; so-called post-processing bias correction methods. Post-training, two confusion matrices are constructed from the validation datasets described in section \ref{bias_id}. Hardt et al. proposes that in order for the model to be as fair as possible, it has to ensure equalized odds or equal opportunity depending on how much loss of accuracy one can allow in their model. Per definition, equal opportunity follows from equalized odds. A predictor can always fulfill equalized odds by using a constant predictor, i.e. $ \hat Y = 1 $. However, the goal is a to make a sufficient predictor, which ensures non-discriminant behavior. To derive such a predictor, a score function is needed. The output node of the network has a value in $ \hat{R} \in [0,1] $, instead of it being a binary output. Thus, this output value \textit{R}, will be considered as a score function. A way to obtain a binary predictor from this score function \textit{R} is by thresholding the value, where the prediction would be $ \hat Y = \mathbb I (\hat{R} > t) $. If the score function then satisfies equalized odds or equality of opportunity, then the predictor will as well. From this score function, the optimal threshold should be chosen to ensure the strict definition of fairness \ref{bias_def}. If the score function does not satisfy this definition, then a unique threshold needs to be used for each protected group, which can be stated as $ \tilde Y = \mathbb I ( \hat{R} > t_A)$, where A is the protected group. 
	
	A key in Hardt et Al.'s study is the Receiver Operator Characteristic curve (ROC-curve) of the score function. The ROC-curve shows the relationship between the false positive rate and true positive rate at different thresholds. These curves are shown in a two dimensional plane with the true positive rate on the vertical axis the false positive rate on the horizontal axis. The ROC-curves that are considered are the A-conditional ROC-curves which are constructed from the data conditioned on the protected groups respectively. To be more precise, ROC-curves are constructed by thresholding the score function and using the confusion matrices described in section \ref{bias_id} to obtain the true positive and false positive rate, see equation \ref{tpr_fpr_acc}.
	\begin{equation*}\label{key}
	C_{a}(t) \stackrel{\text { def }}{=}(\operatorname{Pr}\{\widehat{R}>t | A=a, Y=0\}, \operatorname{Pr}\{\widehat{R}>t | A=a, Y=1\})
	\end{equation*}
	Here, $C_{a}(t)$ is a function that, when plotted in every threshold, realizes the ROC-curves, $\hat{R}$ is the prediction from the binary classifier, $t$ is the threshold, $A$ is a value of the protected group e.g. Caucasian and $Y$ is the true recidivism value. Meanwhile, $\operatorname{Pr}\{\widehat{R}>t | A=a, Y=0\}$ is the false positive rate and $\operatorname{Pr}\{\widehat{R}>t | A=a, Y=1\}$ is the true positive rate. The conditional curves specify the conditional distributions of both the protected groups. Therefore, a score function obeys equalized odds if and only if the values of the protected attributes are equal. There are several scenarios to take into account, which are shown below \cite{equal_of_oppor}.
	\begin{enumerate}[font=\bfseries]
		\item The simplest case is if the ROC-curves are identical or close to identical while a point on each curve with equal TPRs also have the same thresholds. If this is the case, then any thresholding of R yields an equalized odds predictor. As such, the threshold yielding the best accuracy can be chosen.
		\item In the case where the ROC-curves are identical but two points with equal TPR value on the ROC-curve does not have equal thresholds, a different threshold will have to be chosen for each protected group such that the accuracy of the model is maximized and equalized odds is ensured.
		\item In the case where the ROC-curves are not equal, an intersection could be chosen to fulfill equalized odds, unless the point of the intersection yields a low accuracy or an intersection does not exist at all, except for at the trivial endpoints, which carry with them low accuracy. In this case, randomization has to be used in order to obtain predictors which fulfill the definition of equalized odds. The randomization works by forcing the upper ROC-curve down unto the lower curve in order for the two curves to get the same true and false positive rate.
	\end{enumerate}
	It is also possible for the score function to obey equal opportunity, which is significantly easier to implement. The only necessity for equal opportunity is that the true positive rates should be equal, so every threshold on the ROC-curve is a valid option. Thus, an exhaustive search through every thresholding allows for optimization in order to find the best possible accuracy under the constraint of equal opportunity.
	
	\subsubsection{Derivation of an Optimal Equalized Odds Predictor} \label{optimalboi}
	A convex hull for the conditional ROC-curves is defined as, 
	\begin{equation*}\label{key}
	D_{a} \stackrel{\text { def }}{=} \text { convhull }\left\{C_{a}(t): t \in[0,1]\right\}
	\end{equation*}

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/convex_hull}
		\caption{The figure illustrates the intersection of the convex hulls of the conditional ROC-curves. The intersection is the gray area in the graph between the lower of the ROC-curves and the diagonal. The age-based ROC-curves are used as these best illustrate the convex hulls.}
		\label{fig:convexhull}
	\end{figure}
	\noindent
	A chosen point in the convex hull $ D_a $ represents the false and true positive rates, conditioned on a certain protected group of a randomized derived predictor based on the score function R. A predictor $ \tilde Y $ can always be constructed as a mixture of two different threshold predictors. Conditioned on $ A=a $, the predictor will behave as, 
	\begin{equation}\label{odds_pred}
	\widetilde{Y}=\mathbb{I}\left\{R>T_{a}\right\},
	\end{equation}
	
	\noindent Where the variable $ T_a $ is the randomized threshold which assumes the value $ \underline{t}_{a} $ with a certain probability $ \underline{p}_a $ and the value $\bar t_{a} $ with probability $ \bar p_a $. To obtain an equalized odds predictor, a point in the intersection of the convex hull of each of the conditional ROC-curves has to be chosen and then for each group realize the true and false positive rates with a randomized predictor. For each group this results in either choosing a fixed threshold $ T_a = t_a $ or a mixture of two thresholds $ \underline t_a < t_a $. In the case of mixing two thresholds, if $ A=a $ and $ R < \underline t_a $, the predictor $ \tilde Y $ is set to 0, and if $ R > \bar t_a $, the predictor is set to 1. But if $ \underline t_a < R < \bar t_a $, it is reversed and $ \tilde Y = 1 $ with probability $ \bar p_a$. Phrased in words, the sampled value, which is a random number between 0 and 1, decides whether to threshold at the upper or lower point.
	
	The equalized odds predictors is thus in the intersection of the convex hulls of the A-conditional ROC-curves, and above the main diagonal. This area is illustrated in figure \ref{fig:convexhull}. For any loss function the optimal false and true positive rate will always be on the upper left boundary. This is due to the fact that predictors close to the diagonal are no better than random. Thus, the upper left boundary contains the possible equalized odds predictors with the highest accuracies. This upper left boundary of the intersection between the convex hulls is the point-wise minimum of the two A-conditional ROC-curves. The performance of a predictor that satisfies equalized odds is therefore determined by the minimum performance among the protected groups, which for this project are: Caucasians and African-Americans, males and females, individuals under 25 and over 25 years old. In other words, ensuring equalized odds enforces the trained model to build good predictions for all classes. For a given loss function, finding the optimal threshold can be done by optimizing
	\begin{equation}\label{optim_thresh}
	\min _{\forall a: \gamma \in D_{a}} \gamma_{0} \ell(1,0)+\left(1-\gamma_{1}\right) \ell(0,1),
	\end{equation}
	Assuming without loss of generality $ \ell (0,0) = \ell(1,1)=0 $.
	The optimization problem stated in equation \ref{optim_thresh} can be solved efficiently by numerically using exhaustive search. The result is two predictors for each protected attribute. For the conditional distribution, where the A-condtional ROC-curve has the lowest TPR value of the two A-conditional ROC-curves, the result will be a predictor with some threshold whereas the other conditional population will have a predictor with a randomized threshold, see equation \ref{odds_pred} \cite{equal_of_oppor}.
	\subsubsection{Derivation of an Optimal Equal Opportunity Predictor}\label{optimalboi equal_oppor}
	
	The construction of an equal opportunity predictor follows the same approach as \ref{optimalboi}, but is only constrained on one axis. From the definition of equal opportunity, the predictor must only satisfy that the true positive rates are equal. This corresponds to the A-conditional ROC-curves having the same y-coordinate in the two dimensional plane. Assuming continuity of the A-conditional ROC-curves it is always possible to find points on the boundary of the conditional ROC-curves. This means that no randomization is needed, unlike when deriving an equalized odds predictor. The optimal equality of opportunity predictor corresponds to two different deterministic thresholds for each of the protected groups. The optimization problem can be solved by using exhaustive search over the true positive values \cite{equal_of_oppor}.
	
	\subsection{The ROC-curves of the Original COMPAS Algorithm}
	
	This section focuses solely on how to obtain the ROC-curves from the original COMPAS algorithm. The original COMPAS algorithm has produced a risk assesment score from the COMPAS data, which is present in ProPublica's data as the variable \texttt{decile\_score}. This variable has a numerical value between 1 and 10 which describes how likely the subject is to recidivate (10 being maximum risk of recidivism). By thresholding this variable and comparing it to the variable \texttt{is\_recid}, which is whether the subject truly recidivated after two years, a ROC-curve can be constructed for each protected group, which allows for a peek to be taken into what bias was present when ProPublica made their initial analysis. The thresholds used in this project are the following: 
	\[ [-0.5,  0.5,  1.5, 2.5,  3.5,  4.5, 5.5,  6.5,  7.5,  8.5,  9.5, 10.5] \]
	This will result in 12 points in the $ (FPR,TPR) $-plane, which can then be used to construct the ROC-curve. Furthermore, the TPR and FPR dependency on the thresholds can be examined to quantify if the biases was present in the original analysis.
	
	
	\subsection{Reproducibility}\label{repro}
	This section aims to give the reader a precise insight into how the methods of this project were implemented and how it would be possible to reproduce these results in the future. Hopefully, this will also simplify the methods. Reproducibility is an important part of writing any project or performing any research. It shows significance of the given result if it is possible for others at a later time to replicate or reproduce the results, since it validates the result of the original study and shows that it was not a fluke. All of the code is written in Python Version 3.7.4, PyTorch version 1.4.0 and Jupyter Notebook 6.0.3. Furthermore, the versions of the packages used in the project are given in the table below. All code for the project is accessible at \url{https://github.com/oskarwiese/fagprojekt/blob/master/src/Classifier_notebook.ipynb}.
	\begin{table}[H]
		\begin{center}
			\begin{tabular}{l l l l l l l l l l}
				\toprule
				& \textbf{Package}      & & & & & & & \textbf{Version}  & \\ \midrule
				& Numpy        & & & & & & & $1.16.5$ & \\
				& pandas       & & & & & & & $0.25.2$ & \\
				& matplotlib   & & & & & & & $2.2.5$  & \\
				& seaborn      & & & & & & & $0.9.0$  & \\
				& torch        & & & & & & & $1.2.0$  & \\
				& sklearn      & & & & & & & $1.16.5$ & \\
				& scipy        & & & & & & & $1.3.1$  & \\
				& ipywidgets   & & & & & & & $7.5.1$  & \\ \bottomrule
			\end{tabular}
		\end{center}
	\end{table}
	\noindent Since the implementation of a binary classifier and equalized odds uses random number generation, random seeds have been set in order to ensure equivalent results for every run of the notebook. The seed number was set to 42 and was used for the python seed \texttt{random.seed}, the numpy seed \texttt{np.random.seed} and the pytorch seed \texttt{torch.manual\_seed}.
	
	In order to make the results as reproducible as possible, the data as well as the way the data was obtained also plays a big role. The way the data was obtained can be seen described in detail in section \ref{dataDescription}. The data comes from the GitHub repository \url{https://github.com/propublica/compas-analysis}, from which the dataset \texttt{compas-scores-two-years.csv} was used as the only dataset for the project. \\

	\noindent Below, a thorough walkthrough of the most essential parts of the code will be made. To implement the bias correction methods and classifier, firstly, the dataset was loaded into a Pandas dataframe in order to easily structurize the data. Several plots of this data were then made using the Seaborn plot library. The code for this can be found in the \texttt{Data Visualization} cell. In order to pre-process the data for the implementation of a neural network, all of the features were converted into PyTorch tensors (the chosen features and order of features for this project can be seen in section \ref{Feed-forward neural}). All of the numerical variables were normalized to avoid the exploding gradient problem and to give every variable a level playing field, as the numerical variables have been shows to have varying sizes, see figure \ref{fig:boxplotnumericals} \& \ref{fig:histogramnumerical}. A FFNN was constructed using the PyTorch NN module, see the \texttt{Neural Network} cell in the notebook. The binary classifier was trained using BCE-loss with Adam as the chosen optimizer. All of the hyperparameters can be seen in section \ref{Feed-forward neural}. A permutation test was then performed to show that the acquired accuracy was adequately better than random. The classifier was used to construct predictions for conditional distributions for the protected attributes, which were saved in an array and used to construct confusion matrices at different thresholds. By using different thresholds, two  different conditional ROC-curves were constructed with Numpy and MatPlotLib. A plot of the TPR and FPR for each of the protected groups, as a function of thresholds were constructed as well, the code for which can be seen at the bottom of the \texttt{Classification} function at the top of the notebook. The next goal was to implement equalized odds and equal opportunity using these conditional ROC-curves. The code for this is located in the \texttt{BiasCorrection} function. The implementation of both equal opportunity and equalized odds is explained both by reading the code and by the pseudo code, which has been supplied in the appendix section \ref{Pseudo Equalized} \& \ref{Pseudo Equal}.
	
% Forklaring af implementation af equalized odds
%	For each point on the boundary of the intersection of the convex hulls for each of the conditional distributions, the true postive and false postive rate were found. From the point on the boundary, a line was drawn between this point and the point (0,0). The slope is found using $ m=\frac{\Delta y}{\Delta x} $. Then, the intersection between the upper curve and the line was determined. The distance from (0,0) to the point on the lower curve was found and divided by the length from (0,0) to the point of the upper curve. The distance between the point on the lower curve and the intersection of the line and the upper curve is determined as well and divided by the length from (0,0) to the intersection of the line and the upper curve. This will result in two values, which are referred to as $ \bar p_a $ and $ \underline p_a $ in section \ref{optimalboi}. These are values between zero and one which will sum to one, and decides the proportion of sampling from the randomized predictor. This was done for each point on the upper left boundary of the intersection between the convex hull of the two conditional ROC-curves.

% Forklaring af implementation af equal opportunity
%As for the implementation of Equal Opportunity, it only requires that the TPR is equal for both populations. This was done by using exhaustive search, by choosing a TPR for one population, then subtracting this from the TPR-array for the other population and using \texttt{argmin} to find the one closest to zero.
	
	\section{Results}\label{results}
	The bias identification and bias correction in this section focuses on bias from race, sex and age. This section will focus on the results of the analyses and give an indication of how to read the illustrations and explain the significance of each illustration. To see the discussion of these results and how the results are obtained or can be improved see section \ref{discussion}. Section \ref{training_loss_and_accuracy} shows the training loss and training accuracy as well as the validation accuracy of the neural network after training. The result of the permutation test can be seen in section \ref{modelAccuracy}. Section \ref{confusionMatrices} shows the bias in the neural network after training on possibly biased data using confusion matrices. As another way to both check and correct bias is to use ROC-curves. These are shown for all the protected groups in section \ref{ROC}, after which equal opportunity and equalized odds have been implemented and shown in sections \ref{equalOpportunity} \& \ref{equalizedOdds}. The ROC-curves for \texttt{decile\_scores} for the ProPublica bias are shown in section \ref{propub}.
	
	Since the same methods have been used for all the protected groups -- race, sex and age -- the results are shown by having the results for the race protected attributes on the top of the page, having the sex protected attribute in the middle and age at the bottom. This is true for every subsection except for section \ref{training_loss_and_accuracy} and section \ref{modelAccuracy}, since every protected attribute uses the same trained binary classifier.
	
	\subsection{Training Loss and Accuracy}\label{training_loss_and_accuracy}
	
	\begin{figure}[H]
		%\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.5\linewidth]{imgs/loss_curve.png}
		%\end{subfigure}
%		\begin{subfigure}{0.5\textwidth}
%			\centering
%			\includegraphics[width=0.9\linewidth]{imgs/loss_curve_sex.png}
%		\end{subfigure}
		\caption{The training loss and training accuracy after training for 400 epocs. It is clear, as expected, that the accuracy rises with the number of epochs. The validation accuracy of the binary classifier obtained after training with a default threshold, $ T=0.5 $, is $ 68.05\%$ which is shown as the green line and the green area is the standard deviation of the accuracy $0.015\% \pm 68.05\%$.% \textbf{Right:} the validation accuracy after running the classifier on sex is $67.61\% \pm 0.017\%$}
		}
		\label{fig:losscurve}
	\end{figure}
	
	\subsection{Significance of Model Accuracy}\label{modelAccuracy}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/rigtig_perm_test.png}
		\caption{An illustration of the permutation test. The black dashed line illustrates the mean permuted accuracy, the red dashed line represents the distribution that the permuted data accuracies follows, the blue dashed line is the accuracy of the model on the original non-permuted data and the standard deviation of this accuracy is shown as the blue area. The standard deviation and mean represents the normal distribution which the permutations form. The number of permutations was $ M =500 $.}
		\label{fig:permtest}
	\end{figure}
	
	\noindent From figure \ref{fig:permtest}, no extreme permutations are present, and since the \textit{p}-value cannot be 0, it is estimated by equation \ref{pval}, which results in the following \textit{p}-value:
	\[p=\frac{1}{M}=0.002\]
	\newpage
	\subsection{Confusion Matrices}\label{confusionMatrices}
	The following section will show the confusion matrices conditioned on three different protected groups which are the following: race, gender and age. As mentioned in section \ref{bias_id}, the postive group belongs to those who have not reactivated vice versa. \vspace*{-0.28cm} 
	\renewcommand\arraystretch{1.5}
	\setlength\tabcolsep{0pt}
	\vspace*{-0.2cm}
	\begin{figure}[H]
	\begin{subfigure}{0.5\textwidth}
	\scalebox{0.7}{
	\begin{tabular}{c >{\bfseries}r @{\hspace{0.9em}}c @{\hspace{0.8em}}c @{\hspace{0.9em}}l}
		\multirow{10}{*}{\parbox{1.3cm}{\bfseries\raggedleft Predicted\\ condition}} & 
		& \multicolumn{2}{c}{\bfseries True Condition} & \\
		& & \bfseries p & \bfseries n & \bfseries Total \\
		& p$'$ & \MyBoxgreen{\hspace*{-0.1cm}TP=193}{\hspace*{-0.1cm}TPP=0.412} \vspace*{0.2cm} & \MyBoxred{\hspace*{-0.1cm}FP=103}{\hspace*{-0.1cm}FPP=0.213} & 296 \\[2.4em]
		& n$'$ & \MyBoxred{\hspace*{-0.1cm}FN=62}{\hspace*{-0.1cm}FNP=0.128} & \MyBoxgreen{\hspace*{-0.1cm}TN=119}{\hspace*{-0.1cm}TNP=0.246} & 181\\
		& Total & 255 & 222 & $ N_{Caucasian}=477 $ 
	\end{tabular}
	}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
	\scalebox{0.7}{
	\begin{tabular}{c >{\bfseries}r @{\hspace{0.9em}}c @{\hspace{0.8em}}c @{\hspace{0.9em}}l}
		\multirow{10}{*}{\parbox{1.3cm}{\bfseries\raggedleft Predicted\\ condition}} & 
		& \multicolumn{2}{c}{\bfseries True Condition} & \\
		& & \bfseries p & \bfseries n & \bfseries Total \\
		& p$'$ & \MyBoxgreen{\hspace*{-0.1cm}TP=259}{\hspace*{-0.1cm}TPP=0.347} \vspace*{0.2cm} & \MyBoxred{\hspace*{-0.1cm}FP=92}{\hspace*{-0.1cm}FPP=0.123} & 351 \\[2.4em]
		& n$'$ & \MyBoxred{\hspace*{-0.1cm}FN=165}{\hspace*{-0.1cm}FNP=0.221} & \MyBoxgreen{\hspace*{-0.1cm}TN=231}{\hspace*{-0.1cm}TNP=0.309} & 396\\
		& Total & 424 & 323 & $ N_{African-American}=747 $ 
	\end{tabular}
	}
	\end{subfigure}
	
		\begin{subfigure}{0.5\textwidth}
			\scalebox{0.7}{
			\begin{tabular}{c >{\bfseries}r @{\hspace{0.9em}}c @{\hspace{0.8em}}c @{\hspace{0.9em}}l}
					\multirow{10}{*}{\parbox{1.3cm}{\bfseries\raggedleft Predicted\\ condition}} & 
					& \multicolumn{2}{c}{\bfseries True Condition} & \\
					& & \bfseries p & \bfseries n & \bfseries Total \\
					& p$'$ & \MyBoxgreen{\hspace*{-0.1cm}TP=123}{\hspace*{-0.1cm}TPP=0.415} \vspace*{0.2cm} & \MyBoxred{\hspace*{-0.1cm}FP=55}{\hspace*{-0.1cm}FPP=0.184} & 179 \\[2.4em]
					& n$'$ & \MyBoxred{\hspace*{-0.1cm}FN=42}{\hspace*{-0.1cm}FNP=0.140} & \MyBoxgreen{\hspace*{-0.1cm}TN=78}{\hspace*{-0.1cm}TNP=0.261} & 120\\
					& Total & 166 & 133 & $ N_{Female}=299 $ 
				\end{tabular}
			}
		\end{subfigure}
		\begin{subfigure}{0.5\textwidth}
			\scalebox{0.7}{
				\begin{tabular}{c >{\bfseries}r @{\hspace{0.9em}}c @{\hspace{0.8em}}c @{\hspace{0.9em}}l}
					\multirow{10}{*}{\parbox{1.3cm}{\bfseries\raggedleft Predicted\\ condition}} & 
					& \multicolumn{2}{c}{\bfseries True Condition} & \\
					& & \bfseries p & \bfseries n & \bfseries Total \\
					& p$'$ & \MyBoxgreen{\hspace*{-0.1cm}TP=484}{\hspace*{-0.1cm}TPP=0.423} \vspace*{0.2cm} & \MyBoxred{\hspace*{-0.1cm}FP=122}{\hspace*{-0.1cm}FPP=0.107} & 606 \\[2.4em]
					& n$'$ & \MyBoxred{\hspace*{-0.1cm}FN=244}{\hspace*{-0.1cm}FNP=0.213} & \MyBoxgreen{\hspace*{-0.1cm}TN=293}{\hspace*{-0.1cm}TNP=0.256} & 537\\
					& Total & 728 & 415 & $ N_{Male}=1143 $ 
				\end{tabular}
			}
		\end{subfigure}

		\begin{subfigure}{0.5\textwidth}
			\scalebox{0.7}{
				\begin{tabular}{c >{\bfseries}r @{\hspace{0.9em}}c @{\hspace{0.8em}}c @{\hspace{0.9em}}l}
					\multirow{10}{*}{\parbox{1.3cm}{\bfseries\raggedleft Predicted\\ condition}} & 
					& \multicolumn{2}{c}{\bfseries True Condition} & \\
					& & \bfseries p & \bfseries n & \bfseries Total \\
					& p$'$ & \MyBoxgreen{\hspace*{-0.1cm}TP=409}{\hspace*{-0.1cm}TPP=0.428} \vspace*{0.2cm} & \MyBoxred{\hspace*{-0.1cm}FP=124}{\hspace*{-0.1cm}FPP=0.130} & 533 \\[2.4em]
					& n$'$ & \MyBoxred{\hspace*{-0.1cm}FN=169}{\hspace*{-0.1cm}FNP=0.177} & \MyBoxgreen{\hspace*{-0.1cm}TN=253}{\hspace*{-0.1cm}TNP=0.265} & 422\\
					& Total & 578 & 377 & $ N_{age>25}=955 $ 
				\end{tabular}
			}
		\end{subfigure}
		\begin{subfigure}{0.5\textwidth}
			\scalebox{0.7}{
				\begin{tabular}{c >{\bfseries}r @{\hspace{0.9em}}c @{\hspace{0.8em}}c @{\hspace{0.9em}}l}
					\multirow{10}{*}{\parbox{1.3cm}{\bfseries\raggedleft Predicted\\ condition}} & 
					& \multicolumn{2}{c}{\bfseries True Condition} & \\
					& & \bfseries p & \bfseries n & \bfseries Total \\
					& p$'$ & \MyBoxgreen{\hspace*{-0.1cm}TP=85}{\hspace*{-0.1cm}TPP=0.309} \vspace*{0.2cm} & \MyBoxred{\hspace*{-0.1cm}FP=35}{\hspace*{-0.1cm}FPP=0.127} & 120 \\[2.4em]
					& n$'$ & \MyBoxred{\hspace*{-0.1cm}FN=77}{\hspace*{-0.1cm}FNP=0.280} & \MyBoxgreen{\hspace*{-0.1cm}TN=78}{\hspace*{-0.1cm}TNP=0.284} & 155\\
					& Total & 162 & 113 & $ N_{age<25}=275 $ 
				\end{tabular}
			}
		\end{subfigure}
	\caption{TPP, FPP, FNP and TNP shows the propotion of TP, FP, FN and TN of the conditioned population respectively. The threshold for all of the confusion matrices are, $ T = 0.5 $. \textbf{Top}: The left matrix shows the confusion matrix of the population conditioned on the Caucasian race and right shows the confusion matrix conditioned on the African-American race. \textbf{Middle}: Left shows the confusion matrix conditioned on female and right shows the confusion matrix conditioned on male. \textbf{Bottom}: Left is the confusion matrix of the population conditioned on age greater than 25 and right shows the confusion matrix conditioned on the age less than 25.}
	\end{figure}
	
	\subsection{Conditional ROC-curves and FPR/TPR Dependency on Threshold}\label{ROC}
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{imgs/ROC.png}
			%\caption{Lorem ipsum}
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{imgs/fpr_tpr_plot.png}
		\end{subfigure}
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{imgs/ROC_sex.png}
			%\caption{Lorem ipsum}
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{imgs/fpr_tpr_plot_sex.png}
		\end{subfigure}
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{imgs/ROC_age.png}
			%\caption{Lorem ipsum}
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{imgs/fpr_tpr_plot_age.png}
			%\caption{Lorem ipsum}
		\end{subfigure}
		\caption{\textbf{Left}: An illustration of the conditional ROC-Curves for race sex and age respectively. Each point on the curve is realized by thresholding the score function R at some value. The diagonal (the dashed red line) represents the relationship of the true positive and false positive rate for a random classifier. \textbf{Right}: An illustration of the true and false positive rates as a function of the threshold for race, sex and age respectively.}
		\label{fig:roc-curve}
	\end{figure}

%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=0.5\linewidth]{imgs/fpr_tpr_plot.png}
%		\caption{An illustration of the true and false positive rates as a function of the threshold. It is clear that, although the difference in the ROC-curves seems small in plot ***, the difference in thresholds causes the true and false positive rates between the two conditional ROC-curves to be different, showing inherent bias and difference between the conditional ROC-curves.}
%		\label{fig:fprtprplot}
%	\end{figure}
		
	\subsection{Equal Opportunity}\label{equalOpportunity}
	
	\begin{figure}[H]
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{"imgs/Equal Opportunity Optimal"}
		\end{subfigure}
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{"imgs/Equal Opportunity Optimal_sex"}
		\end{subfigure}	
		\begin{center}
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{"imgs/Equal Opportunity Optimal_age"}
		\end{subfigure}
		\end{center}
		\caption{An illustration of the optimal equal opportunity threshold on the entirety of the ROC-curves. The points have equal \textit{y}-coordinates which is illustrated with the red horizontal dash line between the blue and green points. \textbf{Left:} The constructed binary classifier yields an accuracy of $ 0.678 $, where the threshold used for the Caucasian is $T_{\text{Caucasian}}= 0.56 $ and the threshold for African-American is $ T_{\text{African-American}}=0.510 $, when equal opportunity is ensured. \textbf{Right:} The achieved accuracy is $0.690$ where $T_{\text{Female}}= 0.68$ and $T_{\text{Male}}= 0.45$. \textbf{Bottom:} The accuracy is $0.68$ with $T_{\text{Greater\_than\_{25}}}= 0.49$ and $T_{\text{Less\_than\_{25}}}= 0.24$.}
		\label{fig:equal-opportunity-optimal}
	\end{figure}
	\vspace*{-0.3cm}
	\subsection{Equalized Odds}\label{equalizedOdds}
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{"imgs/Equalized Odds Optimal"}
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{"imgs/Equalized Odds Correctied"}
		\end{subfigure}
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{"imgs/Equalized Odds Optimal_sex"}
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{"imgs/Equalized Odds Correctied_sex"}
		\end{subfigure}
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{"imgs/Equalized Odds Optimal_age"}
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{"imgs/Equalized Odds Correctied_age"}
		\end{subfigure}
		\caption{\textbf{Left}: An illustration of the dashed blue line used to sample the best equalized odds threshold. Mixing the threshold of 1 with the threshold of the blue point, a predictor which satisfies equalized odds can be found. The equalized odds and equal opportunity bias corrections methods choose almost the same threshold.  \textbf{Right}: These illustrations show the result of the predictor which satisfies equalized odds for race, sex and age in that order. Hence, the blue point is moved onto the green point. The accuracies of the equalized odds predictors are $0.671$, $0.685$ and $0.635$, where the age equalized odds classifier probably gets the lowest accuracy because it is peanalized more for lowering the \texttt{Greater\_than\_{25}} accuracy. The thresholds can be seen in the illustrations.}
		\label{fig:equalizedOdds}
	\end{figure}
	
	\subsection{ProPublica Bias Examination}\label{propub}
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{imgs/propub_ROC_race.png}
			%\caption{Lorem ipsum}
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{imgs/propub_fpr_tpr_race.png}
		\end{subfigure}
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{imgs/propub_ROC_sex.png}
			%\caption{Lorem ipsum}
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{imgs/propub_fpr_tpr_sex.png}
		\end{subfigure}
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{imgs/propub_ROC_age.png}
			%\caption{Lorem ipsum}
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{imgs/propub_fpr_tpr_age.png}
			%\caption{Lorem ipsum}
		\end{subfigure}
		\caption{These plots show how bias could be perceived by ProPublica in their original analysis, when they used \texttt{decile\_score} to analyze FPR and TPR differences. \textbf{Left}: An illustration of the conditional ROC-Curves for race, sex and age respectively. Each point on the curve is realized by thresholding at every value of \texttt{decile\_score}. The diagonal (the dashed red line) represents the TPR and FPR relationship for a random classifier. \textbf{Right}: An illustration of the true and false positive rates as a function of the threshold for race, sex and age.}
	\end{figure}
	
	\section{Discussion} \label{discussion}	
	This section will focus on discussing the results as well as the efficacy of the methods and of bias in general. The first section, section \ref{discussionOfResults}, will make sense of the results seen in section \ref{results} and argue the correctness or possible future work on them. Section \ref{tradeoff} also uses the results but instead sets the focus on uncovering how the accuracy dwindles when using bias correction methods to avoid discrimination of a bias classification model. In section \ref{ethical_dilemma_of_bias}, origins of bias and different cognitive biases will be discussed and the victims of bias will be uncovered. The definitions of fairness for machine learning will be looked into further in section \ref{examination_of_fairness_definitions}. Section \ref{perf_model} will uncover if perfect ML modles need definitions of fairness. Section \ref{why_do_we_care} discusses why bias is even an important topic to discuss to begin with. Lastly, section \ref{safeai} will discuss the principles of safe AI.
	
	\subsection{What do the Results Show?}\label{discussionOfResults}
	The efficacy and reliability of the results of the project given in section \ref{results} will be looked into further in this section. From figure \ref{fig:losscurve}, it becomes apparent that 400 epochs seems to let the binary classifier train to near completion, since the value of the binary cross-entroy training loss could still get slightly lower. Meanwhile, it is also important to be aware of overfitting, so it is important to not just focus on getting a loss that is as low as possible. Reducing overfitting means that the validation accuracy will be higher after training than if the model had overfit the training data. Training for more than 400 epochs reduces validation accuracy, probably exactly because the model starts to overfit, so 400 epochs has been chosen, as this gives the best possible accuracy. It can also be seen that the final validation accuracy is lower than the training accuracy, which is to be expected, since the model was trained to maximize success on the training and test data. The low standard deviation, $ \sigma=0.015 $, reveals that the accuracy should lie within $68.05\% \pm 0.015\%$ around $68\%$ of the time, since the distribution of the accuracies is Gaussian, which can be seen in figure \ref{fig:accuracydistribution} \& \ref{fig:qqplotaccuracies}.
	
	From figure \ref{fig:permtest}, it is clear that the obtained accuracy is indeed significant and different from the accuracies of the permuted data, which is evident, since there are no permutations larger than the observed value or even the accuracy minus one standard deviation. From the estimation of the \textit{p}-value in \ref{modelAccuracy}, it is obvious that the \textit{p}-value is far under the desired level of significance. Hence, the null hypothesis is rejected and the permutation test thus shows that the accuracy from the neural network on the non-permuted data is better than random and has found a meaningful interpretation of the data. This also means that the model is better than a random model (that e.g. only guesses on the largest class), so the classifier could have practical use. The accuracy of $68.05\% \pm 0.015\%$ on the validation set also lines up quite well with the accuracies fround in ProPublica's analysis of the COMPAS recidivism algorithm. They found that their predictive accuracy for Caucasian and African-American defendants were 62.5\% and 62.3\% respectively, while Northpointe found accuracies of 69\% and 67\% for Caucasian and African-American defendants respectively \cite{propublicaAnalysis}. \\\\
	\noindent
	It is obvious from the two normalized confusion matrices for Caucasian and African-American defendants in section \ref{confusionMatrices} that Caucasian defendants have almost a twice as large false positive value, while the same is true for the false negative values for African-American defendants. This signifies that the binary classifier more often labels Caucasians as not being at risk of recidivism even though they were at risk, while African-Americans are more often labeled as being at risk of recidivism when they were not truly at risk. This clearly shows that the model is very biased and that the original bias that seemed to be present in the data when plotting the recidivism scores for Caucasian and African-Americans, shown in section \ref{dataVisuals} and figure \ref{fig:predictedrecidrace}, has been learned by the model as well. Furthermore in section \ref{propub}, it is shown that the orignal COMPAS-algorithm has the racial bias as well. The findings of this project therefore confirms the study done by ProPublica. However, another bias in the dataset is uncovered. From the confusion matrices seen in results (see \ref{confusionMatrices}) for sex, it is obvious that women have a higher false positive value thus showing a bias towards not classifying women at risk to recidivate. From the plots seen in section \ref{dataVisuals} no indication of any bias in relation to sex can be seen, as was seen with race and age. This shows that care needs to be taken to avoid biased datasets. Biases are not always easy to detect nor to calibrate for, either before, during or after training of the model. Meanwhile though, care needs to be taken when dealing with bias removal, as it is necessary to prove that the bias is unjustified in the data. This can be proven by looking at conditional populations and their FPR \& TPR as a function of thresholds. This can reveal if there is an underlying bias which shows discrimination in a classifier. 
	
	The conditional ROC-curves from section \ref{ROC} figure \ref{fig:roc-curve} may at first glance seem very similar for both sex and race, but when the dependency of the specific threshold is taken into consideration, it becomes apparent that there are major differences. The difference in thresholds causes the true and false positive rates between the two conditional ROC-curves, in both cases, to be different, showing an inherent bias and of course a difference between the conditional ROC-curves. This also confirms the suspected bias seen from figure \ref{fig:predictedrecidrace} \& \ref{fig:proirs} for race in the data section. The data visualtion for sex, as earlier mentioned, does not suggest any bias. Since the two ROC-curves are different, using just one common threshold for both races, age groups or genders would result in a biased model, exactly like what happened in Northpointe Inc.'s algorithm. For this reason, equal opportunity or equalized odds with a threshold for each protected attribute (Caucasians and African-Americans \& Male and Female or ages over and under 25 in this project) needs to be used, sometimes even when the curves look deceivingly similar. If the curves are completely similar, another solution is to use group-specific thresholds. \\\\
	\noindent
	The bias between women and men in the dataset has not been uncovered in any papers before and thus the project underlines the importance of using TPR \& FPR dependency on threshold for bias detection. The visualization of the data gave no indication of this bias, but by looking at the conditional populations TPR and FPR as a function of thresholds it is clear that a bias is present. From section \ref{propub}, the original COMPAS algorithm does not seem to have a strong bias for gender. This suggests that machine learning models might be able to construct biases themselves, that are not necessarily present in the dataset. A theory could be that the sigmoid function stretches the dataset thus resulting in the classifier becoming biased. A bias in age has also never been shown in a study before and is also present in the original COMPAS algorithm, see \ref{propub}. This bias can result in young people being unfairly put back into jail which will prevent them from education and much more in life. \\\\ 
	\noindent
	In section \ref{equalOpportunity} figure \ref{fig:equal-opportunity-optimal}, equal opportunity has been implemented to correct the bias in the model. The middle of the ROC-curves seems to yield the best accuracy, which is reasonable, since a classifier with higher true positive rate and lower false positive rate will have a higher accuracy. This also implies that the trivial endpoints, at which the false and true positive rates are already equal, will not bring much value when optimizing for an unbiased model with high accuracy, as the accuracy in these points will be very close to random. As for the implementation of equalized odds in figure \ref{fig:equalizedOdds} from section \ref{equalizedOdds}, it makes sense that the same is true, since almost the same points and thresholds have been chosen to maximize accuracy while keeping the condition of equalized odds. In the case of accuracy reduction, it seems that both methods only carry with them a small decrease in accuracy. The accuracy of the original biased binary classifier is 68.3\%. This accuracy is reduced to 67.8\% using equal opportunity and to 67.1\% using equalized odds. From this, it is obvious that equalized odds takes a bigger hit on accuracy, though the differences in accuracy is still 0.5\% for equal opportunity and 1.2\% for equalized odds, so the overall loss in accuracy definitely seems to be worth it to get a properly working model that does not discriminate on any of the protected attributes. However, the threshold has not been a hyperparameter for the network which mean that the default threshold of $ T = 0.5 $ is not necessarily the optimal threshold to obtain the highest accuracy. Therefore, the loss of accuracy can be more punishing that what it seems from the results of this study. Furthermore, the accuracy found for the protected variable age can be misleading due to the fact that the ROC-curves are not convex. The method implemented in this project has assumed that the ROC-curves are concave. Thus, all possible classifiers in the intersection of the convex hulls of the A-conditional ROC-curves are not examined, which means a higher accuracy can be achieved by examining this area.
	
	\subsection{Trade-Off Between Accuracy and Fairness}\label{tradeoff}
	Normally, when correcting for bias in a binary classifier, the accuracy will take a hit to some extent. This happens because the optimal predictor from both of the groups will probably not be chosen when the true or true and false positive rates have to be equal. This also has to be true because two classifiers are forced to have the same false and true positive rates, which means that the accuracy will be the same over all protected groups, so the lower of the accuracies will be fixed as the accuracy for both of the protected attributes. This is not true in the same sense in equal opportunity, as this method chooses the values with the same true positive rate only. As such, it is possible for the upper of the two curves to utilize the better accuracy, which still ensures some bias correction. As such, only a slight loss of accuracy will be observed \cite{equal_of_oppor}. This can be seen in our project from section \ref{equalOpportunity} and section \ref{equalizedOdds} from figures \ref{fig:equal-opportunity-optimal} and \ref{fig:equalizedOdds}. The accuracy difference of these methods has also been described in section \ref{discussionOfResults}. Here, it is clear that the difference in accuracy between the biased model and equalized odds-corrected model is larger than between the biased model and equal opportunity-corrected model, as is to be expected. In the case of this project though, the differences in accuracies of 0.5\% and 1.2\% is still more than low enough for the bias correction to be worth it, as the non-discrimination also makes the model more accurate for practical use in return.\\\\
	\noindent	
	To better quantify the differences in accuracy between a binary classifier and a random classifier as well as finding the true discrimination of a model, Indrė Žliobaitė proposes that the accuracy difference can be found using Cohen's Kappa, which normalizes the accuracy of the model based on the value of the random classifier accuracy. He also proposes normalizing the discrimination (which, in this case, is defined as the difference between true positives in the protected groups) by the highest discrimination value over all thresholds, as this ensures that the discrimination reaches a maximum if the binary classifier first accepts everyone from the favored protected attribute and only then starts accepting individuals from the non-favored attribute. This finding could make sense to look into for further work into this project, as the true model difference from a baseline or random model would have significance, when attempting to communicate the effectiveness of the model \cite{bias-accuracy}.
	
	\subsection{The Ethical Dilemma of Bias}\label{ethical_dilemma_of_bias}
	
	\subsubsection{Where does Bias Come From?}\label{rootofbias}
	Bias is defined in the dictionary as \textit{\enquote{a strong feeling in favour of or against one group of people, or one side in an argument, often not based on fair judgement.}} \cite{oxford}. In general bias can be natural or innate,  but can also be learned. Bias can be developed by humans for or against people, certain individuals, objects, beliefs or even cultures. Several types of cognitive biases exist. A cognitive bias is a systematic error in thinking that occurs when people are processing and interpreting information in the world around them that affects the decisions and judgments that they make \cite{verywellmind}. A few example of these are: confirmation bias, the halo and horn effect and the misinformation effect. Aside from the three mentioned biases, a plethora of others exist, and all of these biases have consequences of how we as humans make decisions and judge other people. Consider a court room where an evaluation of whether a defendant should be granted parole or not is taking place. Imagine how the Misinformation effect influences the defendants memory processing or how confirmation bias or the halo effect influences the jurisdiction of the judge. Furthermore, studies show that being hungry affects judgement and how one make decisions \cite{eat}.
	
	The consequences of cognitive biases can lead to distorted thinking. Conspiracy theories as well as extremist political beliefs stems from cognitive biases, which can lead to irrational beliefs \cite{cog_bias}. Thus, humans are in general very biased and are affected by bias every day of their life. The question is then if bias is necessarily a bad thing. In psychology it is a well accepted theory that many of the cognitive biases serve humans to optimize our way of living. Among other reasons by allowing decisions within milliseconds, which can be vital in dangerous situations \cite{reisberg}. However, bias can also be discriminating, as bias and prejudice are closely related. Prejudice is prejudgment, through forming an opinion of someone or something beforehand. Prejudices and biases can lead to racism and sexism, which is why detecting bias is an important topic. To summarize, data that has been generated by humans, e.g. the COMPAS dataset, will be biased and needs to be properly handled in order to ensure fair ML models.
	
	\subsubsection{Victims of a Biased Classifier}
	As mentioned in section \ref{rootofbias} all humans are affected by bias. However, when considering the question of who it affects, it refers specifically to the protected groups. These are minority groups in society and undiscovered biases can have major impacts on these groups, as machine learning plays a growing role in the everyday life of humans. For instance, it is important to ensure that the opportunity of getting a loan is equal for all genders, races and age groups. Using historical data from the past to train machine learning models to determine the probability of a person getting a loan can lead to sexist and racist models that reject women or African-Americans when they apply for a loan in the bank. This problem stems from racist and sexist behavior of human beings in the past and therefore, this study calls for the importance of acknowledging biased human behavior. Machine learning models are often viewed as objective, but this can be extremely misleading. The results found in this study confirms the opposite, which proves that a model inherits bias from the data used to train the model. Machine learning models are implemented in several areas of society including autonomous vehicles, the healthcare industry, public safety, the retail sector and many other applications. Thus, ensuring fairness in machine learning models is an import step towards making all models comply with the declaration of human rights (The declaration of human rights can be found at \url{https://www.un.org/en/universal-declaration-human-rights/}).
	
	\subsubsection{Length Time Bias}
	In the world of medicine, length time bias is commonly known. People develop symptoms and deceases at different rates. Prostate cancer for men is  an example where some develop slow growing tumors and some have rapid growing lethal tumors. This difference in rate of development of the sickness is what commonly leads to length time bias \cite{length}.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.3\linewidth]{imgs/length}
		\caption{An illustration of length time bias. A screening in this case will not find all the people with a certain decease. This can be due to people developing symptoms at different rates. Furthermore, it is also seen from the figure that the span of sickness is different for each individual. The origin of the image is \href{http://sphweb.bumc.bu.edu/otlt/MPH-Modules/EP/EP713_Screening/EP713_Screening8.html}{Boston University School of Public Health}. }
		\label{fig:length}
	\end{figure}
	\noindent
	Length time bias can also be present in ProPublica's screening to find out if people recidivated or not. ProPublica did a screening 2 years after the initial Northpointe analysis, which they used to show that the original COMPAS-algorithm is biased towards imprisoning African-Americans. However, this might be misleading, since many of the people in their screening could still potentially recidivate in the future. Therefore, our study also calls for followup screenings to further confirm the present potential bias. To confirm a bias is present, ideally, screenings after 4, 6, 8 and possibly more years in the future need to be performed in order to truly prove the misclassification of the protected groups by the COMPAS algorithm.
	
	\subsection{Examination of Fairness Definitions}\label{examination_of_fairness_definitions}
	The fairness definitions that have been implemented in this project can be seen in section \ref{bias_def}. These are definitions which are proposed by Mortiz Hardt et al. and this section will thoroughly discuss these. Equality of opportunity states that the true positive rate for all groups has to be equal. Consider a hiring algorithm designed to pick the best applicants for a certain job \footnote{a real life example of this is Amazon, which tried to create a job hiring algorithm, see an article by \href{https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G}{Reuters} for more information if desired, however, this is not relevant for this section.}. There are two groups, A \& B, of applicants which both consists of 100 people each. Furthermore, if $ 58 $ people in group A are qualified for the job, whereas $ 2 $ people in group B are qualified for the job, and both groups apply for the job. Then, equalized odds states if $ 30 $ people are to be hired then $ 29 $ people from group A should be hired whereas $ 1 $ from group B should be hired, in order for the algorithm to ensure equal opportunity \cite{towardsdata}. If this was a high end job at Google, then group A would have a higher living standard than B. Group A would afford better education for their children, more expensive houses or apartments etc. This leads to the algorithm, which according to Hardt ensures fairness, to create a larger gap between to possible groups in society. One could then ask if creating a larger gap between groups, resulting in a society with a superior elite, is by any means a way to create fair machine learning models. \newline
	\\
	\noindent 
	The pressing issue of creating a gap between groups is concerning due to the fact that equal opportunity is considered among the best current fairness notions. Furthermore, the stronger notion, equalized odds, requires the false positive rates of two groups to be equal. This constraint enforces a good classifier to be good at predicting all classes, as mentioned in \ref{tradeoff}, in order to achieve a pinnacle accuracy. This could potentially be an advantage that will ensure that models used in society are fair and persistent no matter the protected group. 
	
	\subsection{Does The Perfect Model Need Definitions of Fairness?}\label{perf_model}
	One could imagine that with better methods for training models the world could create sophisticated intelligence with very high accuracies. In an ideal world the best possible classifier would be close to $ 100 \% $ meaning that the model will classify correctly in almost all cases. However, FP and FN will still occur. The question then becomes if a model needs fairness notions if only one in thousands is classified wrong. Would the human race still tolerate racism or sexism then? Through the eyes of some obtaining the best possible model is a way to fight discrimination, moreover, this will not eliminate the behavior but is rather a way to treat the symptoms of discriminating behavior. This is merely a thought experiment of why we need strict rules and regulations in order to not accept discrimination. Obtaining sophisticated and well trained models will not suffice or prevent discriminating behavior and thus notions of fairness, such as Hardt et al.'s proposal, is needed in order to learn from history and not repeat it.
	
	\subsection{Why do we Care about Fairness?}\label{why_do_we_care}
	The thoughts of discrimination and notions of fairness within the field of machine learning came vastly later than the methods themselves. An important question to shed light on is why the concept of fairness is important and which impact the definitions we make now have on the future of the world. In the modern society we live in today, most AI-models implemented commercially are supervised learning models which all learn from data. However, in future settings models will have to make a wider extent of decisions such as controlling traffic lights, driving cars, engaging with lonely individuals and more. At some point in time the human race will develop AI with an intelligence that surpass humans which is known as superintelligence. \cite{superint}. When the human race reaches this pinnacle of inventions, we need to have mastered and defined the utmost notions of fairness. A superintelligence will make vital decisions such as whether to wage war, what a country should invest in and much more. Therefore, notions such as equalized odds will not work as this ultimately could create a gap in society and result in forming an elite that controls the outcome of society as a whole. This study calls for more attention to developing a notion that ensures fairness, while we still live in a world where the consequences of unfair models are merely in the nature of unfair treatment. The discrimination that is present in the world today is due to models such as the one commercialized by Northpointe Inc., and this pressing issue grows as the knowledge and number of studies of fairness in machine learning grows. One of the many reasons why discriminating models are allowed to exist is due to the lack of regulation on machine learning models. Take for instance Rigshospitalet (The National, State or Kingdom Hospital of Denmark) into consideration, where there is currently no regulations on presently used  machine learning models. These are models that are primarily trained on young Caucasian males, and at time of classification of an elderly African-American women, there is no law to ensure that no discrimination is happening or that the classification is unbiased under any notion of fairness. The machine learning models used in the medical industry are only meant to be used as pointers that assist the educated doctor that has the final call. However, due to known cognitive biases such as anchoring, which can affect the outcome, this study calls for proper laws and regulation on the topic of commercialized machine learning models.
	\\\\
	In conclusion, we care about fairness because discrimination is happening right now, which is a pressing issue in society. With no laws or regulations a rise in discriminatory classifiers and machine learning models happens every day that passes by. A lot of people can be unaware that they are being treated unfair, which is why shedding light on this topic will have a meaningful impact on the common good of the world. Furthermore, proper notions of fairness, regulations and laws has to be made in order for the possibility of developing artificial intelligence which acts as a part of society. A solution can be a UN-body(United Nations), which will be able to pass resolutions with proper regulations and fairness notions. The UN-body can furthermore act as a council which ensures that all models fulfill the deceleration of human rights and future resolutions. 
	
	
	%\subsection{The value alignment problem}
	
	
	\subsection{Safe AI}\label{safeai}
	It is important for society to gain trust in machine learning models and artificial intelligence. Steps have already been taken in order to ensure this trust. The Ministry of Foreign Affairs of Denmark has, with the cooperation of the research environment, taken steps in order to ensure that any future development and use of AI lives up to democratic values and control mechanisms. If there is no trust in the machine learning industry from society, there will be no personal data available. Without the availability of personal data that companies, such as Google has, no AI models or machine learning models can be developed. And without AI, no personalized services will be available. The aim is to make sure that consumers and companies, who interact with AI services and products developed in Denmark, are able to trust that they live up to the highest ethical standards and to the standards of Safe AI \cite{larsk}. Professor Lars Kai Hansen from the Technological University of Denmark has produced a list of principles, which ensure safe AI, shown  below. 
	

	\begin{table}[H]
		\centering
		\begin{tabular}{l  l  l l l  l  l}
			\toprule
			\textbf{Safe AI... }                & & & & & & \textbf{Explanation} \\ \midrule
			is secure                     & & & & & & Test of verified software and hardware, adversarials \\
			is open-source                & & & & & & Methods, code, hardware, check and evolution         \\
			is self-conscious             & & & & & & Understands own role                                 \\
			is secretive                  & & & & & & Privacy by design                                    \\
			has calibrated values      & & & & & & Debug for stereotypes, biases                        \\
			is accountable             & & & & & & Transparent, communicating, "right to explanation"   \\
			understands social relations& & & & & & Understands user's knowledge graph                   \\
			understands power          & & & & & & Digital self-defense                                 \\
			generates trust            & & & & & & Use in public relations                              \\ \bottomrule
		\end{tabular}
	\end{table} 

	\noindent
	One of these principles revolves around the model having calibrated values, which corresponds to the model being debugged for stereotypes and bias. Furthermore, the model needs to understand emotions. This is merely an example of a step in the right direction which calls for more studies to define bias notions and to invent methods to calibrate for the biases. This project confirms that the current methods are not sufficient and more sophisticated and fair methods needs to be made. A list of principles will not ensure fairness in machine learning models, but shedding light on the current issues within machine learning, however, will inspire more studies to be done on the topic.
	

	
	\section{Conclusion} \label{conclusion}
	%How is the data "crooked" in the data-set used by the COMPAS algorithm? What bias exists in the data?
	
	%How does bias in the data affect a classification algorithm?
	
	%Understand and implement a bias-correction method from the paper "Equality of Opportunity in Supervised Learning". 
	
	%Which applications can bias-correction algorithms as well as AI-models have in society? And how can society gain trust in these models? 
	
	Generally, after having implemented the bias correction methods equal opportunity and equalized odds, a few things have come to light. Firstly, it is clear that the data could be crooked when looking at section \ref{dataVisuals} figure \ref{fig:predictedrecidrace} \& \ref{fig:predictedrecidage}. It is seen that Caucasians are $ 1.5 $ times as often classified as belonging to the class of low recidivism risk, while African-Americans are more than twice as often labeled as having high risk of recidivism than Caucasians. As such, the data seems to have a bias towards rating African-Americans as having higher risk of recidivism. Similar trends can be seen for the protected group age. A bias towards sex however cannot be seen from figure \ref{fig:predictedrecidsex} where it looks like males and females are treated approximately equal. Furthermore, these illustrations do not necessarily show bias and the actual bias had to be analytically examined.
	\\\\
	As can be understood from section \ref{confusionMatrices}, the bias that is suspected to exist in the data rears it head in the binary classifier too, further signifying the actual existence of this bias. The fact that the classifier is biased is shown by comparing the false negative and false positive values of these confusion matrices, as this signifies that the model is attempting to place more Caucasians and women in the positive class and more African-Americans, men and people under 25 years old in the negative class, which is a biased viewpoint. Meanwhile, the true positive and true negative values do not express bias, as this is the actual recidivism pattern seen in the data. Looking at the conditional ROC-curves and false/true positive rate dependencies on threshold from figure \ref{fig:roc-curve}, the model is also shown to be biased, since the same threshold for the two protected groups yield widely differing true positive and false positive rates. Thus, another way to find bias in the algorithm is to show the ROC-curves of each protected group. From figure \ref{fig:roc-curve} it is clear that a bias is present for race, sex and age. From this plot and the confusion matrices in section \ref{confusionMatrices}, women and Caucasians are most often classified as belonging to the class of low recidivism risk, while men, African-Americans and people younger than 25 have high risk of recidivism. This shows that the classifier seems to have a bias towards rating men, African-Americans and people under 25 as higher risk of recidivism. The conclusion is thus that racial and age biases is obvious in the data from plots and has been inherited by the constructed classifier. These biases are also found in the original COMPAS-algorithm, which was seen from section \ref{propub}. However, a bias towards women is only present through thorough analysis of the conditional ROC-curves. Possibly, the gender bias is learned through the training of a classifier and further analysis is needed to explain where the bias itself comes from, since this bias is not present in the original COMPAS-algorithm.  Moreover, the bias of age is present in both the COMPAS-algorithm and the FFNN constructed in this study and can be calibrated through equalized odds or equality of opportunity.
	\\\\
	Figure \ref{fig:equal-opportunity-optimal} \& \ref{fig:equalizedOdds} demonstrates the implementation of equal opportunity and equalized odds. The accuracy is still high and African-Americans, men and young people, are now no longer being discriminated upon by the binary classifier, since the bias correction methods have ensured either equal TPR or equal TPR and FPR. As such, African-Americans, men and young people are now equally as often as Caucasians, females and older people misclassified. This change was made with little penalty made to the accuracy. As such, for this project, it is definitely worth it to make the model less biased for such a small decrease in accuracy. 
	\\\\
	The applications of bias correction algorithms span across the whole field of AI. The bias correction methods ensure that no minority or any person in society is being discriminated against by machine learning models. The proposed correction methods can be applied to all classifiers post-training and can therefore be used in already commercialized models. However, with the current methods and notions, society as we know it can be turned into two groups; a superior elite and a lesser fortunate group. In order for society to gain trust in these models, this project calls for more studies within this field. Furthermore, organizations such an UN needs to agree upon resolutions, which can certify the rights and fairness of individuals being classified by machine learning models. Each country needs to pass laws and regulations in industries such as healthcare, the court of law ect. to guarantee that all people within a society has trust in the models that will be embedded into modern life. If biased models are implemented, societal bias can be reinforced, possibly leading to even larger inequality. As seen in America at this time, too much discrimination leads to backlash in the form of riots or protests. No one can say for sure what will happen if the bias and discrimination worsens even more and honestly, there is no good reason to find out.
	
	
	\section{Appendix} \label{appendix}
	
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=0.5\linewidth]{imgs/c_charge_degree}
%		\caption{}
%	\end{figure}
%	
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=0.5\linewidth]{imgs/charge_degree_score}
%		\caption{}
%	\end{figure}

	\subsection{Distributions of Categorical Data Variables}\label{dist_categorical}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/races}
		\caption{This illustration shows the number of defendants of each race. It is clear that the number of defendants of all other races than \texttt{Caucasian} and \texttt{African-American} are underrepresented by this dataset, and it will thus become difficult to train a model on these races.}
		\label{fig:races}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/categorical_race}
		\caption{This illustration shows the number of African-American and Caucasian defendants. Obviously, there is a larger amount of African-Americans in the dataset, but not enough to be disproportionate.}
		\label{fig:categoricalrace}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/categorical_sex}
		\caption{The illustration above shows the number of females and males in the dataset. Opposite to \ref{fig:categoricalrace}, the number of males in the dataset dwarfs the number of females.}
		\label{fig:categoricalsex}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/categorical_age_cat}
		\caption{The number of defendants over and under 25 years of age in the dataset. As with \ref{fig:categoricalsex}, a large majority of defendants are over 25 years old.}
		\label{fig:categoricalagecat}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/categorical_c_charge_degree}
		\caption{The number of defendants with each severity of \texttt{c\_charge\_degree}. About twice the amount of defendants have the severity \texttt{F} than \texttt{M}.}
		\label{fig:categoricalcchargedegree}
	\end{figure}
	
	
	\subsection{Distributions of Numerical Data Variables}\label{dist_numerical}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/boxplot_numericals}
		\caption{This illustration shows normalized boxplots for the four numerical variables from the data that the binary classifier is trained on. It is clear that \texttt{age} has more values and is more spread out that the other variables. \texttt{juv\_fel\_count} and \texttt{juv\_misd\_count} contain a low number of distinct values as expected, since most defendants have probably not committed more than 10 felonies or misdemeanors.}
		\label{fig:boxplotnumericals}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/histogram_numerical}
		\caption{Histograms for every numerical variable used in the training of the classifier. Here, it can also be seen that the values of the variables \texttt{juv\_misd\_count} and \texttt{juv\_fel\_count} are a lot lower than the values of \texttt{prior\_count} and \texttt{age}.}
		\label{fig:histogramnumerical}
	\end{figure}
	
	\subsection{Accuracy Distribution}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/accuracy_distribution}
		\caption{From the illustration, it can be seen that the distribution of the accuracies after bootstrapping 1000 datapoints with replacement for 500 different models and training them independently seems to be normally distributed. The mean is a bit lower and the standard deviation a bit higher than what was found in section \ref{permutation_test}.}
		\label{fig:accuracydistribution}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/qqplot_accuracies}
		\caption{Here, a QQ-plot is used to more accurately test the  distribution of the accuracies. It can be seen that this test unanimously concludes, that the accuracies are in fact normally distributed.}
		\label{fig:qqplotaccuracies}
	\end{figure}
	\newpage
	
	\subsection{Significance of Features}
	
	To identify if the features of the FFNN are statistically significant, a permutation test was conducted. It is implemented by permuting every variable in the dataset some number of times and then validating the permuted data with a trained classifier. A null hypothesis $H_{0}$, stating that the observed value comes from the distribution of randomized permutations, is set up and evaluated. To attempt to reject the null hypothesis, each input variable was shuffled randomly and the model was evaluated on the randomized data. This was done one thousand times and the accuracy from each of the permutations was compared to the original accuracy from the model that was trained on the non-permuted validation dataset. The thousand accuracies form a distribution and the result from the neural network on the real data is statistically significant if the \textit{p}-value is under the desired level of significance at $ \alpha = 0.05 $, since this allows for the null hypothesis to be rejected. Intuitively, this is equivalent to the accuracy being significantly far away from this distribution. 
		\begin{figure}[H]
		%\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/perm_test.png}
		%\end{subfigure}
		%		\begin{subfigure}{0.5\textwidth}
		%			\centering
		%			\includegraphics[width=0.9\linewidth]{imgs/perm_test_sex.png}
		%		\end{subfigure}
		\caption{An illustration of a permutation test. The black dashed line illustrates the mean permuted accuracy, the red dashed line represents the distribution that the permuted data accuracies follows, the blue dashed line is the accuracy of the model on the original non-permuted data and the standard deviation of this accuracy is shown as the blue area. The standard deviation and mean represents the normal distribution which the permutations form. %\textbf{Left:} The permutation test for race. \textbf{Right:} Permutation test for sex.}
		}
		\label{fig:permtest}
	\end{figure}\noindent
	From figure \ref{fig:permtest}, no extreme permutations are present, and since the \textit{p}-value cannot be 0, it is estimated by equation \ref{pval}, which results in the following \textit{p}-value:
	\[p=\frac{1}{M}=0.001\]
	\noindent
	Thus, the original features used for training the model are significant for obtaining a good accuracy.
	\newpage
	
	\subsection{Pseudo Code}\label{pseudo_code}
	\subsubsection{Equalized Odds}\label{Pseudo Equalized}
	The implementation of equalized odds below assumes that the ROC-curve belonging to the \texttt{fprs1} and \texttt{tprs1} values is always placed above the other ROC-curve in order to save space, as the implementation is similar for having the curves overlap. The \texttt{accuracy\_score} function used in this implementation is the function from \texttt{sklearn.metrics}. The documentation can be seen here: \url{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html}.
	\begin{lstlisting}[escapechar=@,escapebegin=\itshape]
	def EqualizedOdds(fprs1, fprs2, tprs1, tprs2, true1, true2, probs1, probs2, ...
	                  thresholds1, thresholds2, points):
	                  
	  # @Calculates the equalized odds predictors for two ROC-curves.@
	  # @Parameters:@
	  #    @\textbf{fprs1(list), tprs1(list), fprs2(list), tprs2(list):} False/true positive values for upper@
	  #    @and lower curve respectively.@
	  #    @\textbf{true1(list), true2(list):} The labels for the actual recidivism values.@
	  #    @\textbf{probs1(list), probs2(list):} the probabilities for 0 or 1 recidivism given by the classifier.@
	  #    @\textbf{thresholds1(list), thresholds2(list):} The thresholds used to construct the ROC-curves.@
	  #    @\textbf{points(int):} The number of points used to construct the ROC-curves.@
	  # @Returns:@
	  #    @\textbf{bestAcc(float):} Value of the best accuracy.@
	  #    @\textbf{percentage(float):} The percentage of times the non-1 threshold is sampled.@
	  #    @\textbf{bestThreshold1(float), bestThreshold2(float):} The thresholds used for@
	  #    @the two curves.@
	  
	  accs = empty list of size points
	  thresholdOrder = empty list of size points
	  for pointIndex from 0 to points:
	    # @Defining a line going through (0,0) and the desired point on the lower curve@
	    slope = tprs2[pointIndex] / fprs2[pointIndex]
	    def line(x):
	      return slope * x
	    linediff = abs(tprs1 - line(fprs1))
	    # @Find the intersection of the line and upper curve, considering only points to the right@
	    # @of the lower intersection@
	    intersectionIndex = argmin(linediff[line(fprs1) > tprs2[pointIndex]])
  	    x1 = fprs1[intersectionIndex]
	    x2 = fprs2[pointIndex]
	    y1 = tprs1[intersectionIndex]
	    y2 = tprs2[pointIndex]
		
	    # @Define thresholds and find line lengths to sample from the thresholds@
	    thres1 = 1.0
	    thres2 = thresholds1[intersectionIndex]
	    thresholdOrder[pointIndex] = thres2
	    fullLength = sqrt(y1^2 + x1^2)
	    firstLength = sqrt(y2^2 + x2^2)
	    secondLength = fullLength - firstLength
		
	    sampleThreshold = firstLength / fullLength
	    yPred1Low = if probs1[i] > thres1[i] then 1 otherwise 0 for every value in probs1
	    yPred1High = if probs1[i] > thres2[i] then 1 otherwise 0 for every value in probs1
	    yPred1 = empty list of size probs1
	    for i from 0 to length of yPred1:
	      sampleValue = uniform distribution sample between 0 and 1
	      if sampleValue > sampleThreshold:
	        yPred1[i] = yPred1Low[i]
	      else:
	        yPred1[i] = yPred1High[i]
	    yPred2 = if probs2[i] > thresholds2[i] then 1 otherwise 0 for every value in probs2
	    acc = accuracy_score(true1, yPred1) + accuracy_score(true2, yPred2)) / 2)
	    accs[i] = acc
	  bestIndex = argmax(accs)
	  bestAcc = accs[bestIndex]
	  percentage = 1-sampleThreshold
	  bestThreshold1 = thresholds1[bestIndex]
	  bestThreshold2 = thresholdOrder[bestIndex]
	  return bestAcc, percentage, bestThreshold1, bestThreshold2
	\end{lstlisting}
	
	\subsubsection{Equal Opportunity}\label{Pseudo Equal}
	The \texttt{accuracy\_score} function used in this implementation is the function from \texttt{sklearn.metrics}. The documentation can be seen here: \url{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html}.
	\begin{lstlisting}[escapechar=@,escapebegin=\itshape]
	def EqualOpportunity(fprs1, tprs1, fprs2, tprs2, true1, true2, probs1, probs2, ...
	                     thresholds1, thresholds2, points):
	                     
	  # @Calculates the equal opportunity predictors for two ROC-curves.@
  	  # @Parameters:@
	  #    @\textbf{fprs1(list), tprs1(list), fprs2(list), tprs2(list):} False/true positive values for upper@
	  #    @and lower curve respectively.@
	  #    @\textbf{true1(list), true2(list):} The labels for the actual recidivism values.@
	  #    @\textbf{probs1(list), probs2(list):} the probabilities for 0 or 1 recidivism given by the classifier.@
	  #    @\textbf{thresholds1(list), thresholds2(list):} The thresholds used to construct the ROC-curves.@
	  #    @\textbf{points(int):} The number of points used to construct the ROC-curves.@
	  # @Returns:@
	  #    @\textbf{bestAcc(float):} Value of the best accuracy.@
	  #    @\textbf{bestThres1(float):} Value of the threshold for the upper curve with highest accuracy.@
	  #    @\textbf{bestThres2(float):} Value of the threshold for the lower curve with highest accuracy.@
	  
	  accs = empty list of size points
	  thresholdOrder = empty list of size points
	  for pointIndex from 0 to points:
	    x2 = fprs2[pointIndex]
	    y2 = tprs2[pointIndex]
	    ydiff = abs(y2 - tprs1)
	    intersectionIndex = argmin(ydiff)
	    x1 = fprs1[intersection_idx]
	    y1 = tprs1[intersectionIndex]
	  
	    thres1 = thresholds1[intersectionIndex]
	    thresholdOrder[pointIndex] = thres1
	    thres2 = thresholds2[pointIndex]
	    yPred1 = if probs1[i] > thres1[i] then 1 otherwise 0 for every value in probs1
	    yPred2 = if probs2[i] > thres2[i] then 1 otherwise 0 for every value in probs2
	    acc = accuracy_score(true1, yPred1) + accuracy_score(true2, yPred2)) / 2
	    accs[pointIndex] = acc
	  bestIndex = argmax(accs)
	  bestAcc = accs[bestIndex]
	  bestThres1 = thresholdOrder[bestIndex]
	  bestThres2 = thresholds2[bestIndex]
	  return bestAcc, bestThres1, bestThres2
	  
	\end{lstlisting}
	
	\newpage
	\begin{thebibliography}{9} \label{bibliography}
		
		\bibitem{tv2} K. Andreasen "Flere end 15.000 deltog i dansk Black Lives Matter-demonstration", 2020 Tv2 Lorry, at \url{https://www.tv2lorry.dk/koebenhavn/15000-mennesker-samles-til-dansk-demonstration-mod-racisme-mod-racisme}, visited 19-06-2020
		
		\bibitem{cnn1}  M. Macaya et al. "June 17 Black Lives Matter protests news", 2020 CNN, at \url{https://edition.cnn.com/us/live-news/black-lives-matter-protests-06-17-2020/index.html}, visited at 19-06-2020
		
		\bibitem{cnn2} M. Macaya et al. "June 18 Black Lives Matter protests news", 2020 CNN, at \url{https://edition.cnn.com/us/live-news/black-lives-matter-protests-06-18-2020/index.html}, visited 19-06-2020
		
		\bibitem{euro} N. Liubchenkova "Black Lives Matter protests taking on the world", 2020 Euro News, at \url{https://www.euronews.com/2020/06/15/in-pictures-black-lives-matter-protests-taking-on-the-world}, visited 19-06-2020
		
		\bibitem{guardian} H. Kamara "Black Lives Matter - a photographer's view from the London protests
		Thousands of people have protested across the UK against racism.", 2020 The Guardian, at \url{https://www.theguardian.com/world/gallery/2020/jun/16/black-lives-matter-a-photographers-view-from-the-london-protests}, visited 19-06-2020
		
		\bibitem{equal_of_oppor} M. Hardt, E. Price, and N. Srebro. Equality of Opportunity in Supervised Learning. In NIPS, 2016.
		
		\bibitem{Zafar} M. B. Zafar, I. Valera, M. G. Rodriguez, and K. P. Gummadi. Fairness constraints: A mechanism for fair classification.
		In ICML Workshop on Fairness, Accountability, and Transparency in Machine Learning, 2015.
		
		\bibitem{floridaLaw} The State of Florida, "The 2019 Florida Statutes", 1995,  \url{http://www.leg.state.fl.us/statutes/index.cfm?App_mode=Display_Statute&URL=0100-0199/0119/0119.html}, visited 12-03-2020
		
		\bibitem{propublicaAnalysis} ProPublica, "How We Analyzed the COMPAS Recidivism Algorithm", 2016, \url{https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}, visited 12-03-2020
		
		\bibitem{oxford} Oxford Dictionary, at \url{https://www.oxfordlearnersdictionaries.com/definition/english/bias_1?q=bias}, visited 19-06-2020.
		
		\bibitem{aktiv_gp} F. Bergamin. 02463 “Active Machine Learning and Agency” Lecture 1: Gaussian Processes, 2020.
		
		\bibitem{aktiv_bo} F. Bergamin. 02463 “Active Machine Learning and Agency” Lecture 2: Bayesian Optimization, 2020.
		
		\bibitem{dl} Goodfellow-et-al-2016. Deep Learning, MIT Press in 2016. 
		
		\bibitem{b_woodworth} B. Woodworth et al. Learning Non-Discriminatory Predictors, Toyota Technological Institute at Chicago, Chicago, IL 60637, USA; Proceedings of the 2017 Conference on Learning Theory.
		
		\bibitem{g_goh} G. Goh et al. Satisfying Real-world Goals with Dataset Constraints. In NIPS, 2016.
		
		\bibitem{p-value} Phipson, Belinda and K. Smyth, Gordon "Permutation P-values Should Never Be Zero: Calculating Exact P-values When Permutations Are Randomly Drawn", Walter and Eliza Hall Institute of Medical Research, 31. October 2010.
		
		\bibitem{verywellmind} Verywell Mind, "What Is Cognitive Bias?", 05 May 2020,
		\url{https://www.verywellmind.com/what-is-a-cognitive-bias-2794963#:~:text=A%20cognitive%20bias%20is%20a,and%20judgments%20that%20they%20make.}, visited 08-06-2020
		
		\bibitem{eat} P. Hunter Your decisions are what you eat. Metabolic state can have a serious impact on risk-taking and decision-making in humans and animals. EMBO Rep. 2013
		
		\bibitem{cog_bias} Žeželj, I., \& Lazarević, L. B.  Irrational Beliefs. Europe’s Journal of Psychology, 15(1), 1-7 2019.
		
		\bibitem{reisberg} D. Reisberg Cognition chapter 12 - Judgement and Reasioning, sixth edition (2015)
		
		\bibitem{towardsdata} Ziyuan Zhong, "A Tutorial on Fairness in Machine Learning" 2018, \url{https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb}, visited at 08-06-2020
		
		\bibitem{bias-accuracy} Indre Zliobaite. On the relation between accuracy and fairness in binary classification. CoRR, abs/1505.05723, 2015.
		
		\bibitem{superint} Nick Bostrom, "Superintelligence" Oxford University Press, 29-03-2016
		
		\bibitem{researchgate} J. Ashfaque, "Introduction to Support Vector Machines and Kernel Methods", 2019, \url{https://www.researchgate.net/figure/Diagram-of-k-fold-cross-validation-with-k-10-Image-from-Karl-Rosaen-Log_fig1_332370436}, visited 17-06-2020
		
		\bibitem{larsk} Ministry of Foreign Affairs of Denmark, "Denmark Paves the Way for the Implementation of Trust by Design", 25.03.2019, \url{https://investindk.com/insights/denmark-paves-the-way-for-implementation-of-trust-by-design}, visited 12-06-2020
		
		\bibitem{length} W. LaMorte, "Evaluating Screening Programs", 2016, \url{http://sphweb.bumc.bu.edu/otlt/MPH-Modules/EP/EP713_Screening/EP713_Screening8.html}, visited 21-06-2020
		
		
	\end{thebibliography}
	
	
	\newpage
	\bibliographystyle{IEEEbib}
	\bibliography{refs}
\end{document}
