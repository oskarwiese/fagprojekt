% !TeX spellcheck = en_US
\documentclass[11pt, fleqn, titlepage]{article}
%\usepackage{siunitx}
\usepackage{texfiles/SpeedyGonzales}
\usepackage{texfiles/MediocreMike}
\newcommand{\so}[2]{{#1}\mathrm{e}{#2}}
% \geometry{top=1cm}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{ragged2e}
\usepackage{booktabs}
\usepackage{lipsum}
\usepackage{csquotes}
\usepackage{longtable}
\usepackage{arydshln}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
}
\usepackage{subfig}
\usepackage{graphicx}
\title{Fairness in Classification}
\author{Anders Henriksen \\ Oskar Eiler Wiese Christensen  \\ \texttt{\{s183917, s183904\}@student.dtu.dk}}
\date{\today}

\pagestyle{plain}
\fancyhf{}
\rfoot{Page \thepage{} of \pageref{LastPage}}

\graphicspath{{Billeder/}}

\begin{document}
	
	\maketitle
	\begin{abstract}
		\textbf{TODO: Skal indeholde motivation, problem, fremgangsmåde, resultater og konklusion.} \\ \lipsum[1-2]
	\end{abstract}
	\tableofcontents \newpage
	%\thispagestyle{fancy}
	%\tableofcontents	
	
	\section{Introduction} \label{indledning}
	%\textbf{TODO: \\ Hvad er formålet med projektet? \\ Hvad er problemformuleringen \\ Hvad er state-of-the-art? \\ Hvilken fremgangsmåde bruges til at løse problemet? \\ Hvem har brug for resultaterne?}
	
	\subsection{Motivation}
	%TODO 
	%Der skal skrives mere aktuelt om situtationen i USA 
	Artifical intelligence (AI) and machine learning (ML) methods are playing a bigger and bigger role in modern society. As the accuracies of AI and ML models increase, their applications become wider, allowing for these  models to be implemented either as ground truth or as a pointer in applications like autonomous vehicles, medical imaging, the American judicial system ect. These models are, for the general public, often seen as an objective decision maker. As such, it becomes of essence to avoid discrimination, since model discrimination could lead to reinforced societal discrimination. Bias seems to originate from the dataset, meaning that a saving grace would be to either implement bias correction on the dataset to remove the bias from the source of the problem or to remove bias from the model, thereby removing the risk of discrimination when using the model for classification tasks. This report aims to understand bias and how it affects datasets as well as classification models. Furthermore, a bias correction method will be put to use in order to explore the possibilites of keeping discrimination away from important fields. More specifically, the questions to be answered throughout the report are whether there is a bias in the COMPAS recidivism dataset and which kind of bias there is, as well as what this bias means in the dataset. Meanwhile, it is also analyzed how bias affects a range of classification algorithms, how bias in an algorithms is detected and how to quantify the bias. Lastly, bias correction methods will be implemented and tested and an ethical discussion will be carried out to understand the applications of bias correction algorithms and AI models on society as well as how society can learn to trust these models. \\
	
	\noindent In this report, the rest of section \ref{indledning} will focus on state of the art within the field of bias correction and the contributions from this report. Afterwards, section \ref{data} will present the necessary details of the COMPAS dataset including visualizations of possibly discriminatory variables like "race" and "sex", explanations of the variables of the dataset and how the data was collected and put together. Section \ref{methods} on methods sets up the necessary background knowledge for the topics of FFNNs, bayesian optimization and permutation tests. Results will be illustrated in section \ref{results} through figures and tables, which will then be discussed and put into a greater perspective in section \ref{discussion}. The discussion also contains the ethical discussion of bias and bias correction as a whole. The whole report will be summed up in section \ref{conclusion}. Some lesser relevant figures and points have been left out of the report but can be found in section \ref{appendix} and the relevant sources are found in section \ref{bibliography}.
	
	\subsection{Equal Opportunity and Equalized Odds}\label{bias_def}
	
	The two main fairness definitions used in this project is proposed by Hardt et. al \cite{equal_of_oppor}. \textbf{Definition 1.1}(Equalized Odds)\textbf{:} A given predictor $ \hat Y $ satisfies equalized odds with respect to the protected attribute A and outcome $ Y $, if $ \hat Y $ and A are independent conditional on Y. If the target is binary, then equalized odds can be expressed mathematically as,
	\begin{equation*}\label{key}
	\operatorname{Pr}\{\widehat{Y}=1 | A=0, Y=y\}=\operatorname{Pr}\{\hat{Y}=1 | A=1, Y=y\}, \quad y \in\{0,1\}
	\end{equation*}
	What equalized odds states is, that if $ \hat Y $ has to be a fair predictor, then if $ y = 1 $, then the predictor has to have equal true positive rates for both classes of A, i.e. $ A = 0 $  and $ A = 1 $. However, if $ y = 0 $ then the predictor $ \hat Y $ has to have equal false positive rates. Equalized odds ensures equal accuracy and bias in all classes, and punishes models that perform well only on the majority.
	\\\\
	\textbf{Definition 1.2}(Equal Opportunity)\textbf{:} A binary predictor $ \hat Y $ satisfies equal opportunity with respect to A if $ \operatorname{Pr}\{\hat{Y}=1 | A=0, Y=1\}=\operatorname{Pr}\{\hat{Y}=1 | A=1, Y=1\} $. The equal opportunity ensures that the probability of predicting $ \hat Y = 1 $ is equal across the classes given the fact that $ Y = 1 $. Equal opportunity is a weaker notion of non-discrimination than Equalized Odds, but still allows for better utility.
	
	
	\subsection{State of the Art}
	There are currently two main ways to ensure fairness in classification. One of the ways is by implementing steps during the training process of the classifier in order to ensure fairness definitions. The second method, which is the one which will be implemented in this project, involves post-processing steps. Post-processing methods are often desired because the method can be used on models that have been trained and are commercialized. With the vastly increase of Machine Learning models on the market, society calls for ways to ensure that these models do not discriminate any minority or anyone for that matter. Many different suggestions of how to implement bias correction algorithms within the field of fair AI have been proposed in the last years. However, this is still a young field of science.\\\\
	\noindent
	An approach proposed by Zafar et al. \cite{Zafar}, suggests that a way to achieve fair classifiers is by using linear constraints on the covariance between predicted labels and the value of features. This is a method that is implemented during the training process, which means that trained algorithms would have to be retrained. "Satisfying Real-world Goals with Dataset Constraints" uses the same strategy to remove bias inheritance from the dataset, which is to use linear constraints. The study proposes constraints in the training data, and by using the ramp penalty to quantify cost accurately, they succeed in developing an efficient algorithm to optimize the resulting non-convex constrained optimization problem. This state of the art algorithm can be implemented in situations where one may require a classifier to make predictions with a certain rate of positive predictions in order to maintain fairness for a population. Equality of opportunity is one of such definitions which require the true positive rates of protected attributes to be equal. \cite{g_goh} \\
	
	\noindent The main method that will be demonstrated in this project is proposed by Hardt et al. \cite{equal_of_oppor}. The suggested method can achieve a non-discriminatory classifier by a post-processing step. The idea is to learn thresholds in order to deal with sensitive features or groups in the data in a discriminant classifier. The method requires that the classifier has information about the data at decision-time, which is the case in the experiment conducted in this project. The study proposes, that with the learned thresholds, they can ensure fairness definitions (see \ref{bias_def}) by using different thresholds for certain protected groups. However, another article, namely "Learning Non Discriminatory Predictors", argue that by only using the post-hoc correction (post training methods), the algorithm will still be unfair. \cite{b_woodworth} They show that for several loss-functions such as the 0-1 or hinge loss, the method proposed by Hardt et al. \cite{equal_of_oppor} can fail. In the paper, a notion is defined to obtain an approximation of a non-discrimination algorithm. However, this notion is explored during the paper, and it turns out that creating a non-discrimination algorithm is computationally hard. Therefore, a relaxation definition of equalized odds is presented, which is based on a second-moment condition instead of full conditional independence. Throughout the paper, it is shown that under this condition it is possible to learn a nearly optimal non-discriminatory linear predictor with respect to a convex loss without it being too computational to train. \cite{b_woodworth}\\\\
	 The scope of this project is to implement the post-hoc correction method presented by Hard et al. \cite{equal_of_oppor} on a binary classifier trained on the ProPublica dataset. The goal is to obtain a non-discriminatory classifier which still retains a respected accuracy. The motivation behind implementing the work of Moritz Hardt is due to his great success within the field of fairness in Machine Learning. Hardt has more than 7000 citations, is currently writing a book about fairness in ML and has several scientific studies, including Equality of Opportunity in Supervised Learning. Moreover, the fairness definition of Equalized Odds, Equality of Opportunity and Demographic Parity are currently well accepted fairness definitions. Thus, the benefits and trade offs of using Hard et al.'s definitions will be discussed and examined thoroughly.
	
	\subsection{Contributions}
	This report aims to give an approachable account of the effect of bias as well as the most promising methods of removing bias and avoiding discrimination from learned supervised learning models in datasets. As such, the reader can expect to get the following from this report.
	
	\begin{itemize}
		\item Using permutation and illustrations, an overview of the bias present in the modified COMPAS recidivism dataset as well as the effect of this bias on the classifiers implemented in this report will be made. 
		\item A comprehensive table of all variables present in the modified COMPAS recidivism dataset will be provided. This table includes the name of the variable, a short description of the purpose of the variable as well as the type of variable in order to remove ambiguity for the definitions of the variables.
		\item Two bias correction methods, equalized odds and equal opportunity, will be presented, both through text and mathematical definition, and implemented in order to show the effect of the bias corrections on a variety of classification models including a neural network, random forest, guassian naive bayes and logistic regression.
		\item To put the other contributions into a real-world context, an ethical discussion of the quantified bias and discrimination models will be carried out.
	\end{itemize}
	The general goal of the contributions given above is to avoid models that discriminate with respect to some protected attribute. In this report, race plays the biggest role as a protected attribute, but the findings of the discriminating models and bias correction can equally be transferred to other variables that are suspected to be discriminated against.
	
	\section{Data} \label{data}
%	\textbf{TODO: \\ Data kan kort introduceres i indledningen \\ Lav etisk diskussion om opbevaring af data samt privacy issues}
	
	\noindent To analyse the efficacy of using bias correction to remove discrimination among races in the American justice system, the modified COMAS recidivism dataset from ProPublica has been used. An important note on the further work in this report on the dataset is that the output variable \texttt{score\_text} has been turned into a binary vector. This method was also used by ProPublica during their analysis and since the bias correction methods only work on binary data, turning the output into a binary vector was logical. The binarization was performed by merging the \texttt{Medium} and \texttt{High} risk of recidivism into zeros and the \texttt{Low} risk of recidivism into ones, as has been in the ProPublica analysis. \\
	A general overview of the dataset and how it came to be as well as the variables used for the classifier is given in \ref{dataDescription}. The data and variables have not been properly explained to any extent in examined literature, so \ref{dataExamination} will cover the meaning of all 53 variables of the dataset to remove ambiguity and set a common ground from which to base the future analysis. Lastly, \ref{dataVisuals} will show important visualizations of features of the dataset to cover which of the variables are most likely to contain biases that will be explored further in later sections.
	
	\subsection{Description of Data} \label{dataDescription}
	The data used in this project stems from an initial analysis of the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm by its developers, Northpointe Inc. After this analysis, ProPublica made a subsequent analysis of this data as well as their own queries of the offenders involved and data of the offenders who actually recidivated. This data is stored in the \texttt{compas-scores-two-year.csv} dataset from ProPublica's GitHub page, which can be found here: \url{https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv}. \\\\
	\noindent The data consists of 53 different variables, 9 of which are used in the binary classification model. Four of the chosen variables are categorical, so these will have to be one-out-of-k encoded to be able to feed the necessary variables into the neural network. Meanwhile, the numerical variables, of which there are also four, will be normalized as to avoid the vanishing gradient problem and to avoid having some variables be of more importance to the final prediction. This will be explained more thoroughly in \ref{Feed-forward neural}. \\
	The 10 chosen variables, of which one will be used as the target variable are shown and explained below. To see an exhaustive explanation of all variables from the full dataset, see \ref{dataExamination}.
	
	
	\begin{table}[H]\label{resultater}
		\centering
		\begin{tabular}{l l l}
			Variable & Description & Type \\ \hline
			age & The age of the offenders & Continuous ratio \\
			priors\_count & The number of previous offences & Discrete interval \\
			juv\_fel\_count & The number of previous juvenile felonies & Discrete interval \\
			juv\_misd\_count & The number of previous juvenile misdemeanor & Discrete interval \\
			c\_charge\_degree & The severity of the offence & Discrete nominal \\
			race & The race of the offender & Discrete nominal \\
			age\_cat & The age category of the offender & Discrete nominal \\
			sex & The sex of the offender & Discrete nominal \\
			score\_text & The COMPAS prediction of chance of recidivism & Discrete interval
		\end{tabular}
		%\caption{text}
	\end{table}
		
		
	\subsection{Explanation of Data Variables} \label{dataExamination}
	Using and researching the extended COMPAS dataset has shown that there is no documentation of the meaning of the variables. This makes interpretation difficult and can make singular variables of the dataset next to impossible to understand. To remove ambiguity about the meaning of the variables, an exhaustive description of every variable and variable type has been produced. This allows for the results of this report to be contained within this interpretation and shines light on the true contents of the dataset. Some variables, mostly those with \textit{c\_}, \textit{r\_} or \textit{vr\_} prefixes,have been grouped together in the table, as these serve the same purpose in the dataset for \textit{custody}, \textit{recidivism} and \textit{violent recidivism} cases respectively. 
	
	\noindent Some variable names have the prefix \textit{v\_}, which has been disregarded as an error. As such, this interpretation of the dataset assumes that these variables are akin to those with the \textit{vr\_} prefix, representing the variables related to violent recidivism. Furthermore, in the case of a few of the variables in the dataset, it has simply not been possible to find a meaningful interpretation, since the entire column is either full of NaN's like in the case of \textit{violent\_recid} or every value of the column one to one with another variable, which is the case between \textit{decile\_score} and \textit{decile\_score\_1} as well as \textit{priors\_count} and \textit{priors\_count\_2}. These variables are kept in the table for completeness but seem to have no discernible purpose in the dataset.
	
	
	\begin{longtable}{l l l} \label{allVars}
		\centering
		Variable & Description & Type \\ \hline
		id & Index of the column & Discrete interval \\
		name & Full name of the offender & Discrete nominal \\
		first & First name of the offender & Discrete nominal \\
		last & Last name of the offender & Discrete nominal \\
		compas\_screening\_date & The date the COMAS score was given & Discrete nominal \\
		sex & The sex of the offender & Discrete nominal \\
		dob & The offender's date of birth & Discrete nominal \\
		age & The offender's age & Continuous ratio \\
		age\_cat & Which age category the offender belongs to & Discrete nominal \\
		race & The offender's race & Discrete nominal \\
		juv\_fel\_count & The number of previous juvenile felonies & Discrete interval \\
		juv\_misd\_count & The number of previous juvenile misdemeanor & Discrete interval \\
		juv\_other\_count & The other types of previous juvenile crimes & Discrete interval \\
		days\_b\_screening\_arrest & ----- & Discrete interval \\ \hdashline
		priors\_count & The number of previous offences & Discrete interval \\
		priors\_count\_2 & & \\ \hdashline
		c\_jail\_in & The date the offender was jailed & Discrete nominal \\
		r\_jail\_in & & \\ \hdashline
		c\_jail\_out & The date the offender was removed from jail & Discrete nominal \\
		r\_jail\_out & & \\ \hdashline
		c\_case\_number & The offense case number & Discrete nominal \\
		r\_case\_number & & \\
		vr\_case\_number & & \\ \hdashline
		c\_offense\_date & The date the offense was performed & Discrete nominal \\
		r\_offense\_date & & \\
		vr\_offense\_date & & \\ \hdashline
		c\_charge\_degree & The severity of the offense & Discrete nominal \\
		r\_charge\_degree & & \\
		vr\_charge\_degree & & \\ \hdashline
		c\_charge\_desc & The offense that was performed & Discrete nominal \\
		r\_charge\_desc & & \\
		vr\_charge\_desc & & \\ \hdashline
		c\_arrest\_date & ----- & Discrete nominal \\
		c\_days\_from\_compas & ------ & Discrete interval \\
		r\_days\_from\_arrest & Recidivism days until arrested for the crime & Discrete interval \\
		violent\_recid & Completely empty data column & NaN \\ \hdashline
		is\_recid & Whether the offender truly recidivated after two years (0/1) & Discrete nominal \\
		is\_violent\_recid & & \\
		two\_year\_recid & & \\ \hdashline
		type\_of\_assessment & What the COMPAS algorithm predicted for the offender & Discrete nominal \\
		v\_type\_of\_assessment & & \\ \hdashline
		decile\_score & Value from 1-10 representing risk of recidivism & Discrete ordinal \\
		decile\_score\_1 & & \\
		v\_decile\_score & & \\ \hdashline
		score\_text & decile score split into three categories & Discrete interval \\
		v\_score\_text & & \\ \hdashline
		screening\_date & When the offender was given the assessment & Discrete nominal \\
		v\_screening\_date & & \\ \hdashline
		in\_custody & When the offender was placed in custody & Discrete nominal \\
		out\_custody & When the offender was removed from custody & Discrete nominal \\
		start & The day the screening started compared to the start of the process & Discrete interval \\
		end & The day the screening ended compared to the start of the process & Discrete interval \\
		event & ------ & Discrete interval
	\end{longtable}
		
	\subsection{Visualization of Data} \label{dataVisuals}
	To better get an understanding of the data, a range of plots are shown below. \ref{fig:predictedrecidrace} aims to show wheter there is a difference between the fraction of whites and african-americans that are predicted by the COMPAS classifier as having low, medium and high risk of recidivism. \ref{fig:predictedrecidsex} has the same purpose, utilizing sex instead of race. \ref{fig:truerecid} and \ref{fig:proirs} give some insight into the actual amount of crime done by african-americans and whites and whether the amounts seem to be similar.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/predicted_recid_race}	
		\begin{table}[H]
			\centering
			\begin{tabular}{|l|l|l|l|}
				\hline
				& Low   & Medium & High  \\ \hline
				Caucasian        & 0.652 & 0.236  & 0.112 \\ \hline
				African-American & 0.412 & 0.311  & 0.277 \\ \hline
			\end{tabular}
		\end{table}
		\caption{It is clear from this illustration that the number of caucasian and african-american people in the low group are almost equal. Meanwhile the medium and high groups contain a much larger fraction of african-americans. Since, in total, there are more african-americans in the dataset than whites (3696/2454), it would seem based purely on the data that whites are more often classified as low risk of recidivism while african-americans are often classified as medium or high risk.}
		\label{fig:predictedrecidrace}
	\end{figure}

	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/true_recid}
		\begin{table}[H]
			\centering
			\begin{tabular}{|l|l|l|}
				\hline
				& 0   & 1  \\ \hline
				Caucasian        & 0.582 & 0.418  \\ \hline
				African-American & 0.449 & 0.551  \\ \hline
			\end{tabular}
		\end{table}
		\caption{This illustration shows the true recidivism values (0 being no recidivism after two years and 1 being recidivism) for all offenders in the dataset seperated by the race of each offender. It is clear that close to an equal amount of white and african-american offenders did not recidivate, while a much larger proportion of african-americans re-offended than whites. This makes it difficult to prove bias in the data based only on the data, as it is not necessarily a bias that african-americans more often re-offend.}
		\label{fig:truerecid}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/predicted_recid_sex}
		\begin{table}[H]
			\centering
			\begin{tabular}{|l|l|l|l|}
				\hline
				& Low   & Medium & High  \\ \hline
				Male      & 0.500 & 0.274  & 0.226 \\ \hline
				Female    & 0.540 & 0.301  & 0.152 \\ \hline
			\end{tabular}
		\end{table}
		\caption{This illustration shows the relationship between sex and the COMPAS prediction of recidivism. The first point to be noted is that there are a lot more men in the dataset than women (5819/1395). Aside from this, it does not seem like there is a difference in the proportions of men or women being classified as belonging to each of the categories.}
		\label{fig:predictedrecidsex}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/proirs}
		\caption{An illustration of the relation between race (african-american or caucasian) and the number of previous felonies prior to the study taking place. This plot gives a clear indication that the number of whites and african-americans who have performed no prior felonies is similar. Otherwise, it is clear that the proportion of african-american to whites becomes larger as the number of previous felonies becomes larger. This seems to indicate that african-americans are more often engaged in criminal activity, which also weakens the indication of a bias from \ref{fig:predictedrecidrace}.}
		\label{fig:proirs}
	\end{figure}
	
	
	
	\subsection{Data Origin}
	The COMPAS dataset as well as ProPublica's expanded data and analysis of this data is freely available from their public GitHub repository. As such, the data is not stored safely. This is also true for the variables of the data such as age, full name and number of previous felonies, which give ample opportunity to identify people who have recidivated, leading to compromise of their personal data. This is not a concern though, as the data is collected from Florida, where public records are subject to the broad legislated public right of inspection. According to chapter 119 section 1 of the law of the State of Florida, 
	\begin{displayquote}
		"It is the policy of this state that all state, county, and municipal records are open for personal inspection and copying by any person. Providing access to public records is a duty of each agency." \cite{floridaLaw}
	\end{displayquote}
	
	\noindent As such, since ProPublica submitted a public records request for access to the data for use in research \cite{propublicaAnalysis}, which inherently is a type of inspection, they are within the legal ramifications to use the dataset as they please. Another debate entirely, which will in particular be taken up in \ref{discussion}, is the more subjective question of the ethicality of open-record laws and public criminal data in their entirety being shared among whomever and what exactly should be allowed to be done with this data.
	
		
	\section{Methods} \label{methods}
%	\textbf{TODO: \\ referér til kode og software \\ brug lang tid på nye %metoder, kort tid på gamle metoder}
	
	\subsection{Binary Classifier}\label{Feed-forward neural}
	%initialization schemes
	\subsubsection{Feed-forward Neural Network}
	Feed-forward neural networks (FFNN) are the simplest kind of neural network. The flow of information in a feed-forward neural network is one-directional, which means that the network has an input layer, and information flow from these nodes through the hidden layers unto the output layer. The main purpose of a FFNN is to approximate a function. In this project $ y = f^*(x) $ maps an input $ \mathbf x $ to a category $ \mathbf y $. The FFNN is thereby a classifier, the goal of which is to determine the recidvism risk of a person given the input variable $ \mathbf x $. $ \mathbf x $ contains both categorical and continuous variables. The categorical variables that are fed to network are the following: \texttt{["c\_charge\_degree", "race", "age\_cat", "sex"]} and the continuous variables are \texttt{["age", "priors\_count", "juv\_fel\_count", "juv\_misd\_count"]}. Most of the other variables contains dates, and would as such make it computationally inefficient to train the network, as one-out-of-k encoding dates takes a lot of rows. The possible values of $ \mathbf y $ are 0 or 1, which corresponds to the classifier classifying a person as either low risk or medium/high risk of recidivism respectively. Hence, this is a binary classifier which constructs a mapping $ \mathbf y = f(\mathbf x \ ; \mathbf w) $. The goal of the FFNN is to learn the weights that most efficiently approximate the function $ f^* $ through supervised learning. The binary classifier in this project has an input layer, five hidden layers and an output layer. \cite{dl}
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{imgs/ffnn}
		\caption{Visualization of the binary classifier model. The input layer has dimension $ \mathbb R ^8$ and the output layer has dimension $ \mathbb R^1 $. The number of nodes in the hidden layers are hyperparameters. The number of nodes used in this project are, layer one through five respectively: $ \mathbb R ^{16} $, $ \mathbb R ^{32} $, $ \mathbb R ^{64} $, $ \mathbb R ^{128} $, $ \mathbb R ^{64} $}
		\label{fig:ffnn}
	\end{figure}
		
	\noindent 
	To obtain the pinnacle of accuracy in a neural network, it needs an excellent architecture as well as the optimal hyperparameters. However, the search for these parameters is usually a costly process of uncertainty, balancing exploration of parameters and the exploitation of previous results. Often, the process of finding the optimal parameters and network architecture comes from expert knowledge, biased heuristics or exhaustive sampling from the parameter space. Since the scope of this project is to demonstrate biased algorithms, the goal is to construct a classifier with an accuracy vastly better than random. The hyperparameters used for the model in this project are based on preliminary runs as well as an exhaustive search through the parameter space. The fully connected layers are trained on the COMPAS dataset with a supervised learning strategy. The hyperparameters used are a drop-out value at $p=0.1$, which is used on each layer. The learning rate, $\alpha$, is initialized as $\alpha = 0.001$. However, the optimization algorithm used for this project is Adaptive Moment Estimation (Adam). Adam is a well known optimizer in the literature and has adaptive an learning rate as well as step size. The activation function used is rectified linear unit (ReLU), which is defined as $ y = \text{max}(0,x) $. Thus, the function returns x for positive values and zero otherwise. The activation function ensures non-linearity which allows for a more expressive model. binary cross-entropy Loss is used as the cost function. The loss in the binary classifier is calculated using the expression below. 
	\begin{equation}\label{key}
	L\left(\boldsymbol{y}_{i}, \hat{\boldsymbol{y}}_{i}\right) = -(\mathbf y \log (\mathbf {\hat y})+(1-\mathbf  y) \log (1-\mathbf {\hat y}))
	\end{equation}
	The binary binary cross-entropy loss punishes the algorithm if the predicted value is far from the label. I.e. if the classifier predicts 0.09 and the label is 1, it would result in a very high loss. The ideal model would have a binary cross-entropy loss of zero. \cite {dl} The result is a FFNN with one output node where a threshold can be set, to manipulate how the network will classify an input \textbf{x}. This will be elaborated upon further in section \ref{biascorr}.

	
	\subsection{Permutation test}
	To identify both whether or not the dataset is biased, and if the accuracy of the FFNN is statistically significant, a permutation test was conducted. It is implemented by permuting every variable in the dataset a set number of times and then validating the permuted data with a trained classifier. If the newly acquired accuracy from the permuted data is different from the non-permuted accuracy, this could possibly mean that the classifier is biased. To phrase differently, if the classifier has a tendency to classify black people as medium/high risk, a permuted race variable would yield a lower accuracy than the non-permuted data. However, it might just be the reality that all black people have a higher chance of recidivism than white people. Thus, further investigations have to be performed to prove the existence of actual bias in an algorithm. \\\\
	Permutation tests can also be used to test whether an accuracy from a model is statistically significant. When the model is trained on the COMPAS dataset with supervised learning and a validation accuracy has been obtained, it is important to know if the model performs better than random, which can be evaluated using a permutation test. A null hypothesis $H_{0}$, stating that the observed value comes from the distribution of randomized permutations, is set up and evaluated. To attempt to reject the null hypothesis, each input variable was shuffled randomly and the model was evaluated on the randomized data. This was done one thousand times and the accuracy from each of the permutations was compared to the original accuracy from the model that was trained on the non-permuted validation dataset. The thousand accuracies will form the distribution of random accuracies and the result from the neural network on the real data is statistically significant if the p-value is under the critical value, chosen to be equal to 0.05, since this allows for the null hypothesis to be rejected. This is intuitively equivalent to the accuracy being significantly far away from this distribution. 
	 This can be evaluated using the p-value. For a permutation test, the p-value is calculated using 
	\[p=\frac{B}{M},\]
	where \textit{B} is the number of extreme permutations (the permutations with values greater than or equal to the observed value) and \textit{M} is the total number of permutations. 
	
%	As for feature selection every feature was randomly shuffled with re-sampling. If the outputs remained unchanged regardless of the permutation the feature was dropped. Every feature is permuted 1000 times.\\\\
	%As for identification of biases in the binary classifier the categorical variable race was used. Permutation was implemented by randomly shuffling the attribute with re-sampling. The permuted data were used as input for the model and several confusion matrices were constructed and compared. The matrices were compared to the original confusion matrix, which in turn results in a \textit{p}-value on the percentage of false negatives (FN) on whites compared to African-Americans in the ProPublica data.
	
	\subsection{Bias Identification}\label{bias_id}
	To identify if the constructed classifier trained on the COMPAS dataset is fair or not, two different datasets are constructed. A validation-set which only contains the Caucasian race and a validation-set which only contains Afro-Americans. Both of the datasets were validated by the classifier and the confusion matrix of the two sets are compared. The big idea behind this split of the validation data is that it is possible to create the conditional distributions of the data that have to be compared, since their true positive and false positive rates should be equal in order for the classification to be fair. The split of the data allows for two different ROC-curves to be constructed, showing differing values for true positive rate and false positive rate values, which can be equated. This is a simplification of the process of the bias definitions used for bias correction methods, which have been explained in \ref{bias_def}. Furthermore, in the next section, \ref{biascorr}, a detailed explanation of how to fulfill these fairness definition will be given. 
	
	\subsection{Bias Correction Methods}\label{biascorr}
	
	The method that is being implemented in this project is introduced by Hard et al. \cite{equal_of_oppor} The proposed fairness implementations are done after the training process; so-called post-processing bias correction methods. Post-training, two confusion matrices are constructed from the validation datasets described in section \ref{bias_id}. Hard et al. proposes that in order for the model to be as fair as possible, it has to ensure equalized odds or equal opportunity depending on how much loss of accuracy one can allow in their model. Per definition, equalized odds entails equal opportunity. A predictor can always fulfill equalized odds by using a constant predictor, i.e. $ \hat Y = 1 $. However, the goal is a to make a sufficient predictor, which ensures non-discriminant behavior. To derive such a predictor, a score function is needed. The output node of the network has a value in $ R \in [0,1] $, instead of it being a binary input. Thus, this output value \textit{R}, will be considered as a score function. A way to obtain a binary predictor from this score function \textit{R} is by thresholding the value, where the prediction would be $ \hat Y = \mathbb I (R > \text{threshold}) $. If the score function then satisfies equalized odds or equality of opportunity, then the predictor will as well. From this score function, the optimal threshold should be chosen to ensure the strict definition of fairness \ref{bias_def}. If the score function does not satisfy this definition, then a unique threshold needs to be used for each protected group, which can be stated as $ \tilde Y = \mathbb I ( R > \text{threshold}_A)$, where A is the protected group. 
	
	A key in Hard et Al.'s study is the Receiver Operator Characteristic curve (ROC-curve) of the score function. The ROC-curve shows the relationship between the false positive rate and true positive rate at different threholds. These curves exist in a two dimensional plane with the true positive rate on the vertical axis the false positive rate on the horizontal axis. The ROC-curves that are considered is the A-conditional ROC-curves which are constructed from the data with the Caucasian and African-American protected groups respectively, 
	\begin{equation*}\label{key}
	C_{a}(t) \stackrel{\text { def }}{=}(\operatorname{Pr}\{\widehat{R}>t | A=a, Y=0\}, \operatorname{Pr}\{\widehat{R}>t | A=a, Y=1\})
	\end{equation*}
	The conditional curves both specify the conditional distributions of the Caucasian and African-Americans. Therefore, a score function obeys equalized odds if and only if the values of the protected attributes are equal. There are several scenarios to take into account, which are shown below. \cite{equal_of_oppor}
	\begin{enumerate}
		\item The simplest case is if the ROC-curves are identical or close to identical. If this is the case, then any thresholding of R yields an equalized odds predictor. As such, the threshold yielding the best accuracy can be chosen.
		\item In the case where the ROC-curves are not equal, then a different threshold needs to be chosen for each protected group. If the resulting predictor has to satisfy equalized odds then only points on the ROC-curves with equal false and true positive rates can be considered. This is equivalent to the two conditional ROC-curves intersecting. 
		\item It can happen that the two ROC-curves do not intersect at all, except for at the trivial endpoints, which carry with them low accuracy. In this case, randomization has to be used in order to obtain predictors which fulfill the definition of equalized odds. The randomization works by forcing the upper ROC-curve down unto the lower curve in order for the two curves to get the same true and false positive rate.
	\end{enumerate}
	It is also possible for the score function to obey equal opportunity, which is significantly easier to implement. The only necessity for equal opportunity is that the true positive rate should be equal, so every threshold on the ROC-curve is a valid option. Thus, a ternary search allows for optimization in order to find the best possible accuracy under the constraint of equal opportunity.
	
	\subsubsection{Derivation of an equalized odds threshold predictor} \label{optimalboi}
	A convex hull for the conditional ROC-curves is defined as, 
	\begin{equation*}\label{key}
	D_{a} \stackrel{\text { def }}{=} \text { convhull }\left\{C_{a}(t): t \in[0,1]\right\}
	\end{equation*}

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/convex_hull}
		\caption{The figure illustrates the intersection of the convex hulls of the conditional ROC-curves (the lower of the ROC-curves defines the border of the convex hull). The intersection is the gray area in the graph between the lower of the ROC-curves and the diagonal.}
		\label{fig:convexhull}
	\end{figure}
	\noindent
	A chosen point in the convex hull $ D_a $ represents the false and true positive rates, conditioned on a certain protected group of a randomized derived predictor based on the score function R. Due to the fact that the space of the conditional ROC-curves is two dimensional, a predictor $ \tilde Y $ can always be constructed as a mixture of two different threshold predictors. Conditioned on $ A=a $, the predictor will behave as, 
	\begin{equation*}\label{key}
	\widetilde{Y}=\mathbb{I}\left\{R>T_{a}\right\},
	\end{equation*}
	
	\noindent Where the variable $ T_a $ is the randomized threshold which assumes the value $ \underline{t}_{a} $ with a certain probability $ \underline{p}_a $ and the value $\bar t_{a} $ with probability $ \bar p_a $. To obtain an equalized odds predictor, a point in the intersection of the convex hull of each of the conditional ROC-curves has to be chosen and then for each group realize the true and false positive rates with a randomized predictor. For each group this results in either choosing a fixed threshold $ T_a = t_a $ or a mixture of two thresholds $ \underline t_a < t_a $. In the case of mixing two thresholds, if $ A=a $ and $ R < \underline t_a $, the predictor $ \tilde Y $ is set to 0, if $ R > \bar t_a $, the predictor is set to 1. But if $ \underline t_a < R < \bar t_a $, it is reversed and $ \tilde Y = 1 $ with probability $ \bar p_a$. Phrased in words, the sampled value, which is a random number between 0 and 1, decides whether to threshold at the upper or lower point.
	
	The equalized odds predictors is thus in the intersection of areas under the A-conditional ROC-curves, and above the main diagonal. This area is illustrated in figure \ref{fig:convexhull}. For any loss function the optimal false and true positive rate will always be on the upper left boundary. This is due to the fact that predictors close to the diagonal are no better than random. This upper left boundary is the possible equalized odds predictors. This ROC-curve, which is the upper left boundary of the intersection between the convex hulls is the point-wise minimum of the two A-conditional ROC-curves. The performance of a predictor that satisfies equalized odds is therefore determined by the minimum performance among the protected groups, Caucasians and African-Americans in this project. In other words, ensuring equalized odds enforces the trained model to build good prediction for all classes. For a given loss function, finding the optimal threshold can be done by optimizing ,
	\begin{equation}\label{optim_thresh}
	\min _{\forall a: \gamma \in D_{a}} \gamma_{0} \ell(1,0)+\left(1-\gamma_{1}\right) \ell(0,1)
	\end{equation}
	Assuming withouth loss of generality $ \ell (0,0) = \ell(1,1)=0 $.
	The optimization problem \ref{optim_thresh} can be done efficiently by numerically using ternary search. \cite{equal_of_oppor}
	\subsubsection{Derivation of an optimal equal opportunity predictor}
	
	The construction of an equal opportunity predictor follows the same approach as \ref{optimalboi}, but has one less constraint. From the definition of equal opportunity, the predictor must only satisfy that the true positive rates are equal. This corresponds to the A-conditional ROC-curves having the same y-coordinate in the two dimensional plane. Assuming continuity of the A-conditional ROC-curves it is always possible to find points on the boundary of the conditional ROC-curves. This means that no randomization is needed, unlike when deriving an equalized odds predictor. The optimal equality of opportunity predictor corresponds to two different deterministic thresholds for each of the protected groups. The optimization problem can be solved by using exhaustive ternary search over the true positive values. \cite{equal_of_oppor}
	
	\subsection{Reproducibility}
	This section aims to give the reader a precise insight into how this project have implemented the methods described and hopefully this will enlighten and simplify how to reproduce the results. All of the code is written in Python Version 3.7.4, PyTorch version 1.4.0 using Jupyter Notebook 6.0.3 and all of the code is available at, \url{https://github.com/oskarwiese/fagprojekt/blob/master/src/Classifier_notebook.ipynb}. Firstly, the dataset was loaded into a Pandas dataframe in order to structurize the data easily. Several plots were made using the Seaborn plot library, the code can be found in the Data Visualization cell. In order to pre-process the data for a neural network all of the features were converted into PyTorch tensors (the chosen features and order of features for this project can be seen in section \ref{Feed-forward neural}). All of the numerical variables are normalized in order to avoid exploding gradients. A FFNN was constructed using PyTorch NN module, see Neural Network cell in the notebook. The Binary Classifier was trained using BCE-loss with Adam as the chosen optimizer. All of the hyperparameters can be seen in section \ref{Feed-forward neural}. A permutation test was then executed to show that the found accuracy was accepted. The Classifier was used to construct predictions for conditional distributions which were saved in an array and used to construct confusion matrices at different thresholds. By using different thresholds, two  different conditional ROC-curves were constructed with Numpy and MatPlotLib. A plot of the TPR and FPR, for each of the protected groups, as a function of thresholds were constructed as well, the code can be seen in the first 2nd cell in the Classification Class, in the Biascorrection function. In order to implement equalized odds, an exhaustive search was conducted: For each point on the boundary of the intersection of the convex hulls for each of the conditional distributions, the true postive and false postive rate was found. From the point on the boundary, a line was drawn between this point and the point (0,0). The slope is found using $ m=\frac{\Delta y}{\Delta x} $. Then, the intersection between the upper curve and the line was determined. The distance from (0,0) to the point on the lower curve was found and divided by the length from (0,0) to the point of the upper curve. The distance between the point on the lower curve and the intersection of the line and the upper curve is determined as well and divided by the length from (0,0) to the intersection of the line and the upper curve. This will result in two values, which are referred to as $ \bar p_a $ and $ \underline p_a $ in section \ref{optimalboi}. These are values between zero and one which will sum to one, and decides the proportion of sampling from the randomized predictor. This was done for each point on the upper left boundary of the intersection between the convex hull of the two conditional ROC-curves. The code can be seen in the Biascorrection function in the notebook. As for the implementation of Equal Opportunity, it only requires that the TPR is equal for both populations. This was done by using exhaustive search, by choosing a TPR for one population, then subtracting this from the TPR-array for the other population and using \texttt{argmin} to find the one closest to zero, the implementation of Equal Opportunity can also be seen in the Biascorrection function. 
	
	\section{Results}\label{results}
	%\textbf{TODO \\ dokumentér reproducerbarheden af resultaterne \\ tabeller og figurer over resultater \\ forklar resultaterne endten i diskussion eller resultater.}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/loss_curve.png}
		\caption{The training loss and training accuracy after training for 400 epocs. It is clear, as expected, that the two curves are inversely proportional and that the accuracy rises with the number of epocs. After 400 epochs, the accuracy is close to having converged, but the loss still seems to get a bit lower.}
		\label{fig:losscurve}
	\end{figure}

	The random permutations and observed accuracy is shown below.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/perm_test.png}
		\caption{An illustration of the permutation test. The black dashed line illustrates the mean permuted accuracy, the red dashed line represents the distribution that the permuted data accuracies seem to follow and the blue dashed line is the accuracy of the model on the original non-permuted data. The standard deviation and mean represent normal distribution which is fitted to the permutations.}
		\label{fig:permtest}
	\end{figure}\noindent
	From figure \ref{fig:permtest} it is clear that the obtained accuracy is indeed significant and different from the accuracies of the permuted data. Since there are no permutations larger than the observed value, this would results in a p-value of 0, which is nonsensical. Thus, the p-value can instead be estimated by \cite{p-value}
	\[p=\frac{1}{M}=0.001,\]
	which is definitely significant. Hence, the null hypothesis is rejected and the permutation test thus shows that the accuracy from the neural network on the non-permuted data is better than random and has found a meaningful interpretation of the data. 


	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/fpr_tpr_plot.png}
		\caption{An illustration of the true and false positive rates as a function of the threshold. It is clear that, although the difference in the ROC-curves seems small in plot ***, the difference in thresholds causes the true and false positive rates between the two conditional ROC-curves to be different, showing inherent bias and difference between the conditional ROC-curves.}
		\label{fig:fprtprplot}
	\end{figure}
		
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{"imgs/Equal Opportunity Optimal"}
		\caption{An illustration of the optimal equal opportunity threshold on the entirety of the ROC-curves. It is clear that the middle of the ROC-curves seems to yield the best accuracy. It is also clear that a point has been chosen at which both the true and false positive rates are close to equal.}
		\label{fig:equal-opportunity-optimal}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{"imgs/Equalized Odds Optimal"}
		\caption{An illustration of the optimal equalized odds thresholds. The best accuracy is obtained in a point where the false and true positive rates are almost equal. To make them equal, the blue dottet line shows the possible values of the threshold when mixing the threshold of 1 with the threshold of the blue point. As such, it is possible to make the false and true positive rates exactly equal. It is also clear that equalized odds and equal opportunity bias corrections methods choose almost the same threshold.}
		\label{fig:equalized-odds-optimal}
	\end{figure}
	
	
	\section{Discussion} \label{discussion}
	%\textbf{TODO: \\ etisk diskussion omkring teknologien der er arbejdet med}
	
	
	
	\subsection{Ethical dilemma of bias}
	
	\subsection{Avoidance of responsibility}

	\subsection{The value alignment problem}
	
	\subsection{Safe AI}
		
	\subsection{Trade-off between accuracy and fairness}
	
	
	\section{Conclusion} \label{conclusion}
	%\textbf{TODO: \\ Opsummér resultater og anbefalinger fra projektet \\ Må ikke indeholde noget nyt \\ Skal svare på spørgsmålene fra problemformuleringen \\ Konklusion, abstract og indledning skal give samlet billede af projektet}
	
	\section{Appendix} \label{appendix}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/c_charge_degree}
		\caption{}
		\label{fig:cchargedegree}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{imgs/charge_degree_score}
		\caption{}
		\label{fig:chargedegreescore}
	\end{figure}
	
	\begin{thebibliography}{9} \label{bibliography}
		
		\bibitem{bo_lib} Machine Learning Group, University of Sheffield: "GPyOpt’s documentation", at \url{https://gpyopt.readthedocs.io}
		
		\bibitem{equal_of_oppor} M. Hardt, E. Price, and N. Srebro. Equality of Opportunity in Supervised Learning. In NIPS, 2016.
		
		\bibitem{Zafar} M. B. Zafar, I. Valera, M. G. Rodriguez, and K. P. Gummadi. Fairness constraints: A mechanism for fair classification.
		In ICML Workshop on Fairness, Accountability, and Transparency in Machine Learning, 2015.
		
		\bibitem{floridaLaw} The State of Florida, "The 2019 Florida Statutes", 1995,  \url{http://www.leg.state.fl.us/statutes/index.cfm?App_mode=Display_Statute&URL=0100-0199/0119/0119.html}, visited 12-03-2020
		
		\bibitem{propublicaAnalysis} ProPublica, "How We Analyzed the COMPAS Recidivism Algorithm", 2016, \url{https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}, visited 12-03-2020
		
		\bibitem{aktiv_gp} F. Bergamin. 02463 “Active Machine Learning and Agency” Lecture 1: Gaussian Processes, 2020.
		
		\bibitem{aktiv_bo} F. Bergamin. 02463 “Active Machine Learning and Agency” Lecture 2: Bayesian Optimization, 2020.
		
		\bibitem{dl} Goodfellow-et-al-2016. Deep Learning, MIT Press in 2016. 
		
		\bibitem{b_woodworth} B. Woodworth et al. Learning Non-Discriminatory Predictors, Toyota Technological Institute at Chicago, Chicago, IL 60637, USA; Proceedings of the 2017 Conference on Learning Theory.
		
		\bibitem{g_goh} G. Goh et al. Satisfying Real-world Goals with Dataset Constraints. In NIPS, 2016.
		
		\bibitem{p-value} Phipson, Belinda and K. Smyth, Gordon "Permutation P-values Should Never Be Zero: Calculating Exact P-values When Permutations Are Randomly Drawn", Walter and Eliza Hall Institute of Medical Research, 31. October 2010.
		
	\end{thebibliography}
	
	\newpage
	\bibliographystyle{IEEEbib}
	\bibliography{refs}
\end{document}
