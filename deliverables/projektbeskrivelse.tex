% !TeX spellcheck = en_US
\documentclass[11pt, fleqn]{article}
%\usepackage{siunitx}
\usepackage{texfiles/SpeedyGonzales}
\usepackage{texfiles/MediocreMike}
\title{Project Description: Fairness in Classification}
\author{\small Oskar Eiler Wiese Christensen s183917, Anders Henriksen s183904, Dagh Nielsen s183923}
\date{}
\begin{document}
	\maketitle
	\vspace*{-0.4cm}
	
	\noindent
	Safe AI is becomming ever increasingly important as researchers realize that the general public has a tendency to blindly trust the predictions of artificial intelligence. This has especially become evident in the US legal system, where judges have started using recidivism (the tendency to reoffend) classifiers to convict criminals, with the downside that african-americans had a tendency to get longer convictions for the same crime and a larger probabity of recidivism. While this could be a fault occurring from the general rampant racism in the US, the classification algorithm does not seem to improve the situation. The reason for the bias of the algorithm could be that the data used to train the classifer also has an inherent bias. Therefore, we wish to understand how bias comes to be in a dataset, how it is possible to quantify bias and what can be done to remove the bias from the data or from the algoritm. Meanwhile, we also wish to make an ethical discussion of using biased AI and which effect this can have on the population. The research questions for this project are outlined below. 
	
	The scope of this project is to implement a binary classifier in order to prove existing bias in the COMPASS data-set. A proof of concept for bias correction will then be demonstrated and evaluated. Hence, the goal of this project is to perform bias correction on a binary classifier trained to predict recidivism of criminals based on the trials of 11.757 americans, such that the classifier does not have a tendency to inprison more african-americans than necessary. We will quantify this by analyzing if african-americans get higher recidivism scores than white people for the same crime based both on the COMPASS algorithm recidivism scores and on our own classifier scores before and after the bias correction. The correction is succesful if african-americans get the same risk of recidivism as white people. The outcome of the project will therefore be to obtain a bias-free classifier.
	
	%\section*{Purpose}
	%\section*{Scope}
	%\section*{Success criteria}
	%\section*{Outcome}
	
	
	
	\section*{Research Questions}
	\begin{enumerate}
		\item How is the data "crooked"? What bias is there in the data?
		\begin{itemize}
			\item[-] How do we analyse whether there is bias in the data and which kind of bias there is? 
			\item[-] What does it mean that there is a bias in the data?
		\end{itemize}
		
		\item How does a bias in the data affect the classification algorithm
		\begin{itemize}
			\item[-] What does it mean? What is bias in an algorithm?
			\item[-] How do we quantify that there is in fact a bias in the algorithm?
		\end{itemize}
		
		\item Understand and implement a bias-correction method from the paper "Equality of Opportunity in Supervised Learning". 
			\begin{itemize}
				\item[-] If time allows for it, other bias correction methods will be taken into consideration and evaluated.
			\end{itemize}
		
		\item Which implications could biased algorithms have on society (ethical discussion)?
		
	\end{enumerate}
	
\end{document}