{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import GPyOpt\n",
    "s = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "#data = pd.read_csv(\"/home/oskar/Desktop/fagprojekt/compas/compas-scores-raw.csv\")\n",
    "url = \"https://raw.githubusercontent.com/oskarwiese/fagprojekt/master/compas/compas-scores-raw.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Til at se p√• dataen \n",
    "#print(data.head)\n",
    "#print(data.columns)\n",
    "\n",
    "# Check if there are any missing values\n",
    "print(np.count_nonzero(data[\"IsDeleted\"] == 1))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots():\n",
    "    # Show distribution of different ethnicities and sexes\n",
    "    chart = sb.countplot(x = \"Ethnic_Code_Text\", data = data)\n",
    "    chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    chart.set(xlabel='Ethnicity', ylabel='Count')\n",
    "    plt.show()\n",
    "    \n",
    "    chart = sb.countplot(x = \"Sex_Code_Text\", data = data)\n",
    "    chart.set(xlabel='Sex', ylabel='Count')\n",
    "    plt.show()\n",
    "    \n",
    "    sb.countplot(x = \"Language\", data = data)\n",
    "    plt.show()\n",
    "    \n",
    "    # Showing the distribution of the raw and decile values\n",
    "    plt.xlabel(\"Raw value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Visualization of the values\")\n",
    "    plt.hist(data[\"RawScore\"])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.xlabel(\"Decile value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Visualization of the decile values\")\n",
    "    plt.hist(data[\"DecileScore\"])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #sb.countplot(x = \"RawScore\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Indication that some black people might get higher sentences that white people\n",
    "    sb.countplot(x = \"DecileScore\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    plt.show()\n",
    "    \n",
    "    sb.countplot(x = \"ScoreText\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"Agency_Text\", \"Sex_Code_Text\", \"Ethnic_Code_Text\", \"ScaleSet_ID\", \"AssessmentReason\", \"Language\", \"LegalStatus\", \"CustodyStatus\", \"MaritalStatus\", \"RecSupervisionLevel\"]\n",
    "\n",
    "if s == 1:\n",
    "    ages = [None] * len(data[\"DateOfBirth\"])\n",
    "    for i in range(len(data[\"DateOfBirth\"])):\n",
    "        ages[i] = 20 +(100 - int(data[\"DateOfBirth\"][i].split(\"/\")[2]))\n",
    "    data[\"DateOfBirth\"] = ages\n",
    "    numericals = [\"DateOfBirth\"]\n",
    "    s+=1\n",
    "else:\n",
    "    pass\n",
    "\n",
    "outputs = [\"ScoreText\"]\n",
    "\n",
    "data = data.dropna(axis = 0, how = 'any')\n",
    "data[outputs] = data[outputs].replace('Low',0)\n",
    "data[outputs] = data[outputs].replace('Medium',1)\n",
    "data[outputs] = data[outputs].replace('High',1)\n",
    "data[outputs] = data[outputs].astype(\"category\")\n",
    "\n",
    "for category in categoricals:\n",
    "    data[category] = data[category].astype(\"category\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparing data for pytorch\n",
    "Xcat = []\n",
    "for i in range(len(categoricals)):\n",
    "    Xcat.append(data[categoricals[i]].cat.codes.values)\n",
    "Xcat = torch.tensor(Xcat , dtype = torch.int64).T\n",
    "\n",
    "#Converting the numerical values to a tensor\n",
    "Xnum = np.stack([data[col].values for col in numericals], 1)\n",
    "Xnum = torch.tensor(Xnum, dtype=torch.float)\n",
    "\n",
    "# Converting the output to tensor\n",
    "y = torch.tensor(data[outputs].values).flatten()\n",
    "\n",
    "# Calculation of embedding sizes for the categorical values in the format (unique categorical values, embedding size (dimension of encoding))\n",
    "categorical_column_sizes = [len(data[column].cat.categories) for column in categoricals]\n",
    "categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]\n",
    "\n",
    "# Train-test split\n",
    "totalnumber = len(Xnum)\n",
    "testnumber = int(totalnumber * 0.2)\n",
    "\n",
    "Xcattrain = Xcat[:totalnumber - testnumber]\n",
    "Xcattest = Xcat[totalnumber - testnumber:totalnumber]\n",
    "Xnumtrain = Xnum[:totalnumber - testnumber]\n",
    "Xnumtest = Xnum[totalnumber - testnumber:totalnumber]\n",
    "ytrain = y[:totalnumber - testnumber]\n",
    "ytest = y[totalnumber - testnumber:totalnumber]\n",
    "\n",
    "def normalize(data, norm_type):\n",
    "    if norm_type == \"minmax\":\n",
    "        for i in range(data.size()[1]):\n",
    "            data[:,i] = (data[:,i]-data[:,i].min()) / (data[:,i].max()-data[:,i].min())\n",
    "        return data\n",
    "    elif norm_type == \"zscore\":\n",
    "        for i in range(data.size()[1]):\n",
    "            data[:,i] = (data[:,i]-data[:,i].mean()) / (data[:,i].std())\n",
    "        return data\n",
    "    elif norm_type == None:\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(\"Please choose a correct normalization type\")\n",
    "        \n",
    "normalize(Xnumtrain, \"zscore\");\n",
    "normalize(Xnumtest, \"zscore\");\n",
    "\n",
    "#new_Xnumtrain = torch.tensor(np.vstack([(new_Xnumtrain[:,i]-new_Xnumtrain[:,i].min()) / (new_Xnumtrain[:,i].max()-new_Xnumtrain[:,i].min()) for i in range(new_Xnumtrain.size()[1]) if \"Tue elsker det her\"])).view(-1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols + num_numerical_cols\n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = torch.cat([x, x_numerical], 1)\n",
    "        x = self.layers(x)\n",
    "        return nn.functional.softmax(x, dim = -1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(4, 2)\n",
      "    (1): Embedding(2, 1)\n",
      "    (2): Embedding(9, 5)\n",
      "    (3): Embedding(2, 1)\n",
      "    (4): Embedding(1, 1)\n",
      "    (5): Embedding(2, 1)\n",
      "    (6): Embedding(5, 3)\n",
      "    (7): Embedding(6, 3)\n",
      "    (8): Embedding(7, 4)\n",
      "    (9): Embedding(4, 2)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.6, inplace=False)\n",
      "  (batch_norm_num): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=10, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "    (4): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.6, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.6, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.6, inplace=False)\n",
      "    (16): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define and show the model\n",
    "#model = Model(categorical_embedding_sizes, 1, 2, [8,16,32,64,128], p=0.6)\n",
    "model = Model(categorical_embedding_sizes, 1, 2, [10,20,20,10], p=0.6)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimization\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001 , weight_decay = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 0.73174411\n",
      "epoch:  26 loss: 0.71595758\n",
      "epoch:  51 loss: 0.69733477\n",
      "epoch:  76 loss: 0.67944568\n",
      "epoch: 100 loss: 0.6590335965\n",
      "[[1849  232]\n",
      " [ 603  443]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.89      0.82      2081\n",
      "           1       0.66      0.42      0.51      1046\n",
      "\n",
      "    accuracy                           0.73      3127\n",
      "   macro avg       0.71      0.66      0.67      3127\n",
      "weighted avg       0.72      0.73      0.72      3127\n",
      "\n",
      "0.7329708986248801\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 100\n",
    "aggregated_losses = []\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(Xcattrain, Xnumtrain)\n",
    "    single_loss = loss_function(y_pred, ytrain)\n",
    "    aggregated_losses.append(single_loss)\n",
    "    \n",
    "    if i % 25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "    \n",
    "\n",
    "\n",
    "# Plot the loss over epocs\n",
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val = model(Xcattest, Xnumtest)\n",
    "    loss = loss_function(y_val, ytest)\n",
    "\n",
    "\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "\n",
    "print(confusion_matrix(ytest,y_val))\n",
    "print(classification_report(ytest,y_val))\n",
    "print(accuracy_score(ytest, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times low scoretext is predicted:  1957\n",
      "Times medium scoretext is predicted:  1170\n",
      "Times high scoretext is predicted:  0\n",
      "Accuracy of the random forest model:  0.8132395267029101\n"
     ]
    }
   ],
   "source": [
    "# Define the model and fit it to the data\n",
    "forestModel = RandomForestClassifier(n_estimators = 53, max_depth = 14, max_features = \"log2\", criterion = \"entropy\")\n",
    "forestModel.fit(Xcattrain, ytrain)\n",
    "\n",
    "# Predict on the test set\n",
    "forestPreds = forestModel.predict(Xcattest)\n",
    "forestProbs = forestModel.predict_proba(Xcattest)[:, 1]\n",
    "\n",
    "print(\"Times low scoretext is predicted: \", len(forestPreds[forestPreds == 0]))\n",
    "print(\"Times medium scoretext is predicted: \", len(forestPreds[forestPreds == 1]))\n",
    "print(\"Times high scoretext is predicted: \", len(forestPreds[forestPreds == 2]))\n",
    "\n",
    "print(\"Accuracy of the random forest model: \", len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baysian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[290.   2.   0.   1.]]\n",
      "[[170.  73.   0.   1.]]\n",
      "[[160.  40.   1.   1.]]\n",
      "[[27. 62.  0.  0.]]\n",
      "[[55. 59.  1.  0.]]\n",
      "[[290.   2.   0.   1.]]\n",
      "[[289.   2.   0.   1.]]\n",
      "[[300.  54.   1.   0.]]\n",
      "[[249.   1.   0.   0.]]\n",
      "[[265.   1.   1.   0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[259.   1.   0.   1.]]\n",
      "[[232.   1.   1.   1.]]\n",
      "[[1. 1. 1. 1.]]\n",
      "[[32.  1.  1.  1.]]\n",
      "[[17.  1.  1.  0.]]\n",
      "[[65.  1.  0.  0.]]\n",
      "[[51.  1.  1.  0.]]\n",
      "[[289.   1.   1.   1.]]\n",
      "[[275.   1.   0.   1.]]\n",
      "[[215.   1.   0.   0.]]\n",
      "[[225.   1.   0.   0.]]\n",
      "[[92.  1.  1.  0.]]\n",
      "[[115.   1.   0.   0.]]\n",
      "[[103.   1.   0.   0.]]\n",
      "[[177.   1.   1.   0.]]\n",
      "[[193.   1.   1.   0.]]\n",
      "[[146.   1.   1.   0.]]\n",
      "[[109.   1.   1.   1.]]\n",
      "[[11.  1.  0.  1.]]\n",
      "[[1. 1. 0. 0.]]\n",
      "[[159.   1.   0.   1.]]\n",
      "[[133.   1.   0.   1.]]\n",
      "[[82.  1.  0.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[68.  1.  1.  1.]]\n",
      "[[184.   1.   0.   1.]]\n",
      "[[150.   1.   0.   0.]]\n",
      "[[204.   1.   1.   1.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[76.  1.  1.  0.]]\n",
      "[[263.   8.   0.   1.]]\n",
      "[[108. 109.   0.   0.]]\n",
      "[[229. 109.   0.   0.]]\n",
      "[[  1. 109.   1.   1.]]\n",
      "[[300. 109.   0.   1.]]\n",
      "[[ 1. 23.  1.  0.]]\n",
      "[[101.  25.   0.   0.]]\n",
      "[[ 53. 109.   0.   1.]]\n",
      "[[300.  13.   0.   1.]]\n",
      "[[201.  21.   1.   0.]]\n",
      "[[164. 109.   1.   1.]]\n",
      "[[51. 16.  0.  1.]]\n",
      "[[262.  82.   1.   0.]]\n",
      "[[139.  13.   0.   0.]]\n",
      "[[103.  71.   1.   1.]]\n",
      "[[125.   1.   1.   0.]]\n",
      "[[213.  72.   1.   1.]]\n",
      "[[167.   1.   1.   0.]]\n",
      "[[42.  1.  0.  1.]]\n",
      "[[39.  1.  1.  0.]]\n",
      "[[25.  1.  0.  0.]]\n",
      "[[255.   1.   1.   0.]]\n",
      "[[ 1. 75.  1.  1.]]\n",
      "[[214.   1.   1.   1.]]\n",
      "[[121.   1.   0.   1.]]\n",
      "[[240.   1.   0.   1.]]\n",
      "[[172.   1.   0.   1.]]\n",
      "[[59.  1.  1.  1.]]\n",
      "[[7. 1. 1. 0.]]\n",
      "[[144.   1.   0.   1.]]\n",
      "[[196. 109.   1.   0.]]\n",
      "[[152.   1.   1.   1.]]\n",
      "[[188.   1.   1.   1.]]\n",
      "[[54.  1.  0.  1.]]\n",
      "[[98.  1.  1.  1.]]\n",
      "[[237.   1.   1.   0.]]\n",
      "[[34.  1.  0.  0.]]\n",
      "[[275.   1.   1.   0.]]\n",
      "[[300.  79.   1.   1.]]\n",
      "[[209.   1.   1.   0.]]\n",
      "[[270.   1.   0.   0.]]\n",
      "[[247.   1.   1.   1.]]\n",
      "[[93.  1.  0.  1.]]\n",
      "[[196.   1.   0.   1.]]\n",
      "[[20.  1.  0.  1.]]\n",
      "[[265. 109.   1.   0.]]\n",
      "[[138.   1.   0.   0.]]\n",
      "[[165.   1.   1.   1.]]\n",
      "[[73.  1.  0.  1.]]\n",
      "[[137.   1.   1.   1.]]\n",
      "[[136.  82.   1.   1.]]\n",
      "[[47.  1.  1.  1.]]\n",
      "[[202.   1.   0.   0.]]\n",
      "[[270.   1.   1.   1.]]\n",
      "[[87.  1.  1.  1.]]\n",
      "[[243.   1.   1.   0.]]\n",
      "[[208.   1.   0.   1.]]\n",
      "[[118.   1.   1.   0.]]\n",
      "[[86.  1.  0.  0.]]\n",
      "[[188.   1.   0.   0.]]\n",
      "[[232.   1.   0.   0.]]\n",
      "[[107.   1.   1.   0.]]\n",
      "[[4. 1. 0. 1.]]\n",
      "[[132.   1.   1.   0.]]\n",
      "[[75. 87.  0.  0.]]\n",
      "[[15.  1.  0.  0.]]\n",
      "[[70.  1.  1.  0.]]\n",
      "[[158.   1.   1.   0.]]\n",
      "[[29. 91.  1.  1.]]\n",
      "[[172.   1.   1.   1.]]\n",
      "[[37.  1.  1.  1.]]\n",
      "[[221.   1.   0.   1.]]\n",
      "[[58.  1.  0.  0.]]\n",
      "[[63.  1.  1.  0.]]\n",
      "[[180.   1.   1.   1.]]\n",
      "[[253.   1.   0.   1.]]\n",
      "[[280.   1.   0.   0.]]\n",
      "[[47.  1.  0.  0.]]\n",
      "[[184.   1.   1.   0.]]\n",
      "[[97.  1.  0.  0.]]\n",
      "[[226.   1.   1.   1.]]\n",
      "[[25.  1.  1.  1.]]\n",
      "[[128.   1.   0.   1.]]\n",
      "[[82.  1.  1.  0.]]\n",
      "[[163.   1.   0.   0.]]\n",
      "[[115.   1.   1.   1.]]\n",
      "[[280.   1.   1.   1.]]\n",
      "[[29.  1.  0.  1.]]\n",
      "[[199.   1.   1.   0.]]\n",
      "[[265.   1.   0.   1.]]\n",
      "[[229.   1.   1.   0.]]\n",
      "[[14.  1.  1.  1.]]\n",
      "[[126.   1.   0.   0.]]\n",
      "[[142.   1.   0.   0.]]\n",
      "[[62.  1.  0.  1.]]\n",
      "[[245.   1.   0.   1.]]\n",
      "[[78.  1.  1.  1.]]\n",
      "[[261.   1.   1.   1.]]\n",
      "[[9. 1. 0. 0.]]\n",
      "[[261.   1.   0.   0.]]\n",
      "[[206.   1.   0.   0.]]\n",
      "[[170.   1.   0.   0.]]\n",
      "[[199.   1.   1.   1.]]\n",
      "[[190.  53.   0.   0.]]\n",
      "[[177.   1.   0.   1.]]\n",
      "[[285.   1.   1.   0.]]\n",
      "[[180.  11.   1.   1.]]\n",
      "[[136.  54.   0.   0.]]\n",
      "[[104.   1.   1.   1.]]\n",
      "[[78.  1.  0.  0.]]\n",
      "[[44.  1.  1.  0.]]\n",
      "[[112.   1.   1.   0.]]\n",
      "[[235.   1.   0.   1.]]\n",
      "[[284.   1.   0.   1.]]\n",
      "[[254.   1.   1.   1.]]\n",
      "[[55.  1.  1.  0.]]\n",
      "[[229.   1.   0.   1.]]\n",
      "[[162.   1.   1.   0.]]\n",
      "[[240.   1.   0.   0.]]\n",
      "[[146.   1.   0.   0.]]\n",
      "[[138. 107.   1.   0.]]\n",
      "[[180.   1.   0.   0.]]\n",
      "[[107.   1.   0.   1.]]\n",
      "[[89.  1.  0.  1.]]\n",
      "[[154.   1.   0.   1.]]\n",
      "[[174.   1.   0.   0.]]\n",
      "[[116.   1.   0.   1.]]\n",
      "[[125.   1.   0.   1.]]\n",
      "[[29.  1.  1.  0.]]\n",
      "[[284.  68.   1.   0.]]\n",
      "[[239.   1.   1.   1.]]\n",
      "[[148.   1.   1.   1.]]\n",
      "[[143.   1.   1.   1.]]\n",
      "[[288.   1.   0.   0.]]\n",
      "[[127.   1.   1.   1.]]\n",
      "[[37.  1.  0.  1.]]\n",
      "[[218.   1.   1.   1.]]\n",
      "[[2. 1. 1. 0.]]\n",
      "[[110.   1.   0.   0.]]\n",
      "[[22.  1.  1.  0.]]\n",
      "[[68.  1.  0.  0.]]\n",
      "[[4. 1. 1. 1.]]\n",
      "[[15.  1.  0.  1.]]\n",
      "[[84. 50.  0.  1.]]\n",
      "[[242.   1.   1.   1.]]\n",
      "[[290.   2.   1.   0.]]\n",
      "[[168.   1.   0.   1.]]\n",
      "[[11.  1.  1.  0.]]\n",
      "[[130.   1.   0.   0.]]\n",
      "[[ 82. 109.   1.   0.]]\n",
      "[[211.   1.   0.   0.]]\n",
      "[[51.  1.  0.  1.]]\n",
      "[[73.  1.  1.  1.]]\n",
      "[[21.  1.  1.  1.]]\n",
      "[[8. 1. 0. 1.]]\n",
      "[[191.   1.   0.   1.]]\n",
      "[[96.  1.  1.  1.]]\n",
      "[[101.   1.   1.   0.]]\n",
      "[[270.   1.   1.   0.]]\n",
      "[[266.   1.   1.   1.]]\n",
      "[[257.   1.   1.   1.]]\n",
      "[[233.  87.   1.   0.]]\n",
      "[[203.   1.   1.   0.]]\n",
      "[[223.   1.   1.   1.]]\n",
      "[[250.   1.   0.   1.]]\n",
      "[[42.  1.  0.  0.]]\n",
      "[[156.   1.   1.   1.]]\n",
      "[[214.   1.   1.   0.]]\n",
      "[[89.  1.  1.  0.]]\n",
      "[[175.   1.   1.   1.]]\n",
      "[[139.   1.   1.   0.]]\n",
      "[[153.   1.   1.   0.]]\n",
      "[[140.   1.   0.   1.]]\n",
      "[[73.  1.  0.  0.]]\n",
      "[[197.   1.   0.   0.]]\n",
      "[[196.   1.   1.   1.]]\n",
      "[[171.   1.   1.   0.]]\n",
      "[[ 24. 109.   1.   0.]]\n",
      "[[258.   1.   1.   0.]]\n",
      "[[42.  1.  1.  1.]]\n",
      "[[196.  87.   1.   0.]]\n",
      "[[45.  1.  0.  1.]]\n",
      "[[119.   1.   1.   1.]]\n",
      "[[32.  1.  0.  0.]]\n",
      "[[250.   1.   1.   0.]]\n",
      "[[20.  3.  0.  1.]]\n",
      "[[ 1. 48.  1.  0.]]\n",
      "[[78. 14.  1.  0.]]\n",
      "[[90.  1.  1.  1.]]\n",
      "[[161.   1.   1.   1.]]\n",
      "[[157.   1.   0.   0.]]\n",
      "[[280.  92.   0.   0.]]\n",
      "[[101.   1.   0.   1.]]\n",
      "[[217.   1.   0.   1.]]\n",
      "[[92.  1.  0.  0.]]\n",
      "[[ 6. 90.  0.  0.]]\n",
      "[[225.   1.   1.   0.]]\n",
      "[[54.  1.  1.  1.]]\n",
      "[[25.  1.  0.  1.]]\n",
      "[[186.   1.   0.   1.]]\n",
      "[[83.  1.  1.  1.]]\n",
      "[[277.   1.   1.   1.]]\n",
      "[[53.  1.  0.  0.]]\n",
      "[[133.   1.   1.   1.]]\n",
      "[[118.   1.   0.   0.]]\n",
      "[[273.   1.   0.   0.]]\n",
      "[[279.   1.   1.   0.]]\n",
      "[[58.  1.  0.  1.]]\n",
      "[[66.  1.  0.  1.]]\n",
      "[[254.   1.   0.   0.]]\n",
      "[[184.   1.   1.   1.]]\n",
      "[[219.  49.   0.   0.]]\n",
      "[[6. 1. 0. 0.]]\n",
      "[[286.   1.   1.   1.]]\n",
      "[[112.   1.   0.   1.]]\n",
      "[[64.  1.  1.  1.]]\n",
      "[[21.  1.  0.  0.]]\n",
      "[[33.  1.  1.  0.]]\n",
      "[[204.   1.   0.   1.]]\n",
      "[[235.   1.   1.   1.]]\n",
      "[[283.   1.   1.   0.]]\n",
      "[[110.  45.   1.   1.]]\n",
      "[[236.   1.   0.   0.]]\n",
      "[[154.  96.   0.   0.]]\n",
      "[[288.   4.   1.   1.]]\n",
      "[[300.  73.   0.   0.]]\n",
      "[[229.  16.   0.   0.]]\n",
      "[[246.  58.   0.   1.]]\n",
      "[[113.   9.   1.   1.]]\n",
      "[[300.   4.   0.   0.]]\n",
      "[[51. 82.  0.  0.]]\n",
      "[[39. 42.  0.  1.]]\n",
      "[[154.  62.   1.   0.]]\n",
      "[[115.  89.   0.   0.]]\n",
      "[[211.   6.   0.   1.]]\n",
      "[[241.  35.   0.   0.]]\n",
      "[[198.  37.   1.   0.]]\n",
      "[[63. 39.  0.  0.]]\n",
      "[[266.  56.   0.   0.]]\n",
      "[[177.  94.   1.   0.]]\n",
      "[[289.   1.   1.   0.]]\n",
      "[[158.   8.   1.   0.]]\n",
      "[[247. 109.   1.   1.]]\n",
      "[[76. 68.  1.  0.]]\n",
      "[[94. 91.  1.  1.]]\n",
      "[[243.   7.   1.   0.]]\n",
      "[[214.  96.   1.   1.]]\n",
      "[[122.  66.   0.   1.]]\n",
      "[[222.  33.   1.   1.]]\n",
      "[[173.  52.   1.   1.]]\n",
      "[[131.  36.   0.   1.]]\n",
      "[[19. 41.  0.  1.]]\n",
      "[[190.  70.   1.   0.]]\n",
      "[[300.  36.   0.   1.]]\n",
      "[[280. 109.   1.   1.]]\n",
      "[[235.  68.   0.   0.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[62.  5.  0.  0.]]\n",
      "[[92.  7.  1.  1.]]\n",
      "[[84. 33.  1.  0.]]\n",
      "[[39. 25.  1.  0.]]\n",
      "[[18. 76.  0.  1.]]\n",
      "[[194.   1.   0.   0.]]\n",
      "[[123. 109.   1.   1.]]\n",
      "[[181.  34.   0.   1.]]\n",
      "[[122.   1.   1.   0.]]\n",
      "[[76.  1.  0.  1.]]\n",
      "[[290.   2.   0.   0.]]\n",
      "[[96.  1.  1.  0.]]\n",
      "[[39. 73.  1.  1.]]\n",
      "[[70.  1.  0.  1.]]\n",
      "[[282.  49.   0.   1.]]\n",
      "[[1. 6. 1. 0.]]\n",
      "[[278.   1.   0.   1.]]\n",
      "[[1. 4. 0. 0.]]\n",
      "[[ 1. 12.  1.  0.]]\n",
      "[[262.  38.   1.   1.]]\n",
      "[[291.   2.   1.   0.]]\n",
      "[[ 1. 34.  1.  1.]]\n",
      "[[40.  7.  1.  1.]]\n",
      "[[154.  79.   1.   1.]]\n",
      "[[249.  92.   0.   0.]]\n",
      "[[180. 109.   1.   0.]]\n",
      "[[ 68. 109.   0.   1.]]\n",
      "[[288.   1.   0.   1.]]\n",
      "[[ 38. 109.   0.   0.]]\n",
      "[[ 9. 61.  1.  1.]]\n",
      "[[212. 109.   1.   1.]]\n",
      "[[300.  94.   1.   0.]]\n",
      "[[52. 30.  0.  0.]]\n",
      "[[205.  57.   1.   1.]]\n",
      "[[211.   1.   1.   1.]]\n",
      "[[98. 56.  0.  0.]]\n",
      "[[250.  73.   1.   1.]]\n",
      "[[117.  30.   1.   1.]]\n",
      "[[277.   1.   0.   0.]]\n",
      "[[129.   5.   1.   1.]]\n",
      "[[146.  31.   0.   0.]]\n",
      "[[193.   5.   0.   0.]]\n",
      "[[6. 1. 1. 1.]]\n",
      "[[267.   1.   0.   0.]]\n",
      "[[135.   1.   1.   0.]]\n",
      "[[234.  49.   1.   1.]]\n",
      "[[60. 94.  1.  1.]]\n",
      "[[37.  1.  0.  0.]]\n",
      "[[213.   1.   0.   1.]]\n",
      "[[88. 76.  0.  1.]]\n",
      "[[163.   1.   0.   1.]]\n",
      "[[63. 74.  0.  1.]]\n",
      "[[97. 39.  1.  0.]]\n",
      "[[284.   1.   1.   1.]]\n",
      "[[224.   1.   0.   1.]]\n",
      "[[169.   1.   1.   1.]]\n",
      "[[69. 53.  1.  0.]]\n",
      "[[222.   1.   0.   0.]]\n",
      "[[1. 7. 1. 1.]]\n",
      "[[19. 27.  1.  0.]]\n",
      "[[279.  34.   0.   0.]]\n",
      "[[270.  71.   0.   1.]]\n",
      "[[139.  68.   0.   0.]]\n",
      "[[ 95. 109.   1.   0.]]\n",
      "[[122.  51.   1.   0.]]\n",
      "[[151. 109.   1.   0.]]\n",
      "[[168.  26.   0.   0.]]\n",
      "[[130.  95.   0.   0.]]\n",
      "[[70. 27.  0.  1.]]\n",
      "[[44. 94.  1.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[1. 1. 1. 0.]]\n",
      "[[81.  1.  1.  0.]]\n",
      "[[52.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[70.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[147.  46.   1.   1.]]\n",
      "[[41. 57.  0.  1.]]\n",
      "[[149.   1.   1.   0.]]\n",
      "[[256.   1.   0.   1.]]\n",
      "[[209.  31.   0.   1.]]\n",
      "[[207.   1.   1.   1.]]\n",
      "[[187.   1.   1.   0.]]\n",
      "[[160.   1.   0.   0.]]\n",
      "[[123.   1.   0.   0.]]\n",
      "[[251.  46.   0.   0.]]\n",
      "[[136.   1.   0.   1.]]\n",
      "[[265.  95.   1.   1.]]\n",
      "[[18.  1.  0.  0.]]\n",
      "[[220.  83.   0.   0.]]\n",
      "[[28.  1.  1.  1.]]\n",
      "[[234.   1.   1.   0.]]\n",
      "[[223.  62.   1.   1.]]\n",
      "[[182.  81.   0.   1.]]\n",
      "[[130.   1.   1.   1.]]\n",
      "[[ 14. 101.   0.   1.]]\n",
      "[[185.   1.   0.   0.]]\n",
      "[[48.  1.  0.  1.]]\n",
      "[[165.  87.   1.   0.]]\n",
      "[[149.   1.   0.   1.]]\n",
      "[[285.  81.   0.   1.]]\n",
      "[[181.   1.   1.   0.]]\n",
      "[[190.   1.   1.   0.]]\n",
      "[[100.   1.   0.   0.]]\n",
      "[[121.  78.   1.   1.]]\n",
      "[[273.   1.   1.   1.]]\n",
      "[[85.  1.  0.  1.]]\n",
      "[[193.   1.   1.   1.]]\n",
      "[[174.   1.   1.   0.]]\n",
      "[[178.   1.   1.   1.]]\n",
      "[[201.   1.   1.   1.]]\n",
      "[[39.  1.  0.  1.]]\n",
      "[[246.   1.   0.   0.]]\n",
      "[[101.   1.   1.   1.]]\n",
      "[[10.  1.  1.  1.]]\n",
      "[[228.   1.   0.   0.]]\n",
      "[[264.   1.   0.   0.]]\n",
      "[[257.   1.   0.   0.]]\n",
      "[[23.  1.  0.  1.]]\n",
      "[[131.   1.   0.   1.]]\n",
      "[[61.  1.  1.  1.]]\n",
      "[[123.   1.   1.   1.]]\n",
      "[[66.  1.  1.  0.]]\n",
      "[[112.   1.   1.   1.]]\n",
      "[[202.  76.   0.   1.]]\n",
      "[[243.   1.   0.   0.]]\n",
      "[[218.   1.   0.   0.]]\n",
      "[[285.   1.   0.   0.]]\n",
      "[[134.   1.   0.   0.]]\n",
      "[[18.  1.  1.  1.]]\n",
      "[[181.   1.   0.   1.]]\n",
      "[[182.   1.   0.   0.]]\n",
      "[[33.  1.  0.  1.]]\n",
      "[[262.   1.   1.   0.]]\n",
      "[[216.   1.   1.   0.]]\n",
      "[[196.   1.   1.   0.]]\n",
      "[[52. 46.  1.  1.]]\n",
      "[[85.  1.  1.  0.]]\n",
      "[[179.  63.   0.   0.]]\n",
      "[[48.  1.  1.  0.]]\n",
      "[[200.   1.   0.   1.]]\n",
      "[[128.   1.   1.   0.]]\n",
      "[[111.  60.   1.   1.]]\n",
      "[[ 1. 63.  0.  0.]]\n",
      "[[96.  1.  0.  1.]]\n",
      "[[3. 1. 0. 0.]]\n",
      "[[252.   1.   1.   0.]]\n",
      "[[61.  1.  0.  0.]]\n",
      "[[237.  99.   0.   1.]]\n",
      "[[262.   1.   0.   1.]]\n",
      "[[165.   1.   1.   0.]]\n",
      "[[231.   1.   1.   0.]]\n",
      "[[83.  1.  0.  0.]]\n",
      "[[264.   1.   1.   1.]]\n",
      "[[246.   1.   1.   0.]]\n",
      "[[19.  1.  1.  0.]]\n",
      "[[28.  1.  0.  0.]]\n",
      "[[251.   1.   1.   1.]]\n",
      "[[28. 49.  1.  1.]]\n",
      "[[104.   1.   0.   1.]]\n",
      "[[106.   1.   0.   0.]]\n",
      "[[29. 34.  1.  0.]]\n",
      "[[44.  1.  0.  0.]]\n",
      "[[272.  44.   0.   0.]]\n",
      "[[81. 97.  1.  1.]]\n",
      "[[251.   1.   0.   0.]]\n",
      "[[190.  98.   0.   1.]]\n",
      "[[26.  1.  1.  0.]]\n",
      "[[143.   1.   1.   0.]]\n",
      "[[190.  26.   1.   0.]]\n",
      "[[153.   1.   0.   0.]]\n",
      "[[166.   1.   0.   0.]]\n",
      "[[94.  1.  0.  0.]]\n",
      "[[238.   1.   0.   1.]]\n",
      "[[59.  1.  1.  0.]]\n",
      "[[206.   1.   1.   0.]]\n",
      "[[247.   1.   0.   1.]]\n",
      "[[75.  1.  1.  1.]]\n",
      "[[88. 63.  0.  1.]]\n",
      "[[191.   1.   0.   0.]]\n",
      "[[269.   1.   0.   1.]]\n",
      "[[140.   1.   1.   1.]]\n",
      "[[44.  1.  1.  1.]]\n",
      "[[105.  98.   1.   1.]]\n",
      "[[10. 33.  1.  0.]]\n",
      "[[212.   1.   1.   0.]]\n",
      "[[79.  1.  0.  1.]]\n",
      "[[108.   1.   0.   0.]]\n",
      "[[151.   1.   0.   1.]]\n",
      "[[36.  1.  1.  0.]]\n",
      "[[254.  27.   0.   0.]]\n",
      "[[ 1. 98.  1.  0.]]\n",
      "[[3. 6. 1. 0.]]\n",
      "[[227.   4.   1.   1.]]\n",
      "[[234.  25.   0.   1.]]\n",
      "[[104.  83.   0.   0.]]\n",
      "[[116.   1.   1.   0.]]\n",
      "[[143.  93.   1.   1.]]\n",
      "[[230.   1.   0.   0.]]\n",
      "[[272.   1.   0.   1.]]\n",
      "[[258.  64.   1.   0.]]\n",
      "[[290.  41.   0.   0.]]\n",
      "[[158.   1.   1.   1.]]\n",
      "[[50.  1.  1.  1.]]\n",
      "[[300.  25.   1.   0.]]\n",
      "[[161.  53.   0.   0.]]\n",
      "[[241.   1.   1.   0.]]\n",
      "[[171.   4.   0.   0.]]\n",
      "[[146.   4.   0.   1.]]\n",
      "[[134.  24.   1.   0.]]\n",
      "[[290. 101.   0.   0.]]\n",
      "[[268.   1.   1.   0.]]\n",
      "[[203.  98.   0.   0.]]\n",
      "[[75. 40.  1.  1.]]\n",
      "[[232.   1.   0.   1.]]\n",
      "[[155.   1.   1.   0.]]\n",
      "[[145.   1.   1.   1.]]\n",
      "[[13.  1.  0.  0.]]\n",
      "[[14.  1.  1.  0.]]\n",
      "[[30. 80.  0.  0.]]\n",
      "[[110.   1.   0.   1.]]\n",
      "[[219.  23.   1.   0.]]\n",
      "[[16.  1.  1.  1.]]\n",
      "[[174.   1.   0.   1.]]\n",
      "[[155.  24.   1.   1.]]\n",
      "[[147.   1.   0.   1.]]\n",
      "[[177.   1.   0.   0.]]\n",
      "[[207.  45.   0.   0.]]\n",
      "[[16. 52.  0.  0.]]\n",
      "[[151.   1.   1.   0.]]\n",
      "[[204.   1.   0.   0.]]\n",
      "[[225.  97.   0.   0.]]\n",
      "[[ 11. 109.   1.   0.]]\n",
      "[[222.   1.   1.   0.]]\n",
      "[[99.  1.  0.  1.]]\n",
      "[[168.   1.   0.   0.]]\n",
      "[[73.  1.  1.  0.]]\n",
      "[[193.   1.   0.   1.]]\n",
      "[[191.   1.   1.   1.]]\n",
      "[[57.  1.  1.  1.]]\n",
      "[[35.  1.  1.  1.]]\n",
      "[[206.   1.   0.   1.]]\n",
      "[[186.   1.   1.   1.]]\n",
      "[[155.   1.   0.   0.]]\n",
      "[[291.  60.   0.   1.]]\n",
      "[[104.   1.   1.   0.]]\n",
      "[[189.   1.   0.   1.]]\n",
      "[[18. 88.  1.  0.]]\n",
      "[[89.  1.  0.  0.]]\n",
      "[[160.   1.   1.   0.]]\n",
      "[[171.  40.   0.   0.]]\n",
      "[[220.   1.   1.   1.]]\n",
      "[[29. 18.  1.  0.]]\n",
      "[[239.   1.   1.   0.]]\n",
      "[[17.  1.  0.  1.]]\n",
      "[[51. 70.  0.  1.]]\n",
      "[[107.  34.   0.   1.]]\n",
      "[[230.   1.   1.   1.]]\n",
      "[[165.   1.   0.   1.]]\n",
      "[[41.  1.  1.  0.]]\n",
      "[[71.  1.  0.  0.]]\n",
      "[[245.   1.   1.   1.]]\n",
      "[[120.   1.   0.   0.]]\n",
      "[[56.  1.  0.  0.]]\n",
      "[[274.  82.   1.   0.]]\n",
      "[[87.  1.  0.  1.]]\n",
      "[[267.  28.   0.   0.]]\n",
      "[[93.  1.  1.  1.]]\n",
      "[[113.   1.   0.   0.]]\n",
      "[[142.   1.   0.   1.]]\n",
      "[[227.  75.   0.   1.]]\n",
      "[[255. 102.   1.   1.]]\n",
      "[[210.   1.   0.   1.]]\n",
      "[[86. 23.  0.  1.]]\n",
      "[[114.   1.   1.   0.]]\n",
      "[[161.   1.   0.   1.]]\n",
      "[[11.  1.  0.  0.]]\n",
      "[[242.  80.   0.   0.]]\n",
      "[[80.  1.  1.  1.]]\n",
      "[[1. 1. 0. 1.]]\n",
      "[[282.   1.   0.   0.]]\n",
      "[[167.  99.   0.   1.]]\n",
      "[[81.  1.  0.  0.]]\n",
      "[[166.  63.   1.   1.]]\n",
      "[[2. 1. 0. 1.]]\n",
      "[[209.   1.   1.   1.]]\n",
      "[[118. 100.   1.   0.]]\n",
      "[[28.  4.  1.  0.]]\n",
      "[[ 9. 71.  1.  0.]]\n",
      "[[201.   1.   1.   0.]]\n",
      "[[120.  39.   0.   0.]]\n",
      "[[ 91. 100.   0.   0.]]\n",
      "[[30.  1.  0.  0.]]\n",
      "[[66. 63.  0.  1.]]\n",
      "[[144.   1.   0.   0.]]\n",
      "[[70. 99.  0.  0.]]\n",
      "[[260.   1.   1.   0.]]\n",
      "[[292.  88.   1.   0.]]\n",
      "[[121.   1.   1.   1.]]\n",
      "[[79.  1.  1.  0.]]\n",
      "[[40.  1.  0.  0.]]\n",
      "[[157.   1.   0.   1.]]\n",
      "[[202.  65.   0.   0.]]\n",
      "[[ 31. 100.   0.   0.]]\n",
      "[[ 1. 32.  1.  0.]]\n",
      "[[130.   1.   1.   0.]]\n",
      "[[135.   1.   1.   1.]]\n",
      "[[242.   1.   0.   1.]]\n",
      "[[179.   1.   0.   1.]]\n",
      "[[167.   1.   1.   1.]]\n",
      "[[60.  1.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "## define the domain of the considered parameters\n",
    "n_estimators = tuple(np.arange(1,301,1, dtype= np.int))\n",
    "# print(n_estimators)\n",
    "max_depth = tuple(np.arange(1,110,1, dtype= np.int))\n",
    "# max_features = ('log2', 'sqrt', None)\n",
    "max_features = (0, 1)\n",
    "# criterion = ('gini', 'entropy')\n",
    "criterion = (0, 1)\n",
    "\n",
    "\n",
    "# define the dictionary for GPyOpt\n",
    "domain = [{'n_estimators': 'var_1',  'type': 'discrete',     'domain': n_estimators},\n",
    "          {'max_depth': 'var_2',     'type': 'discrete',     'domain': max_depth},\n",
    "          {'max_features': 'var_3',  'type': 'categorical',  'domain': max_features},\n",
    "          {'criterion': 'var_4',     'type': 'categorical',  'domain': criterion}]\n",
    "\n",
    "\n",
    "## we have to define the function we want to maximize --> validation accuracy, \n",
    "## note it should take a 2D ndarray but it is ok that it assumes only one point\n",
    "## in this setting\n",
    "def objective_function(x): \n",
    "    print(x)\n",
    "    # we have to handle the categorical variables that is convert 0/1 to labels\n",
    "    # log2/sqrt and gini/entropy\n",
    "    \n",
    "    param = x[0]\n",
    "    \n",
    "    if param[2] == 0:\n",
    "        var_3 = \"log2\"\n",
    "    else:\n",
    "        var_3 = \"sqrt\"\n",
    "    \n",
    "    if param[3] == 0:\n",
    "        var_4 = \"gini\"\n",
    "    else:\n",
    "        var_4 = \"entropy\"\n",
    "        \n",
    "        \n",
    "#fit the model\n",
    "    model = RandomForestClassifier(n_estimators = int(param[0]), criterion = var_4, max_depth = int(param[1]), max_features = var_3)\n",
    "    model.fit(Xcattrain, ytrain)\n",
    "    forestPreds = model.predict(Xcattest)\n",
    "    accuracy = len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "opt = GPyOpt.methods.BayesianOptimization(f = objective_function,   # function to optimize\n",
    "                                              domain = domain,         # box-constrains of the problem\n",
    "                                              acquisition_type = \"EI\",      # Select acquisition function MPI, EI, LCB\n",
    "                                             )\n",
    "opt.acquisition.exploration_weight=.1\n",
    "\n",
    "opt.run_optimization(max_iter = 100) \n",
    "\n",
    "\n",
    "x_best = opt.X[np.argmin(opt.Y)]\n",
    "print(\"The best parameters obtained: n_estimators=\" + str(x_best[0]) + \", max_depth=\" + str(x_best[1]) + \", max_features=\" + str(\n",
    "    x_best[2])  + \", criterion=\" + str(\n",
    "    x_best[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
