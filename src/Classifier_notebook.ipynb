{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import GPyOpt\n",
    "s = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "#data = pd.read_csv(\"/home/oskar/Desktop/fagprojekt/compas/compas-scores-raw.csv\")\n",
    "url = \"https://raw.githubusercontent.com/oskarwiese/fagprojekt/master/compas/compas-scores-raw.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/oskarwiese/fagprojekt/master/compas_propublica/compas-scores-two-years.csv\"\n",
    "new_data = pd.read_csv(url)\n",
    "# Til at se p√• dataen \n",
    "#print(data.head)\n",
    "#print(data.columns)\n",
    "\n",
    "# Check if there are any missing values\n",
    "print(np.count_nonzero(data[\"IsDeleted\"] == 1))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'name', 'first', 'last', 'compas_screening_date', 'sex', 'dob',\n",
      "       'age', 'age_cat', 'race', 'juv_fel_count', 'decile_score',\n",
      "       'juv_misd_count', 'juv_other_count', 'priors_count',\n",
      "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
      "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas',\n",
      "       'c_charge_degree', 'c_charge_desc', 'is_recid', 'r_case_number',\n",
      "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
      "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
      "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
      "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
      "       'decile_score.1', 'score_text', 'screening_date',\n",
      "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
      "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
      "       'start', 'end', 'event', 'two_year_recid'],\n",
      "      dtype='object')\n",
      "0        NaN\n",
      "1       (F3)\n",
      "2       (M1)\n",
      "3        NaN\n",
      "4        NaN\n",
      "        ... \n",
      "7209     NaN\n",
      "7210     NaN\n",
      "7211     NaN\n",
      "7212     NaN\n",
      "7213    (M2)\n",
      "Name: r_charge_degree, Length: 7214, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(new_data.columns)\n",
    "print(new_data[\"r_charge_degree\"])\n",
    "def is_plot():\n",
    "    sb.countplot(x = \"score_text\", hue = \"race\", data = new_data)\n",
    "    plt.show()\n",
    "\n",
    "    sb.countplot(x = \"two_year_recid\", hue = \"race\", data = new_data)\n",
    "    plt.show()\n",
    "    sb.countplot(x = \"is_recid\", hue = \"race\", data = new_data)\n",
    "    plt.show()\n",
    "    sb.countplot(x = \"is_violent_recid\", hue = \"race\", data = new_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots():\n",
    "    # Show distribution of different ethnicities and sexes\n",
    "    chart = sb.countplot(x = \"Ethnic_Code_Text\", data = data)\n",
    "    chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    chart.set(xlabel='Ethnicity', ylabel='Count')\n",
    "    plt.show()\n",
    "    \n",
    "    chart = sb.countplot(x = \"Sex_Code_Text\", data = data)\n",
    "    chart.set(xlabel='Sex', ylabel='Count')\n",
    "    plt.show()\n",
    "    \n",
    "    sb.countplot(x = \"Language\", data = data)\n",
    "    plt.show()\n",
    "    \n",
    "    # Showing the distribution of the raw and decile values\n",
    "    plt.xlabel(\"Raw value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Visualization of the values\")\n",
    "    plt.hist(data[\"RawScore\"])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.xlabel(\"Decile value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Visualization of the decile values\")\n",
    "    plt.hist(data[\"DecileScore\"])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #sb.countplot(x = \"RawScore\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Indication that some black people might get higher sentences that white people\n",
    "    sb.countplot(x = \"DecileScore\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    plt.show()\n",
    "    \n",
    "    sb.countplot(x = \"ScoreText\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"Agency_Text\", \"Sex_Code_Text\", \"Ethnic_Code_Text\", \"ScaleSet_ID\", \"AssessmentReason\", \"Language\", \"LegalStatus\", \"CustodyStatus\", \"MaritalStatus\", \"RecSupervisionLevel\"]\n",
    "new_categoricals = [\"c_charge_degree\", \"race\", \"age_cat\", \"sex\", \"is_recid\"] # \"two_year_recid\"\n",
    "# Changing date of birth into age,as this should work better in a neural network\n",
    "new_numericals = [\"age\", \"priors_count\"] # \"days_b_screening_arrest\"\n",
    "\n",
    "if s == 1:\n",
    "    ages = [None] * len(data[\"DateOfBirth\"])\n",
    "    for i in range(len(data[\"DateOfBirth\"])):\n",
    "        ages[i] = 20 +(100 - int(data[\"DateOfBirth\"][i].split(\"/\")[2]))\n",
    "    data[\"DateOfBirth\"] = ages\n",
    "    numericals = [\"DateOfBirth\"]\n",
    "    s+=1\n",
    "else:\n",
    "    pass\n",
    "\n",
    "outputs = [\"ScoreText\"]\n",
    "new_outputs = [\"score_text\"]\n",
    "\n",
    "data = data.dropna(axis = 0, how = 'any')\n",
    "data[outputs] = data[outputs].replace('Low',0)\n",
    "data[outputs] = data[outputs].replace('Medium',1)\n",
    "data[outputs] = data[outputs].replace('High',1)\n",
    "data[outputs] = data[outputs].astype(\"category\")\n",
    "\n",
    "\n",
    "new_data[new_outputs] = new_data[new_outputs].replace('Low',0)\n",
    "new_data[new_outputs] = new_data[new_outputs].replace('Medium',1)\n",
    "new_data[new_outputs] = new_data[new_outputs].replace('High',1)\n",
    "new_data[new_outputs] = new_data[new_outputs].astype(\"category\")\n",
    "\n",
    "\n",
    "for category in categoricals:\n",
    "    data[category] = data[category].astype(\"category\")\n",
    "    \n",
    "for new_category in new_categoricals:\n",
    "    new_data[new_category] = new_data[new_category].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[26.],\n",
      "        [26.],\n",
      "        [26.],\n",
      "        ...,\n",
      "        [28.],\n",
      "        [32.],\n",
      "        [32.]])\n",
      "tensor([[69.,  0.],\n",
      "        [34.,  0.],\n",
      "        [24.,  4.],\n",
      "        ...,\n",
      "        [24.,  2.],\n",
      "        [52.,  0.],\n",
      "        [29.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preparing data for pytorch\n",
    "Xcat = []\n",
    "for i in range(len(categoricals)):\n",
    "    Xcat.append(data[categoricals[i]].cat.codes.values)\n",
    "Xcat = torch.tensor(Xcat , dtype = torch.int64).T\n",
    "\n",
    "\n",
    "new_Xcat = []\n",
    "for i in range(len(new_categoricals)):\n",
    "    new_Xcat.append(new_data[new_categoricals[i]].cat.codes.values)\n",
    "new_Xcat = torch.tensor(new_Xcat , dtype = torch.int64).T\n",
    "\n",
    "#Converting the numerical values to a tensor\n",
    "Xnum = np.stack([data[col].values for col in numericals], 1)\n",
    "Xnum = torch.tensor(Xnum, dtype=torch.float)\n",
    "\n",
    "\n",
    "new_Xnum = np.stack([new_data[col].values for col in new_numericals], 1)\n",
    "new_Xnum = torch.tensor(new_Xnum, dtype=torch.float)\n",
    "\n",
    "# Converting the output to tensor\n",
    "y = torch.tensor(data[outputs].values).flatten()\n",
    "new_y = torch.tensor(new_data[new_outputs].values).flatten()\n",
    "\n",
    "# Calculation of embedding sizes for the categorical values in the format (unique categorical values, embedding size (dimension of encoding))\n",
    "categorical_column_sizes = [len(data[column].cat.categories) for column in categoricals]\n",
    "categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]\n",
    "\n",
    "\n",
    "new_categorical_column_sizes = [len(new_data[column].cat.categories) for column in new_categoricals]\n",
    "new_categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in new_categorical_column_sizes]\n",
    "\n",
    "# Train-test split\n",
    "totalnumber = len(Xnum)\n",
    "testnumber = int(totalnumber * 0.2)\n",
    "\n",
    "\n",
    "\n",
    "new_totalnumber = len(new_Xnum)\n",
    "new_testnumber = int(new_totalnumber * 0.2)\n",
    "\n",
    "Xcattrain = Xcat[:totalnumber - testnumber]\n",
    "Xcattest = Xcat[totalnumber - testnumber:totalnumber]\n",
    "Xnumtrain = Xnum[:totalnumber - testnumber]\n",
    "Xnumtest = Xnum[totalnumber - testnumber:totalnumber]\n",
    "ytrain = y[:totalnumber - testnumber]\n",
    "ytest = y[totalnumber - testnumber:totalnumber]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_Xcattrain = new_Xcat[:new_totalnumber - new_testnumber]\n",
    "new_Xcattest = new_Xcat[new_totalnumber - new_testnumber:new_totalnumber]\n",
    "new_Xnumtrain = new_Xnum[:new_totalnumber - new_testnumber]\n",
    "new_Xnumtest = new_Xnum[new_totalnumber - new_testnumber:new_totalnumber]\n",
    "new_ytrain = new_y[:new_totalnumber - new_testnumber]\n",
    "new_ytest = new_y[new_totalnumber - new_testnumber:new_totalnumber]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols + num_numerical_cols\n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = torch.cat([x, x_numerical], 1)\n",
    "        x = self.layers(x)\n",
    "        return nn.functional.softmax(x, dim = -1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(4, 2)\n",
      "    (1): Embedding(2, 1)\n",
      "    (2): Embedding(9, 5)\n",
      "    (3): Embedding(2, 1)\n",
      "    (4): Embedding(1, 1)\n",
      "    (5): Embedding(2, 1)\n",
      "    (6): Embedding(5, 3)\n",
      "    (7): Embedding(6, 3)\n",
      "    (8): Embedding(7, 4)\n",
      "    (9): Embedding(4, 2)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.6, inplace=False)\n",
      "  (batch_norm_num): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=10, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "    (4): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.6, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.6, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.6, inplace=False)\n",
      "    (16): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define and show the model\n",
    "#model = Model(categorical_embedding_sizes, 1, 2, [8,16,32,64,128], p=0.6)\n",
    "model = Model(categorical_embedding_sizes, 1, 2, [10,20,20,10], p=0.6)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimization\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001 , weight_decay = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4355, 0.5645],\n",
      "        [0.7099, 0.2901],\n",
      "        [0.0156, 0.9844],\n",
      "        ...,\n",
      "        [0.6057, 0.3943],\n",
      "        [0.8905, 0.1095],\n",
      "        [0.3582, 0.6418]], grad_fn=<SoftmaxBackward>)\n",
      "epoch:   1 loss: 0.68714494\n",
      "tensor([[0.7941, 0.2059],\n",
      "        [0.7968, 0.2032],\n",
      "        [0.6633, 0.3367],\n",
      "        ...,\n",
      "        [0.4477, 0.5523],\n",
      "        [0.9412, 0.0588],\n",
      "        [0.4467, 0.5533]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1924, 0.8076],\n",
      "        [0.4937, 0.5063],\n",
      "        [0.7391, 0.2609],\n",
      "        ...,\n",
      "        [0.6050, 0.3950],\n",
      "        [0.9794, 0.0206],\n",
      "        [0.5274, 0.4726]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3225, 0.6775],\n",
      "        [0.5261, 0.4739],\n",
      "        [0.6587, 0.3413],\n",
      "        ...,\n",
      "        [0.0811, 0.9189],\n",
      "        [0.3868, 0.6132],\n",
      "        [0.5051, 0.4949]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5400, 0.4600],\n",
      "        [0.7700, 0.2300],\n",
      "        [0.8101, 0.1899],\n",
      "        ...,\n",
      "        [0.6388, 0.3612],\n",
      "        [0.6138, 0.3862],\n",
      "        [0.5650, 0.4350]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7891, 0.2109],\n",
      "        [0.2746, 0.7254],\n",
      "        [0.8102, 0.1898],\n",
      "        ...,\n",
      "        [0.6953, 0.3047],\n",
      "        [0.5616, 0.4384],\n",
      "        [0.5376, 0.4624]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8068, 0.1932],\n",
      "        [0.4753, 0.5247],\n",
      "        [0.5969, 0.4031],\n",
      "        ...,\n",
      "        [0.8792, 0.1208],\n",
      "        [0.5597, 0.4403],\n",
      "        [0.5791, 0.4209]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6120, 0.3880],\n",
      "        [0.4976, 0.5024],\n",
      "        [0.6875, 0.3125],\n",
      "        ...,\n",
      "        [0.1116, 0.8884],\n",
      "        [0.1012, 0.8988],\n",
      "        [0.9915, 0.0085]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[9.7615e-01, 2.3848e-02],\n",
      "        [5.7827e-01, 4.2173e-01],\n",
      "        [6.4679e-01, 3.5321e-01],\n",
      "        ...,\n",
      "        [1.5121e-02, 9.8488e-01],\n",
      "        [6.4347e-01, 3.5653e-01],\n",
      "        [1.0000e+00, 1.8848e-09]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.0000e+00, 7.1576e-09],\n",
      "        [2.7316e-02, 9.7268e-01],\n",
      "        [8.5555e-01, 1.4445e-01],\n",
      "        ...,\n",
      "        [4.9725e-01, 5.0275e-01],\n",
      "        [6.6534e-01, 3.3466e-01],\n",
      "        [7.9840e-01, 2.0160e-01]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6351, 0.3649],\n",
      "        [0.1144, 0.8856],\n",
      "        [0.6108, 0.3892],\n",
      "        ...,\n",
      "        [0.6739, 0.3261],\n",
      "        [0.7347, 0.2653],\n",
      "        [0.5714, 0.4286]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6218, 0.3782],\n",
      "        [0.9024, 0.0976],\n",
      "        [0.7472, 0.2528],\n",
      "        ...,\n",
      "        [0.9237, 0.0763],\n",
      "        [0.4333, 0.5667],\n",
      "        [0.6205, 0.3795]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5022, 0.4978],\n",
      "        [0.5725, 0.4275],\n",
      "        [0.7243, 0.2757],\n",
      "        ...,\n",
      "        [0.5359, 0.4641],\n",
      "        [0.6542, 0.3458],\n",
      "        [0.7245, 0.2755]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.0034, 0.9966],\n",
      "        [0.4950, 0.5050],\n",
      "        [0.0296, 0.9704],\n",
      "        ...,\n",
      "        [0.5762, 0.4238],\n",
      "        [0.6022, 0.3978],\n",
      "        [0.6670, 0.3330]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.9923, 0.0077],\n",
      "        [0.4217, 0.5783],\n",
      "        [0.5874, 0.4126],\n",
      "        ...,\n",
      "        [0.6620, 0.3380],\n",
      "        [0.4862, 0.5138],\n",
      "        [0.1115, 0.8885]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.0213, 0.9787],\n",
      "        [0.5829, 0.4171],\n",
      "        [0.1729, 0.8271],\n",
      "        ...,\n",
      "        [0.4602, 0.5398],\n",
      "        [0.0058, 0.9942],\n",
      "        [0.4055, 0.5945]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7243, 0.2757],\n",
      "        [0.6052, 0.3948],\n",
      "        [0.6101, 0.3899],\n",
      "        ...,\n",
      "        [0.5351, 0.4649],\n",
      "        [0.6152, 0.3848],\n",
      "        [0.7372, 0.2628]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6318, 0.3682],\n",
      "        [0.5475, 0.4525],\n",
      "        [0.8587, 0.1413],\n",
      "        ...,\n",
      "        [0.6221, 0.3779],\n",
      "        [0.5274, 0.4726],\n",
      "        [0.5882, 0.4118]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.9932, 0.0068],\n",
      "        [0.4649, 0.5351],\n",
      "        [0.6915, 0.3085],\n",
      "        ...,\n",
      "        [0.1399, 0.8601],\n",
      "        [0.4380, 0.5620],\n",
      "        [0.1329, 0.8671]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5615, 0.4385],\n",
      "        [0.6218, 0.3782],\n",
      "        [0.6405, 0.3595],\n",
      "        ...,\n",
      "        [0.6926, 0.3074],\n",
      "        [0.3924, 0.6076],\n",
      "        [0.7650, 0.2350]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4788, 0.5212],\n",
      "        [0.3997, 0.6003],\n",
      "        [0.6799, 0.3201],\n",
      "        ...,\n",
      "        [0.6662, 0.3338],\n",
      "        [0.9968, 0.0032],\n",
      "        [0.4733, 0.5267]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7729, 0.2271],\n",
      "        [0.6805, 0.3195],\n",
      "        [0.7602, 0.2398],\n",
      "        ...,\n",
      "        [0.6428, 0.3572],\n",
      "        [0.5248, 0.4752],\n",
      "        [0.5203, 0.4797]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-8c7ca3267576>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0msingle_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "aggregated_losses = []\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(Xcattrain, Xnumtrain)\n",
    "    single_loss = loss_function(y_pred, ytrain)\n",
    "    aggregated_losses.append(single_loss)\n",
    "    \n",
    "    if i % 25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "    \n",
    "\n",
    "\n",
    "# Plot the loss over epocs\n",
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val = model(Xcattest, Xnumtest)\n",
    "    loss = loss_function(y_val, ytest)\n",
    "\n",
    "\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "\n",
    "print(confusion_matrix(ytest,y_val))\n",
    "print(classification_report(ytest,y_val))\n",
    "print(accuracy_score(ytest, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(4, 2)\n",
      "    (1): Embedding(2, 1)\n",
      "    (2): Embedding(9, 5)\n",
      "    (3): Embedding(2, 1)\n",
      "    (4): Embedding(1, 1)\n",
      "    (5): Embedding(2, 1)\n",
      "    (6): Embedding(5, 3)\n",
      "    (7): Embedding(6, 3)\n",
      "    (8): Embedding(7, 4)\n",
      "    (9): Embedding(4, 2)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.6, inplace=False)\n",
      "  (batch_norm_num): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=10, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "    (4): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.6, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.6, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.6, inplace=False)\n",
      "    (16): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define and show the model\n",
    "#model = Model(categorical_embedding_sizes, 1, 2, [8,16,32,64,128], p=0.6)\n",
    "model = Model(categorical_embedding_sizes, 1, 2, [10,20,20,10], p=0.6)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimization\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001 , weight_decay = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 0.69400436\n",
      "epoch:  26 loss: 0.67817098\n",
      "epoch:  51 loss: 0.66534072\n",
      "epoch:  76 loss: 0.65446168\n",
      "epoch: 101 loss: 0.64749914\n",
      "epoch: 126 loss: 0.64207011\n",
      "epoch: 151 loss: 0.62764639\n",
      "epoch: 176 loss: 0.61703765\n",
      "epoch: 201 loss: 0.60495639\n",
      "epoch: 226 loss: 0.59885734\n",
      "epoch: 251 loss: 0.59658116\n",
      "epoch: 276 loss: 0.58502489\n",
      "epoch: 300 loss: 0.5790482759\n",
      "[[1741  340]\n",
      " [ 306  740]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.84      2081\n",
      "           1       0.69      0.71      0.70      1046\n",
      "\n",
      "    accuracy                           0.79      3127\n",
      "   macro avg       0.77      0.77      0.77      3127\n",
      "weighted avg       0.80      0.79      0.79      3127\n",
      "\n",
      "0.7934122161816437\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd5hdVbn48e97zvTe+0xmksyE9MKkkEgIRQiIAlIEFBELXgEVuRb8WVC8Xq+FKxdFFBQpgiAgECQSQ4QQUkgvTOr0TO89U8/6/bH3nEwmJ8kE5syZ8n6eZ57ss/beZ96dk8w7a6293yXGGJRSSqnBHL4OQCml1OikCUIppZRHmiCUUkp5pAlCKaWUR5oglFJKeeTn6wCGS1xcnMnMzPR1GEopNabs2LGjzhgT72nfuEkQmZmZbN++3ddhKKXUmCIiJafap0NMSimlPNIEoZRSyiNNEEoppTzSBKGUUsojTRBKKaU80gShlFLKI00QSimlPJrwCaL5WA//9+YR9hxt8nUoSik1qkz4BAHw6zcP815Rva/DUEqpUWXCJ4jIYH8igvwoazzm61CUUmpUmfAJAiAtOkQThFJKDaIJAkiLDqassYOKpmP09Ll8HY5SSo0KmiCwehDFdR1c+Ku3eWZLCX0uw7HuPl+HpZRSPjVuqrl+GGnRwXTbPYf3ihp4bEMR5U3HuO/jM7htWZaPo1NKKd/QHgRWgui37kAN5U3HcAi8sqvch1EppZRvaYLAGmLq19+T+NzSLPaUNVPd0umrsJRSyqc0QQAZsSGEBfpx0TkJAEyOC+WmRekArN1f7cvQlFLKZ3QOAggL9GPzdy+isb2Hfx+sYWFmDFMTwogO8edAZYuvw1NKKZ/QHoQtPMif9JhgvviRLG45bxIiQmp0MOVNx/jbtqPs1lIcSqkJRnsQA4gI379yhvt1alQwh6pa+fZLewG488IpXLsgjcnxYb4KUSmlRoz2IE4jNSqE4voO9+vfvV3AL9445MOIlFJq5GiCOI3UAbe//uOrH+HaBWlsKarH5TI+jEoppUaGJojTSI2yEoQITIkP47zJsTR19HCwqtXHkSmllPdpgjiN/gfo0qKDCQ5wsmRKLACbC7U0uFJq/NMEcRopdg8iOyEcsHoU0xLD+dOGQhrbu30ZmlJKeZ0miNOIDvEnKSKI+elR7rZfXT+XurZuHnzzsA8jU0op79PbXE9DRPjXPcsJ9ne622anRZKbGc3usmZ3W0tnD34OISRA/zqVUuOH9iDOICLIH3/niX9NOYnh5Fe34nIZ2rt6+fhv3uUbz+92799cUM/6w7UjHapSSg0r/ZX3A8hJDKe9u4/ypmM8sr6AkvoOalu76O51EeDn4P5/7Keju5f137rQ16EqpdQH5tUehIisFJFDIpIvIvee4pgbRGS/iOSJyLMD2n9htx0QkYdERLwZ69nISbSepP78E9t49r1S5qZF0tHdx96yJjq6ezlc3UpJfQdNHTqRrZQau7yWIETECTwMXA7MAG4SkRmDjskGvgssM8bMBO6225cCy4A5wCxgIXCBt2I9W9mJ1l1NR2rauHlxBk/ctggR2JhfT15FC332g3R7BsxTKKXUWOPNHsQiIN8YU2iM6QaeA64adMyXgIeNMY0Axpgau90AQUAAEAj4A6Om7nZksL97+wcfm0F0aADnJEWwvaSBPQOK+u3VAn9KqTHMm3MQqcDRAa/LgMWDjskBEJGNgBP4kTHmDWPMZhF5C6gEBPitMebA4G8gIrcDtwNkZGQM/xWcxqq7lhEZ7E9wgHWH0+T4UPLKm9l9tInUqGAC/R3ag1BKjWneTBCe5gwGFzHyA7KBFUAasEFEZgFxwHS7DWCtiCw3xrxzwpsZ8yjwKEBubu6IFkiakxZ1wuu06GDW5lXjdAjTkyMID/JjY37dSIaklFLDyptDTGVA+oDXaUCFh2NeNcb0GGOKgENYCeMaYIsxps0Y0wb8E1jixVg/tLToELr7XBTUtjMlIZQ5aZHUtHZR1WwtWdrc0UNZY8cZ3kUppUYPbyaIbUC2iGSJSABwI7Bq0DGvABcCiEgc1pBTIVAKXCAifiLijzVBfdIQ02iSNqDy65S4MHcPY09ZE30uw2f/vJVrH9nknsBWSqnRzmtDTMaYXhG5C1iDNb/wuDEmT0TuB7YbY1bZ+y4Vkf1AH/AtY0y9iLwIXATswxqWesMY85q3Yh0O6QMSxOT4UGamRODnEPaWNVHZdMw9eb29uIHFk2N9FaZSSg2ZVx+UM8asBlYPavvhgG0D3GN/DTymD/iyN2MbbqlRIe7tyfFhBPk7yUkMZ1dpE1UtncxNi+RQdSur91VqglBKjQlaamOYBAc4iQsLICrEn5jQAAAuPCeeTQX1FNa2c11uOhfkxLN2/6i5W1cppU5LE8QwmhQbSnbC8fWqb12aSYCfAxG4bGYii7NiqWju5OktJdz93C4O6cJDSqlRTGsxDaNfXDfnhHt7E8KDuHPFVMqbOkgID2JehjVx/cNX38cYWL2vil9eP4er5qX6JmCllDoNTRDDaEp82EltX78k2709IzkCf6fQ02e4bVkme4428Z2X9vKx2cn4ObUzp5QaXfSn0ggK8ncyIyUSgE8tTOfTiyfR2eOiuP7k5yMKa9v4r3/s19tilVI+owlihF06I5H5GVFMSwwnp7/oX3UrWwrr+f4r+7Bu7IJVeyr447tFHG3Qh+uUUr6hQ0wj7M4Lp3LnhVMBmJoQhohV9fW1PRWUNx3jc0uzmJoQRmFtOwCVzZ1kxoX6MmSl1ASlPQgfCg5wkh4dwuPvFlHedAyALYX1ABTV9SeIYz6LTyk1sWmC8LGcxHC6+1xcPS+FpIggNhfWY4wZkCA6fRyhUmqi0gThYwsmRREV4s//u2I6SybHsKWgnsPVbbR19QJQ0aQ9CKWUb2iC8LEvL5/Cu9+5iISIIK7PTae1s5fLHjxe1Vx7EEopX9EE4WNOhxAWaN0rsGxqHC99Zal73zlJ4ew52sTDb+XT0tlDb5+LbcUN7judlFLKm2S8/LDJzc0127dv93UYw+JQVStvHqimyi7LAZAVF8p5U2J59r1SHvzUPK6er09fK6U+PBHZYYzJ9bRPexCj0LSkcO68cCpOh1W445LpCVQ1d/Lse6UA/HLNITp7+nwZolJqAtAEMYrdkJvOFbOTePDG+fz6U/M4JymcH1w5g/KmYzy3tZQFP1lLfk2br8NUSo1TmiBGsRkpEfzu0+cSFujHyllJvHH3cq6ckwzAE5uKaWjvZvW+Sh9HqZQarzRBjDEJ4YFEBvu76zet3V/N++XNtHf1sqmgDpfWblJKDRMttTHGiAg5iWFsK24EYF95M1f+5l38HEKvy/DjT8zk1qWZvg1SKTUuaA9iDOov8nflnGTmpUfx7ZXTuO7cNM5JCucP6wsormunu9fFfa++7y7hoZRSZ0t7EGPQtCQrQVw6M4lPzE1xt68/XMutj29lxa/e5s4Lp/Dk5hLSokP40vLJ5FU088x7pXx+WSZTE8J9FbpSagzRHsQYdH52PLNSI1gyOeaE9gty4nn1zmUEOB08Y98Sm1/TRmN7N5/83Saefa+Uv+8s90XISqkxSBPEGJQVF8o/vno+CeFBJ+2bmx7F9ORwmjp6AMivbWN/ZQtdvS4AdxlxpZQ6E00Q49CctCj3dn5NGwcqWwA4d1I0BbX63IRSamg0QYxDc9OtBBEa4KT5WA/v5tcRHx7IwswYiuvb6e1z+ThCpdRYoAliHFqYGY3TIXxinlWv6e1DtUxPjmBKfCg9fYbShg4ee6eQnaXWrbI1rZ309LkwxmghQKWUm97FNA5Nig3lnW9fiL9D+OtWa7L6nKRwpiSEAfD63koeWHuYYH8nT9y2kC//ZQczUyKoaOrkmvmpfO3ibF+Gr5QaJbzagxCRlSJySETyReTeUxxzg4jsF5E8EXl2QHuGiPxLRA7Y+zO9Get4kxoVTEJEEE9+fhGzUyO5bGYSU+KtNbAf3VCI0yGEBjr5yjM7aeroYWN+PUV17TyxqZjuXh2CUkp5MUGIiBN4GLgcmAHcJCIzBh2TDXwXWGaMmQncPWD3U8AvjTHTgUVAjbdiHc8uyInnta9+hHMnRRMZ7M9nFk+itbOXpVNiuXlRBg3t3YQH+nH/VTP5z4/m0NDezb8PVp/0Pi6X0bkLpSYYbw4xLQLyjTGFACLyHHAVsH/AMV8CHjbGNAIYY2rsY2cAfsaYtXa73nozTO69/BxKGjq4bVkmU+PD+M1b+Vw0PYHPnpdJn8vw1JYSXttTycpZVlHAn/3zgF3nqZ5ZKZE8dNN8H1+BUmqkeDNBpAJHB7wuAxYPOiYHQEQ2Ak7gR8aYN+z2JhH5O5AFvAnca4w5YREEEbkduB0gIyPDG9cw7oQG+vHU5xe5Xz92Sy7TUyIAa3W75dnxrDtYjctlqGzp5NF3Cumfty6p7+AHV84gPjzQF6ErpUaYN+cgxEPb4Ftk/IBsYAVwE/BHEYmy288HvgksBCYDnzvpzYx51BiTa4zJjY+PH77IJ5BLZiSSGhXsfv2R7FiaOnq4/ekdfOuFPRgDf7o1lyduW0ify/Dqbn0SW6mJwpsJogxIH/A6DajwcMyrxpgeY0wRcAgrYZQBu4wxhcaYXuAVYIEXY1W2ZVPjAHjzQDWbCur5yNQ4Lp6eyIppCcxMiWDt/pPnJwbqcxm6enW1O6XGA28OMW0DskUkCygHbgRuHnTMK1g9hydEJA5raKkQaAKiRSTeGFMLXASMjwWnR7mE8CCuOzeN5MggPj43hZjQAPe+OWmRrMnznCD2lTVT397FO4fr2HCkln99YzkinjqRSqmxwmsJwhjTKyJ3AWuw5hceN8bkicj9wHZjzCp736Uish/oA75ljKkHEJFvAuvE+imzA3jMW7GqE/3q+rke26cmhPPXrUepa+siLuz4PER+TRs3P7aFHpeLYH8njR09FNS2adVYpcY4rz4oZ4xZDawe1PbDAdsGuMf+GnzuWmCON+NTZyfbftDuSHWbO0H0uQx3P78LlzF09rjo7LFuhX37UK0mCKXGOC21oYYsO9FKEPk1rQAYY/jdW/m8X97Cz6+bwzlJ4QT4OUiLDuadI3Xu89q7ejlS3eqTmJVSH5yW2lBDlhQRRHigH4er2zDGcN+qPJ7aXMLKmUl8bHYySRFBlDUeY39lC4+/W0RF0zFSooL5zJ/eY1dpEwX/fQVOh85LKDVWaA9CDZmIMD0lglV7Kvjac7t5anMJn1+WxcOfXoCIkJsZw9XzU/nseZMwwB83FAGwq7QJgIb2bh9Gr5Q6W5og1Fn5n0/OJjshjLcO1nDtgjS+/7HpJ/UK0qJDuHJOMi9sP0prZ4+7vbqlc6TDVUp9CDrEpM7K5PgwXvzK0jMet2JaPK/uruD5bccfpq9p7QQivRidUmo4aQ9CecW5GdZ62X96t8jdVt3S5atwlFIfgCYI5RXpMcHEhQVS2dzJ8hyrDMrAIaZNBXV86antWlpcqVFME4TyChHh3EnW0qdfu2gqcWEBVLd0UVLfzh83FPKtF/aydn81u482+ThSpdSp6ByE8prblmUxLSmC3MwYEsKD2FnSyKf+sIUquychYvUkFmXF+DhSpZQn2oNQXrNkciz3fDQHgMSIQA5Vt9LW1cszX1zMqruWMTMlgs0F9T6OUil1Kpog1Iho77IqvH72vEksmxrHnLQozpscy87SRh5+K5+a1k7+780jdHT3us+pae3kzmd30tShz08o5QuaINSI6B9Gum1Zlrvt1qWZzEiJ5JdrDvGLNw7x6zcP87nHt7mXNv3nvipe31vJe0UNPolZqYlOE4QaEXdfks2uH3z0hNXo0qJD+Pm1swFYk1cFwNbiBt48UOPeBiiqax/haJVSoJPUaoT4OR1ED1hbot+U+DACnA5aO3tZOiWW4rp2HllfwP7KFtYdsNaeKNYEoZRPaA9C+ZS/08FUu4z4rNRIPrUwgz1Hm3ho3RF36fDCunZ6+lz8fWcZXb19GDN45VqllDdoD0L53PTkCPZXtjA9OZxLZyQRExZAoJ+Dx98tIi4skMPVrTyxsZifrj7AwapWntlSwhOfX8TCTL09Vilv0h6E8rmZKRGAlShCA/24ZckkbshN5427l3PelFhqWrv4+RsHAXj0nULau/t480A1x7r7+I+nd7CjpNGX4Ss1bmmCUD53fW4aD1w/l2mJJ69AlxUXCkCvy/DRGYnu9m1FDWwpqueNvCqufWSTVopVygs0QSifCw/y59pz07CWHz/R8px4vvCRLNb95wXcfUk2TocwKzWCfeXNvHO41n3c05tLRjJkpSYEGS8Tfrm5uWb79u2+DkN5WVNHNztLG/n8E9ZnvSAjikA/J7VtXbx5zwU+jk6psUdEdhhjcj3t0x6EGlOiQgI4b3IcqVHBACzIiOby2Unk17S518pWSg0PTRBqzAkOcPLMFxeTOymaq+encumMJADeOlh7hjOVUmdDE4QakzLjQnnxK0uZlRpJUmQQqVHB7C1vPuk4Ywy3/Ok9Xt1d7oMolRrbNEGocWF2aiT7ypqoau6kqaObpT9bx6u7y2ns6GHDkTrWH/bcu2jp7OFYd98IR6vU2KAPyqlxYXZaJG/kVbHkZ+uICwukrq2LR94uIC3amqs42tDh8bzP/mkr0xLD+fl1c0YyXKXGBK/2IERkpYgcEpF8Ebn3FMfcICL7RSRPRJ4dtC9CRMpF5LfejFONfXPTotzbdW3W2tfHevooqrMSw9GGYx7PK6hpY0+ZrmqnlCdD6kGIyBSgzBjTJSIrgDnAU8aYU/7PEhEn8DDwUaAM2CYiq4wx+wcckw18F1hmjGkUkYRBb/MTYP3ZXJCamGanRQJW1djpyRHsLGnkD+8UsqvUesq6urWTlQ++wyfmpXDHiqkAdHT30trVS2FdO30ug9Nx8nMYSk1kQ+1BvAT0ichU4E9AFvDs6U9hEZBvjCk0xnQDzwFXDTrmS8DDxphGAGNMTf8OETkXSAT+NcQY1QQWGezPof9ayd2X5HDZzCQuyIkHYNXuCgCMgYNVrTy39ai72F9Ni9XT6O51Ud7ouYeh1EQ21AThMsb0AtcADxpjvgEkn+GcVODogNdldttAOUCOiGwUkS0ishJARBzAA8C3TvcNROR2EdkuIttra/UWx4ku0M/p3p6VFolDoLWrl4ig4x3l0oYODlS20tHdS0XT8aRQUNs2orEqNRYMNUH0iMhNwK3AP+w2/zOc46m/PvixbT8gG1gB3AT8UUSigDuA1caYo5yGMeZRY0yuMSY3Pj7+DOGoiSQiyN+9HvbcdGt+Ii4sEIfA6/squPI37/KfL+xxH68JQqmTDfUuptuA/wB+aowpEpEs4C9nOKcMSB/wOg2o8HDMFmNMD1AkIoewEsZ5wPkicgcQBgSISJsxxuNEt1Ke3HnhVNJjQqynrf9vAx+dkUB5Uyd/3FBEV6/LfVyQv4P8Gk0QSg02pARhTyx/DUBEooFwY8z/nOG0bUC2nUzKgRuBmwcd8wpWz+EJEYnDGnIqNMZ8uv8AEfkckKvJQZ0tEeGqedao5nO3LyE1Kpj3iupPKPIX4OdgbloUB6ta2VHSQKCfk1mpkb4KWalRZUhDTCLytn3LaQywB/iziPzv6c6x5yzuAtYAB4C/GWPyROR+EfmEfdgaoF5E9gNvAd8yxtR/0ItR6lRmpUYSHRrAxdMTSY0KJj3Gej4iMSKQmSmRHKxq4Wt/3c33Xnn/pHNdLkOfa3wUtVTqbAx1iCnSGNMiIl8E/myMuU9E9p7pJGPMamD1oLYfDtg2wD3216ne4wngiSHGqdRp+TsdvHH3+dS3dbPiV2+TEB7ErNQIOntclDcdo7a1i67evhMmvH/w6vsU17fzzBeX+DBypUbeUCep/UQkGbiB45PUSo1J4UH+TIoNIS4sgKTIIGamHB9S6u5zcaDSqgqbX9PG/ooWNhXUs6WwQUtyqAlnqD2I+7GGgzYaY7aJyGTgiPfCUsq7RIQ/3HIuMaGBpEcHE+jnQAQ6e1zsOdrEvPQo7n5+F/Vt3VS1dGIM7CptZH5GNMEBzjN/A6XGgaFOUr8AvDDgdSFwrbeCUmoknDspxr29ZHIsCeGBrD9cy1+3lhIe5Mf75S0nHH/zH99jenIE//z6+SMdqlI+MdRJ6jQReVlEakSkWkReEpE0bwen1Ej58+cW8vNr5/CFj2RR29rFPX/b4/G4A5UtlNZ7Lvyn1Hgz1DmIPwOrgBSsp6Ffs9uUGhccDsHhEL58wRRe/9r5RIX4kxkbQmpUMDGhAXxy/vEiABvy9al9NTEMaU1qEdltjJl3pjZf0jWp1XA6UNmCMbD7aBONHd18eflkel2Gix9Yz6zUCP5wSy47Sxs5VNXKTYsyfB2uUh/Y6dakHuokdZ2IfAb4q/36JkCfV1Dj1vTkCABmpES42/ycsGJaPC/tLGP30SY+9/hW2rv7+PjcFMICdWkVNf4MdYjp81i3uFYBlcB1WOU3lJpQPr14Ep09Lm74/WY6uvvocxl2ljT6OiylvGJICcIYU2qM+YQxJt4Yk2CMuRr4pJdjU2rUmZESwdIpsbiM4Q+3nIvTIWwtavB1WEp5xYdZUe6UTz8rNZ49dNN8Xr5jGRdPT2RWSgRbi60Ese5ANZ9/YhtDmddTaiz4MAlCl99SE1JcWKB7BbslU2LZVdpIfVsXb7xfxb8P1lDb2uXjCJUaHh8mQeivSWrCu25BGj19hhd3lHHYLhle0nD8OQljDN0DSosrNZacNkGISKuItHj4asV6JkKpCS07MZxFmTE8u7WU/GqrhtPWogZe3V0OwOp9VSz4yVr+tv0o5/1sHU0d3b4MV6mzctp784wx4SMViFJj1XW5aXz7xePFjX+99jC9LkNGTAibCupo6+rley/vo6fPsKes2b1etlKj3YcZYlJKAZfOSMTPcXxKrtdeO+KJTcXkVVj1nHr6rLb9FS0nv4FSo5QmCKU+pKiQAM6bEgvAHHvyOjLYn9f3VrK/soXM2BBCApxEBvtzoFIThBo7NEEoNQzuWDGV25ZlMi89CoCfXD2LPnuC+msXZ7Prhx9lYWa0Jgg1pmiCUGoYnDcllvs+PpOLpydy4bR4rpiVxMXnJAIwMyWSQD8n05MjKKhto7PH88JDdW1d3PzYFsoatVqsGh20gIxSw+iCnHj3JPR3Vk5jUmwIUxPCAFiYGYPLwJ83FvOVFVNOOvetgzVsKqhnW3EDadEhIxq3Up5oD0IpL8lODOcHV87AaU9gn58dx8qZSfx67WGPa0psL7ZqOlU164N2anTQBKHUCBERfnzVTAAe21B40v5tJVbJjqrmYyMal1KnoglCqRGUGBHENfNT+dv2oydMWNe3dVFY2w5AVUunr8JT6gSaIJQaYXddNJXokACue2QTr+wqZ92Bav579UEAkiKCqGo+OUH09rn4+RsHKalvH+lw1QSmCUKpEZYeE8Krdy1jSkIYdz+/my88uZ2XdpbxlRVTWJ4Tx77yZlY++A7vlze7z1mTV80jbxewaneFDyNXE43exaSUDyRGBPH87efx1qEa4sMDae7oYcW0eB76dz4uAwerWvnr1lJ+es1sAJ7cXAxAUZ32INTI8WoPQkRWisghEckXkXtPccwNIrJfRPJE5Fm7bZ6IbLbb9orIp7wZp1K+EBzg5IrZySzMjOGSGYn4OR0kRwa596/Jq6LPZThQ2cLWogZEoFAThBpBXksQIuIEHgYuB2YAN4nIjEHHZAPfBZYZY2YCd9u7OoDP2m0rgQdFJMpbsSo1WkSH+Lu369q62VxQz1ObSwj0c/Cx2ckU1rZhjMEYw49fy2NXqS53qrzHmz2IRUC+MabQGNMNPAdcNeiYLwEPG2MaAYwxNfafh40xR+ztCqAG0BKYatybmWLVcvrdpxcQFxbAL/91iFd2lXP1vFTmZ0TT0tlLY0cPB6ta+fPGYh55u8DHEavxzJtzEKnA0QGvy4DFg47JARCRjYAT+JEx5o2BB4jIIiAAOOl/gojcDtwOkJGRMWyBK+Ur6TEhFP3sCkSEow0d/OyfB4kJDeCOC6e4b4MtqmtzP1T39uFa2rp6CQvU6UQ1/Lz5r8rTkqSDV6HzA7KBFUAasEFEZhljmgBEJBl4GrjVGHPSslzGmEeBRwFyc3N1hTs1LohY/3VuOW8SVS2dXH9uOpNiQ+lf6rqgtp13jtQSEuCko7uPtw7W8PG5un6XGn7eHGIqA9IHvE4DBt+jVwa8aozpMcYUAYewEgYiEgG8DnzfGLPFi3EqNSqFBPhx38dnMiMlArB6F2GBfmwtamBbUSM3LcogPjyQ1fsq3efsK2umrk1Ldajh4c0exDYgW0SygHLgRuDmQce8AtwEPCEicVhDToUiEgC8DDxljHnBizEqNWY4HcL8jChe3V1OT59hxbR4untdvLDjKNf/fhPLpsbxu7cLuGxmEmWNHVyQE8/dl+T4Omw1hnmtB2GM6QXuAtYAB4C/GWPyROR+EfmEfdgaoF5E9gNvAd8yxtQDNwDLgc+JyG77a563YlVqrFiQEU1PnyHI38HCzBgun51EZ4+LbcWNPPjmEbp7Xby+t4JdpU28qg/VqQ/JqzNbxpjVwOpBbT8csG2Ae+yvgcf8BfiLN2NTaizKzYwGYHFWLEH+ThZlxjA3PYpzM6L5y5YS4sMDKW+yiv0V1bXz9OZiFmXFMi1Jl5dXZ09vfVBqDJmfEU1ksD8fm50MgJ/Twat3LgPgunPTiAsPYOWDG8iMDWFnaRM/eDWPrLhQ1n5jOX5Orayjzo4YMz5u/snNzTXbt2/3dRhKeV13rwt/p7jvdhqsrLGDyGB/bvjDFiqbj9HU0cP3rpjObcsyNUmok4jIDmNMrqd92oNQaowJ8Dv9D/n+1ehevmMpAU4H1/1+Ez9dfYDCujZ+9sk5IxGiGif01wmlxqkgfycOh/Dc7edx9bwUXtlVQV1bF7944yB/23b0zG+gJjztQSg1zgX4Obg+N51Xdldw8QPraT7Wg59DOFzdSq/LcNuyTDp7XDqRrU6iCUKpCWBxVgwxoQG0dfbysTnJvL63ksc3FhEfHkhBbRuVzZ28ec8Fvkuz590AABiDSURBVA5TjTKaIJSaAPycDn736QX42yXFX99bictAdUsX7V19dHT30tnTR5C/09ehqlFEE4RSE8SSybHu7eyEMI7UtAHQ1tULWIsUnZMUrklCuekktVIT0K8/NY/f3jz/hLarH97IFQ9toLOnz0dRqdFGE4RSE9Cs1Egun5VM4KBbZgtr2/ntv/MBaGzv5kh1qy/CU6OEJgilJiinQ5gSH0ZiRKC7bVFmDI9uKKSy+RgPrD3EtY9sorfPqrRf29rF+sO1vgpX+YDOQSg1gX32vEm0dvbS1dvHgcpW7r38HC564G0eWneE/Jo2Wjp7eb+ihXnpUXzv5X2sO1jDgftXnvFhPTU+aIJQagK7cdHJKzFePS+VVbsrCLQnq7cU1hMW6MfaA9UYA9UtnaTHhIx0qMoH9NcApdQJFmXF0N7dR0N7N2AliMfeKXSvaPfI+gKW/+ItuntPWuQRgNL6Dr789HZaOntGKmTlJZoglFInmJse5d5Oiw5mU349f99VxtIp1m2yq3ZXUNrQQXF9u8fz1x+uYU1eNW8drBmReJX3aIJQSp1gSnwYIQHW8NL/3jCPxMhA+lyGH358BnD8uYnDp7jDqaS+A0ATxDigcxBKqRM4HcKs1Eh2lDQyLz2KF768lJL6ds5JiiAiyI+Wzv4E0ebx/JIGK0GsP1xLn8vgdHguS65GP00QSqmTXH9uGlmxoQT4OUiKDCIpMgiA5MhgWjqtnsPhqlbaunoJC/TDGENbVy/hQf6U1ncQ6OegsaOHtw/VsPtoE3deOFWf0B6DdIhJKXWS63PT+fl1J68dkRxlJYqQACdv5FVx3n+vo6a1k3/srST3v96koLaN0oYOrpqXQoCfgy88uZ3f/Dufd4/UAdbDdzc+upmyxg73e3b3urjn+d0U1Xme01C+owlCKTVkyZHBwPG6Tq1dvby2p5J1B6rp6nXxP/88yLGePmamRHLRtAT3efXtXQDkVbSwpbCB7cWN7n0l9e38fVc56w/pnMVoo0NMSqkhm5UaQXx4ID+9ZhZbixp49J1CXt5VRm1rFyKwdn81ABkxISRFBvFGXhUA5U2dwPFEUd3S6X7PpmPW7bANHXpb7GijPQil1JDdvCiDTfdeRHJkMFfNS+X6c9N4v7yF6pYuvnpRNqlRVg8jOzGMS2cksuHbF5IcGUR54zEA6tqsZytqWrvc79loP2/R/6caPbQHoZQaMhHB33n8rqTPLJnE01tKKKht55r5qXz94mzq2rpIjLDmKtJjQkiJCqaiyUoQ9W3HexD960+4exDt3ZTWd5ASFYSfU393HQ30U1BKfWB+Tgf/+Or5PH/7ErLiQnE6xJ0c+qVGBVPR3J8grF7CrtIm5vzoX2zKr6Opw2orqG3jkv9dz993lo/sRahT0gShlPpQggOcLB6wGNFgKVHBVDZ14nIZ6uweRHnTMbr7XGwurKfJnns4VN1Kd5/rlA/gqZGnCUIp5VWpUUF097n4r9cPUG4PNfV7v7yZRjtB9Nd6Kms8NvgtlI94NUGIyEoROSQi+SJy7ymOuUFE9otInog8O6D9VhE5Yn/d6s04lVLek2ZXfn18YxEHq07sHeRVtLiHmPqVNXWgRgevJQgRcQIPA5cDM4CbRGTGoGOyge8Cy4wxM4G77fYY4D5gMbAIuE9Eor0Vq1LKe86fGscvrj3+0F18uLVAkZ9DqGntcq+N3e9ow+l7EA+/lc+m/LrhD1SdxJs9iEVAvjGm0BjTDTwHXDXomC8BDxtjGgGMMf1PylwGrDXGNNj71gIrvRirUspL/JwObliYzjy7SuyM5AgALp5uPUiXPyhBNB/rcZcKN8bwqzWHeMt+iK63z8Wv1x7m0Q2FIxX+hObNBJEKHB3wusxuGygHyBGRjSKyRURWnsW5iMjtIrJdRLbX1upSiEqNZpmx1lBT7qRolk2N5e5LcvCzC/nFhR3vVQDu5ybyKlr47Vv5fPHJ7azJq6KyuZNel2FHcSN9LuODq5hYvJkgPJVwHPyJ+gHZwArgJuCPIhI1xHMxxjxqjMk1xuTGx8d/yHCVUt50nr2eRHJUMM98cQnTkyOYnRYJwOT4UADm2K8v/78NbCqoY/W+SpwOIScxnHtf2svOUqtER2tXLwerWnxwFROLNxNEGZA+4HUaUOHhmFeNMT3GmCLgEFbCGMq5Sqkx5IbcdJ64bSGfnH98MKB/2Kn/CexlU+Pc+17bU8nqfZUsnRLLQzfOo62rl/tf2+/ev7WoYYQin7i8mSC2AdkikiUiAcCNwKpBx7wCXAggInFYQ06FwBrgUhGJtienL7XblFJjlIiwYloCjgHrQ8xMsXoMQf4OfnrNLD6/LIt/fWM5M5IjWH+ohuL6Dj46I5HsxHAWZsZQ396Nv1NIjgxiW7GVINq7euns6fPJNY13Xiu1YYzpFZG7sH6wO4HHjTF5InI/sN0Ys4rjiWA/0Ad8yxhTDyAiP8FKMgD3G2P01wWlxpkr5ySzq7SRr6yYQlq0NUcRHRrAwsxontxcAlhrZINVQXZTQT1p0SHMTYvk3fx6jDHc/NgWchLD+eX1c096/7auXvr6DJEh/iN3UeOIV2sxGWNWA6sHtf1wwLYB7rG/Bp/7OPC4N+NTSvlWkL+Tn14z+6T26fadThFBfuQkhAPHS4xnxISwKCuWV3ZXkF/TRl5FCyUNHXzhiW109vbxwPXzeGxDIcV17aw7WMOizBj+9h/nnfQ9nthYxLv5dfzx1oVevMKxTYv1KaVGnf4EkZsZ4x6SmpseSWiAk8nxoSzKsh6LenFnGb0uQ1NHD+vsNbCv/M0G6tq63UUFtxY30NPnorvXRU+fi+ZjPdS1dbOxoJ63D+myqKejCUIpNepMSwonKsSfC6cdvzsx0M/JS3csJTE8iKgQf2JDA3hpx/HCfgF+Dv742VzueGYnGTEhvPbVj/DG+5V856V9FNS28acNReTXtjEpJoQthQ3EhwfSa9eHGlxgUFk0QSilRp0gfyeb772YQL8T76M5JynCvb1kSiyv760EYEFGFLNSI1meE88/v34+/k4HkcH+LMiwehp55S2U1HdQUt9BsL+TqpZO2rt7Aahs7tQEcQqaIJRSo1JwgPO0+8+fGsfreyuJCQ3g73csc7en27WfACbHhxHk7yCvooW69i4aO7rdixW1dtoJoumY+3bbfr19Lho6ukkIn9iJQ6u5KqXGpP5nJrLiQk95jNMhTE+OYH9lM/Vt3RhjrYE9UGVz50nn/WVLCRf9av2Ev31WE4RSakxKjwlhdmokCzKiTntcVlwoBbXtNNsr1/X0nViUobL55OKAe8uaaevqpbRhYleW1SEmpdSY9fIdS3HI6e9ASo0KpnbAGtgD+TnkhB7E3rIm3txfTUGd1csoqe8gJzF8+AIeY7QHoZQas/ycjhOezPakv4yHJzNSIvjH3kpu+P1mAJ7eXMJD/84nr7wZgI35dfxhfQHGXs2os6ePoxOoV6E9CKXUuJbiIUGEB/khQGiA9SNwa3EDHd297C2zEkOvXSn2iU3F7vf43dsFHKi0CgRu+PaFJ0yGj1fag1BKjWueEsS3LpvG/7tiOvdcmkN2QhgAh6vbOFJzfMW7gSNXv1xziPyaVhbbZT92HW067ffscxme3lJCW1fvMFyB72iCUEqNaylRx29VDfK3fuStnJXEjYsyWJgZw8+vs1a7W5NXhctAZLBVt2n+gFtfSxs6mJkSydNfWEyA0+EegjqVLYX1/OCV9/ney/uG+3JGlCYIpdS4FhLgR0xoAP5OITPWuiU2OiTAvT8t2uph9D90993Lz+GCnHh37adQ+3mM+RlRBPg5OCc5nH1nSBD9q+St2lNBT59reC9oBOkchFJq3EuJCqKu1UFsWABRIf74O4//bhwfFkiQv4PShg7SY4K5cVEGNy7KoKiunZAAJ4er21i1p4L59lPZM1MieX1vBcYYxMMdVM3HejhYZQ1VGQPvHqmjp8/F8px4gvxP//DfaKM9CKXUuDc7NZLsxDCyE8KZGh92wj4RcZcaX559vPZTVlwod12U7V7lrv95izlpkbR09rKjpPGk73OwqoW5P/4Xf91ayswUqyzIP/ZWcvvTO3jSnvAeS7QHoZQa935y1Sxcxpp49rSWdXp0MPk1bSzPOXnp4k8vnsTc9Ch3EvnYnGR+s+4I335xL6u/fv4JvYIj1W3u7dxJ0VS3dPLmgWoA1u6v5ssXTBnuS/Mq7UEopcY9P6eDAD8H/k6Hx2GezLhQ/J3CUnvd7IGCA5wszIxxv44I8ueHH59JYV07f91ayjW/2+h+EG/gA3lJkcFkxYW6n+DeUdpIXZvnB/ZGK+1BKKUmvK+smMIVs5MJDxraynPLc+Lwcwi/XHOIju4+XtlVzrqD1e6qsFfPS+HaBakU1raxrbiRAD8H3b0u3jlcyycXpHnzUoaV9iCUUhNeQnjQCb2EMwkJ8GNuehQd3VYxv9+vL2BLYQP/3FdFVlwoD944n4SIIDLtQoJLp8QSGuBkzxmenxhsf0UL1S0nFxMcKZoglFLqA+h/aA6gvr0bgO4+F0kD1paYbCeInMRwZqZGnvH22MFu/fNWfvHGIY/7dpQ0UlDb5nHfcNEEoZRSH8Dls5JJjgw6IVEAJEceTxDZidYdU9MSw5mdGsn+yhZ6BzwXYYzhOy/uZVNBHWCtQ9Ff96murYva1i6KB5Un31rUwLWPbOLaRzZxz/O7vXJt/TRBKKXUBzA7LZLN372YS6YnAtBfMzBpQIKYmhDOM19czCfmpTA7NZLOHhcHq1q59fGtbMqvo7qli+e3H+Xmx97jx6/lMfO+NTy0Lh+Aw/azFIOLA37/lX3utiM13u1B6CS1Ukp9CEsmxxLgdLByVhKr9lSckCDg+MJGs+3nKf6+s5z1h2tZf7iWv3xhsfu4P28sJjY0gGe3lrAxv47yJmudiprWLjp7+gjyd9LnMhTXdXDbskziwgL56eoDNHV0EzXgyfDhpD0IpZT6EGanRZJ3/2VcMTsZ4IQ5iIGyYkMJC/Tj1d3l7rY1eVXu7UtnJHL/VbOobulia3GDO0EA7u2qlk66+1xMig11r6RXVHfiENRw0h6EUkp9SP5OBxfkxHPXhVP5SHacx2McDmFmSgTvFTW4257dWkpEkB9r77mA2NAAel2GmFCrHEhh7fEf/EcbOiipb+dApTXslBkbQoKdiIrq2t1lQIab9iCUUmoYBAc4+eZl0wgJOPXv3bNTrWGmaYnhXHROAn0uw9SEMBIjgvCzH+Jbc/dyVn/tfMKD/PjkglTAWtnuG8/v4ZdrrDuaMuNCyYgJwSFQ7MUehFcThIisFJFDIpIvIvd62P85EakVkd321xcH7PuFiOSJyAEReUg8VcVSSqkxpH8eYlpSOFfPt374ZyecuKRpfHggQf5O9t53Kb+6bi4BTgev76t0P5EN1jBWgJ+DtOgQCsfiEJOIOIGHgY8CZcA2EVlljNk/6NDnjTF3DTp3KbAMmGM3vQtcALztrXiVUsrb5qRZBf+mJYXz0emJTI4LZenUk8t7gFVEUARyksLYOmBYCnAvs5qTGEZeRYvX4vVmD2IRkG+MKTTGdAPPAVcN8VwDBAEBQCDgD1R7JUqllBohWXGh/PpTc7l5UQbBAU7+/c0VXDUv9bTnfP9jMwCYZy9g1D9MBdYdUkV17ZTWe2edbG9OUqcCRwe8LgMWezjuWhFZDhwGvmGMOWqM2SwibwGVgAC/NcYcGHyiiNwO3A6QkZEx3PErpdSwu2b+2dViWjI5lgeun0tqdDAzUyLwcxz/vf4Cu/rs+iO13BI7aVjjBO/2IDzNGQyus/sakGmMmQO8CTwJICJTgelAGlaiuchOIie+mTGPGmNyjTG58fEnl+lVSqnx4Npz01gyOZbwIH+CA45Xo82KCyU9Jpj1h2q98n29mSDKgPQBr9OAioEHGGPqjTH99W8fA861t68Bthhj2owxbcA/gSVejFUppcYcEeHGhRmckxR+5oM/AG8miG1AtohkiUgAcCOwauABIpI84OUngP5hpFLgAhHxExF/rAnqk4aYlFJqorvzwql887JpXnlvr81BGGN6ReQuYA3gBB43xuSJyP3AdmPMKuBrIvIJoBdoAD5nn/4icBGwD2tY6g1jzGveilUppdTJpL9y4FiXm5trtm/f7uswlFJqTBGRHcaYXE/79ElqpZRSHmmCUEop5ZEmCKWUUh5pglBKKeWRJgillFIeaYJQSinl0bi5zVVEaoGSD/EWcUDdMIXja+PlWsbLdYBey2il1wKTjDEeaxWNmwTxYYnI9lPdCzzWjJdrGS/XAXoto5Vey+npEJNSSimPNEEopZTySBPEcY/6OoBhNF6uZbxcB+i1jFZ6LaehcxBKKaU80h6EUkopjzRBKKWU8mjCJwgRWSkih0QkX0Tu9XU8Z0tEikVkn4jsFpHtdluMiKwVkSP2n9G+jtMTEXlcRGpE5P0BbR5jF8tD9ue0V0QW+C7yk53iWn4kIuX2Z7NbRK4YsO+79rUcEpHLfBO1ZyKSLiJvicgBEckTka/b7WPqsznNdYy5z0VEgkRkq4jssa/lx3Z7loi8Z38mz9uLsyEigfbrfHt/5gf6xsaYCfuFtZBRATAZCAD2ADN8HddZXkMxEDeo7RfAvfb2vcDPfR3nKWJfDiwA3j9T7MAVWEvPCtbys+/5Ov4hXMuPgG96OHaG/W8tEMiy/w06fX0NA+JLBhbY2+HAYTvmMfXZnOY6xtznYv/dhtnb/sB79t/134Ab7fbfA1+xt+8Afm9v3wg8/0G+70TvQSwC8o0xhcaYbuA54CofxzQcrgKetLefBK72YSynZIx5B2slwYFOFftVwFPGsgWIGrRkrU+d4lpO5SrgOWNMlzGmCMjH+rc4KhhjKo0xO+3tVqzlflMZY5/Naa7jVEbt52L/3bbZL/3tL4O18uaLdvvgz6T/s3oRuFhE5Gy/70RPEKnA0QGvyzj9P6DRyAD/EpEdInK73ZZojKkE6z8JkOCz6M7eqWIfq5/VXfawy+MDhvrGzLXYQxPzsX5jHbOfzaDrgDH4uYiIU0R2AzXAWqweTpMxptc+ZGC87mux9zcDsWf7PSd6gvCUUcfafb/LjDELgMuBO0Vkua8D8pKx+Fk9AkwB5gGVwAN2+5i4FhEJA14C7jbGtJzuUA9to+Z6PFzHmPxcjDF9xph5QBpWz2a6p8PsP4flWiZ6gigD0ge8TgMqfBTLB2KMqbD/rAFexvqHU93fxbf/rPFdhGftVLGPuc/KGFNt/6d2AY9xfLhi1F+LiPhj/VB9xhjzd7t5zH02nq5jLH8uAMaYJuBtrDmIKBHxs3cNjNd9Lfb+SIY+BOo20RPENiDbvhMgAGsyZ5WPYxoyEQkVkfD+beBS4H2sa7jVPuxW4FXfRPiBnCr2VcBn7TtmlgDN/cMdo9WgcfhrsD4bsK7lRvtOkywgG9g60vGdij1W/SfggDHmfwfsGlOfzamuYyx+LiISLyJR9nYwcAnWnMpbwHX2YYM/k/7P6jrg38aesT4rvp6d9/UX1h0Yh7HG877n63jOMvbJWHdd7AHy+uPHGmtcBxyx/4zxdayniP+vWF38HqzfeL5wqtixuswP25/TPiDX1/EP4VqetmPda/+HTR5w/PfsazkEXO7r+Addy0ewhiP2ArvtryvG2mdzmusYc58LMAfYZcf8PvBDu30yVhLLB14AAu32IPt1vr1/8gf5vlpqQymllEcTfYhJKaXUKWiCUEop5ZEmCKWUUh5pglBKKeWRJgillFIeaYJQahQQkRUi8g9fx6HUQJoglFJKeaQJQqmzICKfsevy7xaRP9gF1NpE5AER2Ski60Qk3j52nohssYvCvTxg/YSpIvKmXdt/p4hMsd8+TEReFJGDIvLMB6m+qdRw0gSh1BCJyHTgU1gFEucBfcCngVBgp7GKJq4H7rNPeQr4jjFmDtaTu/3tzwAPG2PmAkuxnsAGq9ro3VjrEkwGlnn9opQ6Db8zH6KUsl0MnAtss3+5D8YqWOcCnreP+QvwdxGJBKKMMevt9ieBF+zaWanGmJcBjDGdAPb7bTXGlNmvdwOZwLvevyylPNMEodTQCfCkMea7JzSK/GDQcaerX3O6YaOuAdt96P9P5WM6xKTU0K0DrhORBHCv0TwJ6/9Rf0XNm4F3jTHNQKOInG+33wKsN9Z6BGUicrX9HoEiEjKiV6HUEOlvKEoNkTFmv4h8H2sFPwdW5dY7gXZgpojswFq561P2KbcCv7cTQCFwm91+C/AHEbnffo/rR/AylBoyreaq1IckIm3GmDBfx6HUcNMhJqWUUh5pD0IppZRH2oNQSinlkSYIpZRSHmmCUEop5ZEmCKWUUh5pglBKKeXR/wcXsfdxeaJjNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 300\n",
    "aggregated_losses = []\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(Xcattrain, Xnumtrain)\n",
    "    single_loss = loss_function(y_pred, ytrain)\n",
    "    aggregated_losses.append(single_loss)\n",
    "    \n",
    "    if i % 25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "    \n",
    "\n",
    "\n",
    "# Plot the loss over epocs\n",
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val = model(Xcattest, Xnumtest)\n",
    "    loss = loss_function(y_val, ytest)\n",
    "\n",
    "\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "\n",
    "print(confusion_matrix(ytest,y_val))\n",
    "print(classification_report(ytest,y_val))\n",
    "print(accuracy_score(ytest, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times low scoretext is predicted:  1954\n",
      "Times medium scoretext is predicted:  534\n",
      "Times high scoretext is predicted:  639\n",
      "Accuracy of the random forest model:  0.7547169811320755\n"
     ]
    }
   ],
   "source": [
    "# Define the model and fit it to the data\n",
    "forestModel = RandomForestClassifier(n_estimators = 53, max_depth = 14, max_features = \"log2\", criterion = \"entropy\")\n",
    "forestModel.fit(Xcattrain, ytrain)\n",
    "\n",
    "# Predict on the test set\n",
    "forestPreds = forestModel.predict(Xcattest)\n",
    "forestProbs = forestModel.predict_proba(Xcattest)[:, 1]\n",
    "\n",
    "print(\"Times low scoretext is predicted: \", len(forestPreds[forestPreds == 0]))\n",
    "print(\"Times medium scoretext is predicted: \", len(forestPreds[forestPreds == 1]))\n",
    "print(\"Times high scoretext is predicted: \", len(forestPreds[forestPreds == 2]))\n",
    "\n",
    "print(\"Accuracy of the random forest model: \", len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baysian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[290.   2.   0.   1.]]\n",
      "[[170.  73.   0.   1.]]\n",
      "[[160.  40.   1.   1.]]\n",
      "[[27. 62.  0.  0.]]\n",
      "[[55. 59.  1.  0.]]\n",
      "[[290.   2.   0.   1.]]\n",
      "[[289.   2.   0.   1.]]\n",
      "[[300.  54.   1.   0.]]\n",
      "[[249.   1.   0.   0.]]\n",
      "[[265.   1.   1.   0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[259.   1.   0.   1.]]\n",
      "[[232.   1.   1.   1.]]\n",
      "[[1. 1. 1. 1.]]\n",
      "[[32.  1.  1.  1.]]\n",
      "[[17.  1.  1.  0.]]\n",
      "[[65.  1.  0.  0.]]\n",
      "[[51.  1.  1.  0.]]\n",
      "[[289.   1.   1.   1.]]\n",
      "[[275.   1.   0.   1.]]\n",
      "[[215.   1.   0.   0.]]\n",
      "[[225.   1.   0.   0.]]\n",
      "[[92.  1.  1.  0.]]\n",
      "[[115.   1.   0.   0.]]\n",
      "[[103.   1.   0.   0.]]\n",
      "[[177.   1.   1.   0.]]\n",
      "[[193.   1.   1.   0.]]\n",
      "[[146.   1.   1.   0.]]\n",
      "[[109.   1.   1.   1.]]\n",
      "[[11.  1.  0.  1.]]\n",
      "[[1. 1. 0. 0.]]\n",
      "[[159.   1.   0.   1.]]\n",
      "[[133.   1.   0.   1.]]\n",
      "[[82.  1.  0.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[68.  1.  1.  1.]]\n",
      "[[184.   1.   0.   1.]]\n",
      "[[150.   1.   0.   0.]]\n",
      "[[204.   1.   1.   1.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[76.  1.  1.  0.]]\n",
      "[[263.   8.   0.   1.]]\n",
      "[[108. 109.   0.   0.]]\n",
      "[[229. 109.   0.   0.]]\n",
      "[[  1. 109.   1.   1.]]\n",
      "[[300. 109.   0.   1.]]\n",
      "[[ 1. 23.  1.  0.]]\n",
      "[[101.  25.   0.   0.]]\n",
      "[[ 53. 109.   0.   1.]]\n",
      "[[300.  13.   0.   1.]]\n",
      "[[201.  21.   1.   0.]]\n",
      "[[164. 109.   1.   1.]]\n",
      "[[51. 16.  0.  1.]]\n",
      "[[262.  82.   1.   0.]]\n",
      "[[139.  13.   0.   0.]]\n",
      "[[103.  71.   1.   1.]]\n",
      "[[125.   1.   1.   0.]]\n",
      "[[213.  72.   1.   1.]]\n",
      "[[167.   1.   1.   0.]]\n",
      "[[42.  1.  0.  1.]]\n",
      "[[39.  1.  1.  0.]]\n",
      "[[25.  1.  0.  0.]]\n",
      "[[255.   1.   1.   0.]]\n",
      "[[ 1. 75.  1.  1.]]\n",
      "[[214.   1.   1.   1.]]\n",
      "[[121.   1.   0.   1.]]\n",
      "[[240.   1.   0.   1.]]\n",
      "[[172.   1.   0.   1.]]\n",
      "[[59.  1.  1.  1.]]\n",
      "[[7. 1. 1. 0.]]\n",
      "[[144.   1.   0.   1.]]\n",
      "[[196. 109.   1.   0.]]\n",
      "[[152.   1.   1.   1.]]\n",
      "[[188.   1.   1.   1.]]\n",
      "[[54.  1.  0.  1.]]\n",
      "[[98.  1.  1.  1.]]\n",
      "[[237.   1.   1.   0.]]\n",
      "[[34.  1.  0.  0.]]\n",
      "[[275.   1.   1.   0.]]\n",
      "[[300.  79.   1.   1.]]\n",
      "[[209.   1.   1.   0.]]\n",
      "[[270.   1.   0.   0.]]\n",
      "[[247.   1.   1.   1.]]\n",
      "[[93.  1.  0.  1.]]\n",
      "[[196.   1.   0.   1.]]\n",
      "[[20.  1.  0.  1.]]\n",
      "[[265. 109.   1.   0.]]\n",
      "[[138.   1.   0.   0.]]\n",
      "[[165.   1.   1.   1.]]\n",
      "[[73.  1.  0.  1.]]\n",
      "[[137.   1.   1.   1.]]\n",
      "[[136.  82.   1.   1.]]\n",
      "[[47.  1.  1.  1.]]\n",
      "[[202.   1.   0.   0.]]\n",
      "[[270.   1.   1.   1.]]\n",
      "[[87.  1.  1.  1.]]\n",
      "[[243.   1.   1.   0.]]\n",
      "[[208.   1.   0.   1.]]\n",
      "[[118.   1.   1.   0.]]\n",
      "[[86.  1.  0.  0.]]\n",
      "[[188.   1.   0.   0.]]\n",
      "[[232.   1.   0.   0.]]\n",
      "[[107.   1.   1.   0.]]\n",
      "[[4. 1. 0. 1.]]\n",
      "[[132.   1.   1.   0.]]\n",
      "[[75. 87.  0.  0.]]\n",
      "[[15.  1.  0.  0.]]\n",
      "[[70.  1.  1.  0.]]\n",
      "[[158.   1.   1.   0.]]\n",
      "[[29. 91.  1.  1.]]\n",
      "[[172.   1.   1.   1.]]\n",
      "[[37.  1.  1.  1.]]\n",
      "[[221.   1.   0.   1.]]\n",
      "[[58.  1.  0.  0.]]\n",
      "[[63.  1.  1.  0.]]\n",
      "[[180.   1.   1.   1.]]\n",
      "[[253.   1.   0.   1.]]\n",
      "[[280.   1.   0.   0.]]\n",
      "[[47.  1.  0.  0.]]\n",
      "[[184.   1.   1.   0.]]\n",
      "[[97.  1.  0.  0.]]\n",
      "[[226.   1.   1.   1.]]\n",
      "[[25.  1.  1.  1.]]\n",
      "[[128.   1.   0.   1.]]\n",
      "[[82.  1.  1.  0.]]\n",
      "[[163.   1.   0.   0.]]\n",
      "[[115.   1.   1.   1.]]\n",
      "[[280.   1.   1.   1.]]\n",
      "[[29.  1.  0.  1.]]\n",
      "[[199.   1.   1.   0.]]\n",
      "[[265.   1.   0.   1.]]\n",
      "[[229.   1.   1.   0.]]\n",
      "[[14.  1.  1.  1.]]\n",
      "[[126.   1.   0.   0.]]\n",
      "[[142.   1.   0.   0.]]\n",
      "[[62.  1.  0.  1.]]\n",
      "[[245.   1.   0.   1.]]\n",
      "[[78.  1.  1.  1.]]\n",
      "[[261.   1.   1.   1.]]\n",
      "[[9. 1. 0. 0.]]\n",
      "[[261.   1.   0.   0.]]\n",
      "[[206.   1.   0.   0.]]\n",
      "[[170.   1.   0.   0.]]\n",
      "[[199.   1.   1.   1.]]\n",
      "[[190.  53.   0.   0.]]\n",
      "[[177.   1.   0.   1.]]\n",
      "[[285.   1.   1.   0.]]\n",
      "[[180.  11.   1.   1.]]\n",
      "[[136.  54.   0.   0.]]\n",
      "[[104.   1.   1.   1.]]\n",
      "[[78.  1.  0.  0.]]\n",
      "[[44.  1.  1.  0.]]\n",
      "[[112.   1.   1.   0.]]\n",
      "[[235.   1.   0.   1.]]\n",
      "[[284.   1.   0.   1.]]\n",
      "[[254.   1.   1.   1.]]\n",
      "[[55.  1.  1.  0.]]\n",
      "[[229.   1.   0.   1.]]\n",
      "[[162.   1.   1.   0.]]\n",
      "[[240.   1.   0.   0.]]\n",
      "[[146.   1.   0.   0.]]\n",
      "[[138. 107.   1.   0.]]\n",
      "[[180.   1.   0.   0.]]\n",
      "[[107.   1.   0.   1.]]\n",
      "[[89.  1.  0.  1.]]\n",
      "[[154.   1.   0.   1.]]\n",
      "[[174.   1.   0.   0.]]\n",
      "[[116.   1.   0.   1.]]\n",
      "[[125.   1.   0.   1.]]\n",
      "[[29.  1.  1.  0.]]\n",
      "[[284.  68.   1.   0.]]\n",
      "[[239.   1.   1.   1.]]\n",
      "[[148.   1.   1.   1.]]\n",
      "[[143.   1.   1.   1.]]\n",
      "[[288.   1.   0.   0.]]\n",
      "[[127.   1.   1.   1.]]\n",
      "[[37.  1.  0.  1.]]\n",
      "[[218.   1.   1.   1.]]\n",
      "[[2. 1. 1. 0.]]\n",
      "[[110.   1.   0.   0.]]\n",
      "[[22.  1.  1.  0.]]\n",
      "[[68.  1.  0.  0.]]\n",
      "[[4. 1. 1. 1.]]\n",
      "[[15.  1.  0.  1.]]\n",
      "[[84. 50.  0.  1.]]\n",
      "[[242.   1.   1.   1.]]\n",
      "[[290.   2.   1.   0.]]\n",
      "[[168.   1.   0.   1.]]\n",
      "[[11.  1.  1.  0.]]\n",
      "[[130.   1.   0.   0.]]\n",
      "[[ 82. 109.   1.   0.]]\n",
      "[[211.   1.   0.   0.]]\n",
      "[[51.  1.  0.  1.]]\n",
      "[[73.  1.  1.  1.]]\n",
      "[[21.  1.  1.  1.]]\n",
      "[[8. 1. 0. 1.]]\n",
      "[[191.   1.   0.   1.]]\n",
      "[[96.  1.  1.  1.]]\n",
      "[[101.   1.   1.   0.]]\n",
      "[[270.   1.   1.   0.]]\n",
      "[[266.   1.   1.   1.]]\n",
      "[[257.   1.   1.   1.]]\n",
      "[[233.  87.   1.   0.]]\n",
      "[[203.   1.   1.   0.]]\n",
      "[[223.   1.   1.   1.]]\n",
      "[[250.   1.   0.   1.]]\n",
      "[[42.  1.  0.  0.]]\n",
      "[[156.   1.   1.   1.]]\n",
      "[[214.   1.   1.   0.]]\n",
      "[[89.  1.  1.  0.]]\n",
      "[[175.   1.   1.   1.]]\n",
      "[[139.   1.   1.   0.]]\n",
      "[[153.   1.   1.   0.]]\n",
      "[[140.   1.   0.   1.]]\n",
      "[[73.  1.  0.  0.]]\n",
      "[[197.   1.   0.   0.]]\n",
      "[[196.   1.   1.   1.]]\n",
      "[[171.   1.   1.   0.]]\n",
      "[[ 24. 109.   1.   0.]]\n",
      "[[258.   1.   1.   0.]]\n",
      "[[42.  1.  1.  1.]]\n",
      "[[196.  87.   1.   0.]]\n",
      "[[45.  1.  0.  1.]]\n",
      "[[119.   1.   1.   1.]]\n",
      "[[32.  1.  0.  0.]]\n",
      "[[250.   1.   1.   0.]]\n",
      "[[20.  3.  0.  1.]]\n",
      "[[ 1. 48.  1.  0.]]\n",
      "[[78. 14.  1.  0.]]\n",
      "[[90.  1.  1.  1.]]\n",
      "[[161.   1.   1.   1.]]\n",
      "[[157.   1.   0.   0.]]\n",
      "[[280.  92.   0.   0.]]\n",
      "[[101.   1.   0.   1.]]\n",
      "[[217.   1.   0.   1.]]\n",
      "[[92.  1.  0.  0.]]\n",
      "[[ 6. 90.  0.  0.]]\n",
      "[[225.   1.   1.   0.]]\n",
      "[[54.  1.  1.  1.]]\n",
      "[[25.  1.  0.  1.]]\n",
      "[[186.   1.   0.   1.]]\n",
      "[[83.  1.  1.  1.]]\n",
      "[[277.   1.   1.   1.]]\n",
      "[[53.  1.  0.  0.]]\n",
      "[[133.   1.   1.   1.]]\n",
      "[[118.   1.   0.   0.]]\n",
      "[[273.   1.   0.   0.]]\n",
      "[[279.   1.   1.   0.]]\n",
      "[[58.  1.  0.  1.]]\n",
      "[[66.  1.  0.  1.]]\n",
      "[[254.   1.   0.   0.]]\n",
      "[[184.   1.   1.   1.]]\n",
      "[[219.  49.   0.   0.]]\n",
      "[[6. 1. 0. 0.]]\n",
      "[[286.   1.   1.   1.]]\n",
      "[[112.   1.   0.   1.]]\n",
      "[[64.  1.  1.  1.]]\n",
      "[[21.  1.  0.  0.]]\n",
      "[[33.  1.  1.  0.]]\n",
      "[[204.   1.   0.   1.]]\n",
      "[[235.   1.   1.   1.]]\n",
      "[[283.   1.   1.   0.]]\n",
      "[[110.  45.   1.   1.]]\n",
      "[[236.   1.   0.   0.]]\n",
      "[[154.  96.   0.   0.]]\n",
      "[[288.   4.   1.   1.]]\n",
      "[[300.  73.   0.   0.]]\n",
      "[[229.  16.   0.   0.]]\n",
      "[[246.  58.   0.   1.]]\n",
      "[[113.   9.   1.   1.]]\n",
      "[[300.   4.   0.   0.]]\n",
      "[[51. 82.  0.  0.]]\n",
      "[[39. 42.  0.  1.]]\n",
      "[[154.  62.   1.   0.]]\n",
      "[[115.  89.   0.   0.]]\n",
      "[[211.   6.   0.   1.]]\n",
      "[[241.  35.   0.   0.]]\n",
      "[[198.  37.   1.   0.]]\n",
      "[[63. 39.  0.  0.]]\n",
      "[[266.  56.   0.   0.]]\n",
      "[[177.  94.   1.   0.]]\n",
      "[[289.   1.   1.   0.]]\n",
      "[[158.   8.   1.   0.]]\n",
      "[[247. 109.   1.   1.]]\n",
      "[[76. 68.  1.  0.]]\n",
      "[[94. 91.  1.  1.]]\n",
      "[[243.   7.   1.   0.]]\n",
      "[[214.  96.   1.   1.]]\n",
      "[[122.  66.   0.   1.]]\n",
      "[[222.  33.   1.   1.]]\n",
      "[[173.  52.   1.   1.]]\n",
      "[[131.  36.   0.   1.]]\n",
      "[[19. 41.  0.  1.]]\n",
      "[[190.  70.   1.   0.]]\n",
      "[[300.  36.   0.   1.]]\n",
      "[[280. 109.   1.   1.]]\n",
      "[[235.  68.   0.   0.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[62.  5.  0.  0.]]\n",
      "[[92.  7.  1.  1.]]\n",
      "[[84. 33.  1.  0.]]\n",
      "[[39. 25.  1.  0.]]\n",
      "[[18. 76.  0.  1.]]\n",
      "[[194.   1.   0.   0.]]\n",
      "[[123. 109.   1.   1.]]\n",
      "[[181.  34.   0.   1.]]\n",
      "[[122.   1.   1.   0.]]\n",
      "[[76.  1.  0.  1.]]\n",
      "[[290.   2.   0.   0.]]\n",
      "[[96.  1.  1.  0.]]\n",
      "[[39. 73.  1.  1.]]\n",
      "[[70.  1.  0.  1.]]\n",
      "[[282.  49.   0.   1.]]\n",
      "[[1. 6. 1. 0.]]\n",
      "[[278.   1.   0.   1.]]\n",
      "[[1. 4. 0. 0.]]\n",
      "[[ 1. 12.  1.  0.]]\n",
      "[[262.  38.   1.   1.]]\n",
      "[[291.   2.   1.   0.]]\n",
      "[[ 1. 34.  1.  1.]]\n",
      "[[40.  7.  1.  1.]]\n",
      "[[154.  79.   1.   1.]]\n",
      "[[249.  92.   0.   0.]]\n",
      "[[180. 109.   1.   0.]]\n",
      "[[ 68. 109.   0.   1.]]\n",
      "[[288.   1.   0.   1.]]\n",
      "[[ 38. 109.   0.   0.]]\n",
      "[[ 9. 61.  1.  1.]]\n",
      "[[212. 109.   1.   1.]]\n",
      "[[300.  94.   1.   0.]]\n",
      "[[52. 30.  0.  0.]]\n",
      "[[205.  57.   1.   1.]]\n",
      "[[211.   1.   1.   1.]]\n",
      "[[98. 56.  0.  0.]]\n",
      "[[250.  73.   1.   1.]]\n",
      "[[117.  30.   1.   1.]]\n",
      "[[277.   1.   0.   0.]]\n",
      "[[129.   5.   1.   1.]]\n",
      "[[146.  31.   0.   0.]]\n",
      "[[193.   5.   0.   0.]]\n",
      "[[6. 1. 1. 1.]]\n",
      "[[267.   1.   0.   0.]]\n",
      "[[135.   1.   1.   0.]]\n",
      "[[234.  49.   1.   1.]]\n",
      "[[60. 94.  1.  1.]]\n",
      "[[37.  1.  0.  0.]]\n",
      "[[213.   1.   0.   1.]]\n",
      "[[88. 76.  0.  1.]]\n",
      "[[163.   1.   0.   1.]]\n",
      "[[63. 74.  0.  1.]]\n",
      "[[97. 39.  1.  0.]]\n",
      "[[284.   1.   1.   1.]]\n",
      "[[224.   1.   0.   1.]]\n",
      "[[169.   1.   1.   1.]]\n",
      "[[69. 53.  1.  0.]]\n",
      "[[222.   1.   0.   0.]]\n",
      "[[1. 7. 1. 1.]]\n",
      "[[19. 27.  1.  0.]]\n",
      "[[279.  34.   0.   0.]]\n",
      "[[270.  71.   0.   1.]]\n",
      "[[139.  68.   0.   0.]]\n",
      "[[ 95. 109.   1.   0.]]\n",
      "[[122.  51.   1.   0.]]\n",
      "[[151. 109.   1.   0.]]\n",
      "[[168.  26.   0.   0.]]\n",
      "[[130.  95.   0.   0.]]\n",
      "[[70. 27.  0.  1.]]\n",
      "[[44. 94.  1.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[1. 1. 1. 0.]]\n",
      "[[81.  1.  1.  0.]]\n",
      "[[52.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[70.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[147.  46.   1.   1.]]\n",
      "[[41. 57.  0.  1.]]\n",
      "[[149.   1.   1.   0.]]\n",
      "[[256.   1.   0.   1.]]\n",
      "[[209.  31.   0.   1.]]\n",
      "[[207.   1.   1.   1.]]\n",
      "[[187.   1.   1.   0.]]\n",
      "[[160.   1.   0.   0.]]\n",
      "[[123.   1.   0.   0.]]\n",
      "[[251.  46.   0.   0.]]\n",
      "[[136.   1.   0.   1.]]\n",
      "[[265.  95.   1.   1.]]\n",
      "[[18.  1.  0.  0.]]\n",
      "[[220.  83.   0.   0.]]\n",
      "[[28.  1.  1.  1.]]\n",
      "[[234.   1.   1.   0.]]\n",
      "[[223.  62.   1.   1.]]\n",
      "[[182.  81.   0.   1.]]\n",
      "[[130.   1.   1.   1.]]\n",
      "[[ 14. 101.   0.   1.]]\n",
      "[[185.   1.   0.   0.]]\n",
      "[[48.  1.  0.  1.]]\n",
      "[[165.  87.   1.   0.]]\n",
      "[[149.   1.   0.   1.]]\n",
      "[[285.  81.   0.   1.]]\n",
      "[[181.   1.   1.   0.]]\n",
      "[[190.   1.   1.   0.]]\n",
      "[[100.   1.   0.   0.]]\n",
      "[[121.  78.   1.   1.]]\n",
      "[[273.   1.   1.   1.]]\n",
      "[[85.  1.  0.  1.]]\n",
      "[[193.   1.   1.   1.]]\n",
      "[[174.   1.   1.   0.]]\n",
      "[[178.   1.   1.   1.]]\n",
      "[[201.   1.   1.   1.]]\n",
      "[[39.  1.  0.  1.]]\n",
      "[[246.   1.   0.   0.]]\n",
      "[[101.   1.   1.   1.]]\n",
      "[[10.  1.  1.  1.]]\n",
      "[[228.   1.   0.   0.]]\n",
      "[[264.   1.   0.   0.]]\n",
      "[[257.   1.   0.   0.]]\n",
      "[[23.  1.  0.  1.]]\n",
      "[[131.   1.   0.   1.]]\n",
      "[[61.  1.  1.  1.]]\n",
      "[[123.   1.   1.   1.]]\n",
      "[[66.  1.  1.  0.]]\n",
      "[[112.   1.   1.   1.]]\n",
      "[[202.  76.   0.   1.]]\n",
      "[[243.   1.   0.   0.]]\n",
      "[[218.   1.   0.   0.]]\n",
      "[[285.   1.   0.   0.]]\n",
      "[[134.   1.   0.   0.]]\n",
      "[[18.  1.  1.  1.]]\n",
      "[[181.   1.   0.   1.]]\n",
      "[[182.   1.   0.   0.]]\n",
      "[[33.  1.  0.  1.]]\n",
      "[[262.   1.   1.   0.]]\n",
      "[[216.   1.   1.   0.]]\n",
      "[[196.   1.   1.   0.]]\n",
      "[[52. 46.  1.  1.]]\n",
      "[[85.  1.  1.  0.]]\n",
      "[[179.  63.   0.   0.]]\n",
      "[[48.  1.  1.  0.]]\n",
      "[[200.   1.   0.   1.]]\n",
      "[[128.   1.   1.   0.]]\n",
      "[[111.  60.   1.   1.]]\n",
      "[[ 1. 63.  0.  0.]]\n",
      "[[96.  1.  0.  1.]]\n",
      "[[3. 1. 0. 0.]]\n",
      "[[252.   1.   1.   0.]]\n",
      "[[61.  1.  0.  0.]]\n",
      "[[237.  99.   0.   1.]]\n",
      "[[262.   1.   0.   1.]]\n",
      "[[165.   1.   1.   0.]]\n",
      "[[231.   1.   1.   0.]]\n",
      "[[83.  1.  0.  0.]]\n",
      "[[264.   1.   1.   1.]]\n",
      "[[246.   1.   1.   0.]]\n",
      "[[19.  1.  1.  0.]]\n",
      "[[28.  1.  0.  0.]]\n",
      "[[251.   1.   1.   1.]]\n",
      "[[28. 49.  1.  1.]]\n",
      "[[104.   1.   0.   1.]]\n",
      "[[106.   1.   0.   0.]]\n",
      "[[29. 34.  1.  0.]]\n",
      "[[44.  1.  0.  0.]]\n",
      "[[272.  44.   0.   0.]]\n",
      "[[81. 97.  1.  1.]]\n",
      "[[251.   1.   0.   0.]]\n",
      "[[190.  98.   0.   1.]]\n",
      "[[26.  1.  1.  0.]]\n",
      "[[143.   1.   1.   0.]]\n",
      "[[190.  26.   1.   0.]]\n",
      "[[153.   1.   0.   0.]]\n",
      "[[166.   1.   0.   0.]]\n",
      "[[94.  1.  0.  0.]]\n",
      "[[238.   1.   0.   1.]]\n",
      "[[59.  1.  1.  0.]]\n",
      "[[206.   1.   1.   0.]]\n",
      "[[247.   1.   0.   1.]]\n",
      "[[75.  1.  1.  1.]]\n",
      "[[88. 63.  0.  1.]]\n",
      "[[191.   1.   0.   0.]]\n",
      "[[269.   1.   0.   1.]]\n",
      "[[140.   1.   1.   1.]]\n",
      "[[44.  1.  1.  1.]]\n",
      "[[105.  98.   1.   1.]]\n",
      "[[10. 33.  1.  0.]]\n",
      "[[212.   1.   1.   0.]]\n",
      "[[79.  1.  0.  1.]]\n",
      "[[108.   1.   0.   0.]]\n",
      "[[151.   1.   0.   1.]]\n",
      "[[36.  1.  1.  0.]]\n",
      "[[254.  27.   0.   0.]]\n",
      "[[ 1. 98.  1.  0.]]\n",
      "[[3. 6. 1. 0.]]\n",
      "[[227.   4.   1.   1.]]\n",
      "[[234.  25.   0.   1.]]\n",
      "[[104.  83.   0.   0.]]\n",
      "[[116.   1.   1.   0.]]\n",
      "[[143.  93.   1.   1.]]\n",
      "[[230.   1.   0.   0.]]\n",
      "[[272.   1.   0.   1.]]\n",
      "[[258.  64.   1.   0.]]\n",
      "[[290.  41.   0.   0.]]\n",
      "[[158.   1.   1.   1.]]\n",
      "[[50.  1.  1.  1.]]\n",
      "[[300.  25.   1.   0.]]\n",
      "[[161.  53.   0.   0.]]\n",
      "[[241.   1.   1.   0.]]\n",
      "[[171.   4.   0.   0.]]\n",
      "[[146.   4.   0.   1.]]\n",
      "[[134.  24.   1.   0.]]\n",
      "[[290. 101.   0.   0.]]\n",
      "[[268.   1.   1.   0.]]\n",
      "[[203.  98.   0.   0.]]\n",
      "[[75. 40.  1.  1.]]\n",
      "[[232.   1.   0.   1.]]\n",
      "[[155.   1.   1.   0.]]\n",
      "[[145.   1.   1.   1.]]\n",
      "[[13.  1.  0.  0.]]\n",
      "[[14.  1.  1.  0.]]\n",
      "[[30. 80.  0.  0.]]\n",
      "[[110.   1.   0.   1.]]\n",
      "[[219.  23.   1.   0.]]\n",
      "[[16.  1.  1.  1.]]\n",
      "[[174.   1.   0.   1.]]\n",
      "[[155.  24.   1.   1.]]\n",
      "[[147.   1.   0.   1.]]\n",
      "[[177.   1.   0.   0.]]\n",
      "[[207.  45.   0.   0.]]\n",
      "[[16. 52.  0.  0.]]\n",
      "[[151.   1.   1.   0.]]\n",
      "[[204.   1.   0.   0.]]\n",
      "[[225.  97.   0.   0.]]\n",
      "[[ 11. 109.   1.   0.]]\n",
      "[[222.   1.   1.   0.]]\n",
      "[[99.  1.  0.  1.]]\n",
      "[[168.   1.   0.   0.]]\n",
      "[[73.  1.  1.  0.]]\n",
      "[[193.   1.   0.   1.]]\n",
      "[[191.   1.   1.   1.]]\n",
      "[[57.  1.  1.  1.]]\n",
      "[[35.  1.  1.  1.]]\n",
      "[[206.   1.   0.   1.]]\n",
      "[[186.   1.   1.   1.]]\n",
      "[[155.   1.   0.   0.]]\n",
      "[[291.  60.   0.   1.]]\n",
      "[[104.   1.   1.   0.]]\n",
      "[[189.   1.   0.   1.]]\n",
      "[[18. 88.  1.  0.]]\n",
      "[[89.  1.  0.  0.]]\n",
      "[[160.   1.   1.   0.]]\n",
      "[[171.  40.   0.   0.]]\n",
      "[[220.   1.   1.   1.]]\n",
      "[[29. 18.  1.  0.]]\n",
      "[[239.   1.   1.   0.]]\n",
      "[[17.  1.  0.  1.]]\n",
      "[[51. 70.  0.  1.]]\n",
      "[[107.  34.   0.   1.]]\n",
      "[[230.   1.   1.   1.]]\n",
      "[[165.   1.   0.   1.]]\n",
      "[[41.  1.  1.  0.]]\n",
      "[[71.  1.  0.  0.]]\n",
      "[[245.   1.   1.   1.]]\n",
      "[[120.   1.   0.   0.]]\n",
      "[[56.  1.  0.  0.]]\n",
      "[[274.  82.   1.   0.]]\n",
      "[[87.  1.  0.  1.]]\n",
      "[[267.  28.   0.   0.]]\n",
      "[[93.  1.  1.  1.]]\n",
      "[[113.   1.   0.   0.]]\n",
      "[[142.   1.   0.   1.]]\n",
      "[[227.  75.   0.   1.]]\n",
      "[[255. 102.   1.   1.]]\n",
      "[[210.   1.   0.   1.]]\n",
      "[[86. 23.  0.  1.]]\n",
      "[[114.   1.   1.   0.]]\n",
      "[[161.   1.   0.   1.]]\n",
      "[[11.  1.  0.  0.]]\n",
      "[[242.  80.   0.   0.]]\n",
      "[[80.  1.  1.  1.]]\n",
      "[[1. 1. 0. 1.]]\n",
      "[[282.   1.   0.   0.]]\n",
      "[[167.  99.   0.   1.]]\n",
      "[[81.  1.  0.  0.]]\n",
      "[[166.  63.   1.   1.]]\n",
      "[[2. 1. 0. 1.]]\n",
      "[[209.   1.   1.   1.]]\n",
      "[[118. 100.   1.   0.]]\n",
      "[[28.  4.  1.  0.]]\n",
      "[[ 9. 71.  1.  0.]]\n",
      "[[201.   1.   1.   0.]]\n",
      "[[120.  39.   0.   0.]]\n",
      "[[ 91. 100.   0.   0.]]\n",
      "[[30.  1.  0.  0.]]\n",
      "[[66. 63.  0.  1.]]\n",
      "[[144.   1.   0.   0.]]\n",
      "[[70. 99.  0.  0.]]\n",
      "[[260.   1.   1.   0.]]\n",
      "[[292.  88.   1.   0.]]\n",
      "[[121.   1.   1.   1.]]\n",
      "[[79.  1.  1.  0.]]\n",
      "[[40.  1.  0.  0.]]\n",
      "[[157.   1.   0.   1.]]\n",
      "[[202.  65.   0.   0.]]\n",
      "[[ 31. 100.   0.   0.]]\n",
      "[[ 1. 32.  1.  0.]]\n",
      "[[130.   1.   1.   0.]]\n",
      "[[135.   1.   1.   1.]]\n",
      "[[242.   1.   0.   1.]]\n",
      "[[179.   1.   0.   1.]]\n",
      "[[167.   1.   1.   1.]]\n",
      "[[60.  1.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "## define the domain of the considered parameters\n",
    "n_estimators = tuple(np.arange(1,301,1, dtype= np.int))\n",
    "# print(n_estimators)\n",
    "max_depth = tuple(np.arange(1,110,1, dtype= np.int))\n",
    "# max_features = ('log2', 'sqrt', None)\n",
    "max_features = (0, 1)\n",
    "# criterion = ('gini', 'entropy')\n",
    "criterion = (0, 1)\n",
    "\n",
    "\n",
    "# define the dictionary for GPyOpt\n",
    "domain = [{'n_estimators': 'var_1',  'type': 'discrete',     'domain': n_estimators},\n",
    "          {'max_depth': 'var_2',     'type': 'discrete',     'domain': max_depth},\n",
    "          {'max_features': 'var_3',  'type': 'categorical',  'domain': max_features},\n",
    "          {'criterion': 'var_4',     'type': 'categorical',  'domain': criterion}]\n",
    "\n",
    "\n",
    "## we have to define the function we want to maximize --> validation accuracy, \n",
    "## note it should take a 2D ndarray but it is ok that it assumes only one point\n",
    "## in this setting\n",
    "def objective_function(x): \n",
    "    print(x)\n",
    "    # we have to handle the categorical variables that is convert 0/1 to labels\n",
    "    # log2/sqrt and gini/entropy\n",
    "    \n",
    "    param = x[0]\n",
    "    \n",
    "    if param[2] == 0:\n",
    "        var_3 = \"log2\"\n",
    "    else:\n",
    "        var_3 = \"sqrt\"\n",
    "    \n",
    "    if param[3] == 0:\n",
    "        var_4 = \"gini\"\n",
    "    else:\n",
    "        var_4 = \"entropy\"\n",
    "        \n",
    "        \n",
    "#fit the model\n",
    "    model = RandomForestClassifier(n_estimators = int(param[0]), criterion = var_4, max_depth = int(param[1]), max_features = var_3)\n",
    "    model.fit(Xcattrain, ytrain)\n",
    "    forestPreds = model.predict(Xcattest)\n",
    "    accuracy = len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "opt = GPyOpt.methods.BayesianOptimization(f = objective_function,   # function to optimize\n",
    "                                              domain = domain,         # box-constrains of the problem\n",
    "                                              acquisition_type = \"EI\",      # Select acquisition function MPI, EI, LCB\n",
    "                                             )\n",
    "opt.acquisition.exploration_weight=.1\n",
    "\n",
    "opt.run_optimization(max_iter = 100) \n",
    "\n",
    "\n",
    "x_best = opt.X[np.argmin(opt.Y)]\n",
    "print(\"The best parameters obtained: n_estimators=\" + str(x_best[0]) + \", max_depth=\" + str(x_best[1]) + \", max_features=\" + str(\n",
    "    x_best[2])  + \", criterion=\" + str(\n",
    "    x_best[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
