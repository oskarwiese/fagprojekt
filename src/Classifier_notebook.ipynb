{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import GPyOpt\n",
    "s = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "#data = pd.read_csv(\"/home/oskar/Desktop/fagprojekt/compas/compas-scores-raw.csv\")\n",
    "url = \"https://raw.githubusercontent.com/oskarwiese/fagprojekt/master/compas/compas-scores-raw.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/oskarwiese/fagprojekt/master/compas_propublica/compas-scores-two-years.csv\"\n",
    "new_data = pd.read_csv(url)\n",
    "# Til at se p√• dataen \n",
    "#print(data.head)\n",
    "#print(data.columns)\n",
    "\n",
    "# Check if there are any missing values\n",
    "print(np.count_nonzero(data[\"IsDeleted\"] == 1))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'name', 'first', 'last', 'compas_screening_date', 'sex', 'dob',\n",
      "       'age', 'age_cat', 'race', 'juv_fel_count', 'decile_score',\n",
      "       'juv_misd_count', 'juv_other_count', 'priors_count',\n",
      "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
      "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas',\n",
      "       'c_charge_degree', 'c_charge_desc', 'is_recid', 'r_case_number',\n",
      "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
      "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
      "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
      "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
      "       'decile_score.1', 'score_text', 'screening_date',\n",
      "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
      "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
      "       'start', 'end', 'event', 'two_year_recid'],\n",
      "      dtype='object')\n",
      "0        NaN\n",
      "1       (F3)\n",
      "2       (M1)\n",
      "3        NaN\n",
      "4        NaN\n",
      "        ... \n",
      "7209     NaN\n",
      "7210     NaN\n",
      "7211     NaN\n",
      "7212     NaN\n",
      "7213    (M2)\n",
      "Name: r_charge_degree, Length: 7214, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(new_data.columns)\n",
    "print(new_data[\"r_charge_degree\"])\n",
    "def is_plot():\n",
    "    sb.countplot(x = \"score_text\", hue = \"race\", data = new_data)\n",
    "    plt.show()\n",
    "\n",
    "    sb.countplot(x = \"two_year_recid\", hue = \"race\", data = new_data)\n",
    "    plt.show()\n",
    "    sb.countplot(x = \"is_recid\", hue = \"race\", data = new_data)\n",
    "    plt.show()\n",
    "    sb.countplot(x = \"is_violent_recid\", hue = \"race\", data = new_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots():\n",
    "    # Show distribution of different ethnicities and sexes\n",
    "    chart = sb.countplot(x = \"Ethnic_Code_Text\", data = data)\n",
    "    chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    chart.set(xlabel='Ethnicity', ylabel='Count')\n",
    "    plt.show()\n",
    "    \n",
    "    chart = sb.countplot(x = \"Sex_Code_Text\", data = data)\n",
    "    chart.set(xlabel='Sex', ylabel='Count')\n",
    "    plt.show()\n",
    "    \n",
    "    sb.countplot(x = \"Language\", data = data)\n",
    "    plt.show()\n",
    "    \n",
    "    # Showing the distribution of the raw and decile values\n",
    "    plt.xlabel(\"Raw value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Visualization of the values\")\n",
    "    plt.hist(data[\"RawScore\"])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.xlabel(\"Decile value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Visualization of the decile values\")\n",
    "    plt.hist(data[\"DecileScore\"])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #sb.countplot(x = \"RawScore\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Indication that some black people might get higher sentences that white people\n",
    "    sb.countplot(x = \"DecileScore\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    plt.show()\n",
    "    \n",
    "    sb.countplot(x = \"ScoreText\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"Agency_Text\", \"Sex_Code_Text\", \"Ethnic_Code_Text\", \"ScaleSet_ID\", \"AssessmentReason\", \"Language\", \"LegalStatus\", \"CustodyStatus\", \"MaritalStatus\", \"RecSupervisionLevel\"]\n",
    "new_categoricals = [\"c_charge_degree\", \"race\", \"age_cat\", \"sex\", \"is_recid\", \"two_year_recid\"]\n",
    "# Changing date of birth into age,as this should work better in a neural network\n",
    "new_numericals = [\"age\", \"priors_count\"] # \"days_b_screening_arrest\"\n",
    "\n",
    "if s == 1:\n",
    "    ages = [None] * len(data[\"DateOfBirth\"])\n",
    "    for i in range(len(data[\"DateOfBirth\"])):\n",
    "        ages[i] = 20 +(100 - int(data[\"DateOfBirth\"][i].split(\"/\")[2]))\n",
    "    data[\"DateOfBirth\"] = ages\n",
    "    numericals = [\"DateOfBirth\"]\n",
    "    s+=1\n",
    "else:\n",
    "    pass\n",
    "\n",
    "outputs = [\"ScoreText\"]\n",
    "new_outputs = [\"score_text\"]\n",
    "\n",
    "data = data.dropna(axis = 0, how = 'any')\n",
    "data[outputs] = data[outputs].replace('Low',0)\n",
    "data[outputs] = data[outputs].replace('Medium',1)\n",
    "data[outputs] = data[outputs].replace('High',1)\n",
    "data[outputs] = data[outputs].astype(\"category\")\n",
    "\n",
    "\n",
    "new_data[new_outputs] = new_data[new_outputs].replace('Low',0)\n",
    "new_data[new_outputs] = new_data[new_outputs].replace('Medium',1)\n",
    "new_data[new_outputs] = new_data[new_outputs].replace('High',1)\n",
    "new_data[new_outputs] = new_data[new_outputs].astype(\"category\")\n",
    "\n",
    "\n",
    "for category in categoricals:\n",
    "    data[category] = data[category].astype(\"category\")\n",
    "    \n",
    "for new_category in new_categoricals:\n",
    "    new_data[new_category] = new_data[new_category].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[26.],\n",
      "        [26.],\n",
      "        [26.],\n",
      "        ...,\n",
      "        [28.],\n",
      "        [32.],\n",
      "        [32.]])\n",
      "tensor([[69.,  0.],\n",
      "        [34.,  0.],\n",
      "        [24.,  4.],\n",
      "        ...,\n",
      "        [24.,  2.],\n",
      "        [52.,  0.],\n",
      "        [29.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preparing data for pytorch\n",
    "Xcat = []\n",
    "for i in range(len(categoricals)):\n",
    "    Xcat.append(data[categoricals[i]].cat.codes.values)\n",
    "Xcat = torch.tensor(Xcat , dtype = torch.int64).T\n",
    "\n",
    "\n",
    "new_Xcat = []\n",
    "for i in range(len(new_categoricals)):\n",
    "    new_Xcat.append(new_data[new_categoricals[i]].cat.codes.values)\n",
    "new_Xcat = torch.tensor(new_Xcat , dtype = torch.int64).T\n",
    "\n",
    "#Converting the numerical values to a tensor\n",
    "Xnum = np.stack([data[col].values for col in numericals], 1)\n",
    "Xnum = torch.tensor(Xnum, dtype=torch.float)\n",
    "\n",
    "\n",
    "new_Xnum = np.stack([new_data[col].values for col in new_numericals], 1)\n",
    "new_Xnum = torch.tensor(new_Xnum, dtype=torch.float)\n",
    "\n",
    "# Converting the output to tensor\n",
    "y = torch.tensor(data[outputs].values).flatten()\n",
    "new_y = torch.tensor(new_data[new_outputs].values).flatten()\n",
    "\n",
    "# Calculation of embedding sizes for the categorical values in the format (unique categorical values, embedding size (dimension of encoding))\n",
    "categorical_column_sizes = [len(data[column].cat.categories) for column in categoricals]\n",
    "categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]\n",
    "\n",
    "\n",
    "new_categorical_column_sizes = [len(new_data[column].cat.categories) for column in new_categoricals]\n",
    "new_categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in new_categorical_column_sizes]\n",
    "\n",
    "# Train-test split\n",
    "totalnumber = len(Xnum)\n",
    "testnumber = int(totalnumber * 0.2)\n",
    "\n",
    "\n",
    "\n",
    "new_totalnumber = len(new_Xnum)\n",
    "new_testnumber = int(new_totalnumber * 0.2)\n",
    "\n",
    "Xcattrain = Xcat[:totalnumber - testnumber]\n",
    "Xcattest = Xcat[totalnumber - testnumber:totalnumber]\n",
    "Xnumtrain = Xnum[:totalnumber - testnumber]\n",
    "Xnumtest = Xnum[totalnumber - testnumber:totalnumber]\n",
    "ytrain = y[:totalnumber - testnumber]\n",
    "ytest = y[totalnumber - testnumber:totalnumber]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_Xcattrain = new_Xcat[:new_totalnumber - new_testnumber]\n",
    "new_Xcattest = new_Xcat[new_totalnumber - new_testnumber:new_totalnumber]\n",
    "new_Xnumtrain = new_Xnum[:new_totalnumber - new_testnumber]\n",
    "new_Xnumtest = new_Xnum[new_totalnumber - new_testnumber:new_totalnumber]\n",
    "new_ytrain = new_y[:new_totalnumber - new_testnumber]\n",
    "new_ytest = new_y[new_totalnumber - new_testnumber:new_totalnumber]\n",
    "\n",
    "print(Xnumtrain)\n",
    "print(new_Xnumtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols + num_numerical_cols\n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = torch.cat([x, x_numerical], 1)\n",
    "        x = self.layers(x)\n",
    "        return nn.functional.softmax(x, dim = -1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(4, 2)\n",
      "    (1): Embedding(2, 1)\n",
      "    (2): Embedding(9, 5)\n",
      "    (3): Embedding(2, 1)\n",
      "    (4): Embedding(1, 1)\n",
      "    (5): Embedding(2, 1)\n",
      "    (6): Embedding(5, 3)\n",
      "    (7): Embedding(6, 3)\n",
      "    (8): Embedding(7, 4)\n",
      "    (9): Embedding(4, 2)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.6, inplace=False)\n",
      "  (batch_norm_num): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=10, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "    (4): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.6, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.6, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.6, inplace=False)\n",
      "    (16): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define and show the model\n",
    "#model = Model(categorical_embedding_sizes, 1, 2, [8,16,32,64,128], p=0.6)\n",
    "model = Model(categorical_embedding_sizes, 1, 2, [10,20,20,10], p=0.6)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimization\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001 , weight_decay = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 0.72420615\n",
      "epoch:  26 loss: 0.70609385\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-bd5697ab9542>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0msingle_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "aggregated_losses = []\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(Xcattrain, Xnumtrain)\n",
    "    single_loss = loss_function(y_pred, ytrain)\n",
    "    aggregated_losses.append(single_loss)\n",
    "    \n",
    "    if i % 25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "    \n",
    "\n",
    "\n",
    "# Plot the loss over epocs\n",
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val = model(Xcattest, Xnumtest)\n",
    "    loss = loss_function(y_val, ytest)\n",
    "\n",
    "\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "\n",
    "print(confusion_matrix(ytest,y_val))\n",
    "print(classification_report(ytest,y_val))\n",
    "print(accuracy_score(ytest, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(4, 2)\n",
      "    (1): Embedding(2, 1)\n",
      "    (2): Embedding(9, 5)\n",
      "    (3): Embedding(2, 1)\n",
      "    (4): Embedding(1, 1)\n",
      "    (5): Embedding(2, 1)\n",
      "    (6): Embedding(5, 3)\n",
      "    (7): Embedding(6, 3)\n",
      "    (8): Embedding(7, 4)\n",
      "    (9): Embedding(4, 2)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.6, inplace=False)\n",
      "  (batch_norm_num): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=10, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "    (4): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.6, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.6, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.6, inplace=False)\n",
      "    (16): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define and show the model\n",
    "#model = Model(categorical_embedding_sizes, 1, 2, [8,16,32,64,128], p=0.6)\n",
    "model = Model(categorical_embedding_sizes, 1, 2, [10,20,20,10], p=0.6)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimization\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001 , weight_decay = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 0.68760920\n",
      "epoch:  26 loss: 0.67388910\n",
      "epoch:  51 loss: 0.65201986\n",
      "epoch:  76 loss: 0.63039321\n",
      "epoch: 101 loss: 0.61749661\n",
      "epoch: 126 loss: 0.60663652\n",
      "epoch: 151 loss: 0.59282953\n",
      "epoch: 176 loss: 0.59244663\n",
      "epoch: 201 loss: 0.58666486\n",
      "epoch: 226 loss: 0.58092749\n",
      "epoch: 251 loss: 0.57689965\n",
      "epoch: 276 loss: 0.57460457\n",
      "epoch: 300 loss: 0.5678683519\n",
      "[[1704  377]\n",
      " [ 223  823]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85      2081\n",
      "           1       0.69      0.79      0.73      1046\n",
      "\n",
      "    accuracy                           0.81      3127\n",
      "   macro avg       0.79      0.80      0.79      3127\n",
      "weighted avg       0.82      0.81      0.81      3127\n",
      "\n",
      "0.8081228014070995\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 300\n",
    "aggregated_losses = []\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(Xcattrain, Xnumtrain)\n",
    "    single_loss = loss_function(y_pred, ytrain)\n",
    "    aggregated_losses.append(single_loss)\n",
    "    \n",
    "    if i % 25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "    \n",
    "\n",
    "\n",
    "# Plot the loss over epocs\n",
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val = model(Xcattest, Xnumtest)\n",
    "    loss = loss_function(y_val, ytest)\n",
    "\n",
    "\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "\n",
    "print(confusion_matrix(ytest,y_val))\n",
    "print(classification_report(ytest,y_val))\n",
    "print(accuracy_score(ytest, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times low scoretext is predicted:  1954\n",
      "Times medium scoretext is predicted:  534\n",
      "Times high scoretext is predicted:  639\n",
      "Accuracy of the random forest model:  0.7547169811320755\n"
     ]
    }
   ],
   "source": [
    "# Define the model and fit it to the data\n",
    "forestModel = RandomForestClassifier(n_estimators = 53, max_depth = 14, max_features = \"log2\", criterion = \"entropy\")\n",
    "forestModel.fit(Xcattrain, ytrain)\n",
    "\n",
    "# Predict on the test set\n",
    "forestPreds = forestModel.predict(Xcattest)\n",
    "forestProbs = forestModel.predict_proba(Xcattest)[:, 1]\n",
    "\n",
    "print(\"Times low scoretext is predicted: \", len(forestPreds[forestPreds == 0]))\n",
    "print(\"Times medium scoretext is predicted: \", len(forestPreds[forestPreds == 1]))\n",
    "print(\"Times high scoretext is predicted: \", len(forestPreds[forestPreds == 2]))\n",
    "\n",
    "print(\"Accuracy of the random forest model: \", len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baysian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[290.   2.   0.   1.]]\n",
      "[[170.  73.   0.   1.]]\n",
      "[[160.  40.   1.   1.]]\n",
      "[[27. 62.  0.  0.]]\n",
      "[[55. 59.  1.  0.]]\n",
      "[[290.   2.   0.   1.]]\n",
      "[[289.   2.   0.   1.]]\n",
      "[[300.  54.   1.   0.]]\n",
      "[[249.   1.   0.   0.]]\n",
      "[[265.   1.   1.   0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[259.   1.   0.   1.]]\n",
      "[[232.   1.   1.   1.]]\n",
      "[[1. 1. 1. 1.]]\n",
      "[[32.  1.  1.  1.]]\n",
      "[[17.  1.  1.  0.]]\n",
      "[[65.  1.  0.  0.]]\n",
      "[[51.  1.  1.  0.]]\n",
      "[[289.   1.   1.   1.]]\n",
      "[[275.   1.   0.   1.]]\n",
      "[[215.   1.   0.   0.]]\n",
      "[[225.   1.   0.   0.]]\n",
      "[[92.  1.  1.  0.]]\n",
      "[[115.   1.   0.   0.]]\n",
      "[[103.   1.   0.   0.]]\n",
      "[[177.   1.   1.   0.]]\n",
      "[[193.   1.   1.   0.]]\n",
      "[[146.   1.   1.   0.]]\n",
      "[[109.   1.   1.   1.]]\n",
      "[[11.  1.  0.  1.]]\n",
      "[[1. 1. 0. 0.]]\n",
      "[[159.   1.   0.   1.]]\n",
      "[[133.   1.   0.   1.]]\n",
      "[[82.  1.  0.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[68.  1.  1.  1.]]\n",
      "[[184.   1.   0.   1.]]\n",
      "[[150.   1.   0.   0.]]\n",
      "[[204.   1.   1.   1.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[76.  1.  1.  0.]]\n",
      "[[263.   8.   0.   1.]]\n",
      "[[108. 109.   0.   0.]]\n",
      "[[229. 109.   0.   0.]]\n",
      "[[  1. 109.   1.   1.]]\n",
      "[[300. 109.   0.   1.]]\n",
      "[[ 1. 23.  1.  0.]]\n",
      "[[101.  25.   0.   0.]]\n",
      "[[ 53. 109.   0.   1.]]\n",
      "[[300.  13.   0.   1.]]\n",
      "[[201.  21.   1.   0.]]\n",
      "[[164. 109.   1.   1.]]\n",
      "[[51. 16.  0.  1.]]\n",
      "[[262.  82.   1.   0.]]\n",
      "[[139.  13.   0.   0.]]\n",
      "[[103.  71.   1.   1.]]\n",
      "[[125.   1.   1.   0.]]\n",
      "[[213.  72.   1.   1.]]\n",
      "[[167.   1.   1.   0.]]\n",
      "[[42.  1.  0.  1.]]\n",
      "[[39.  1.  1.  0.]]\n",
      "[[25.  1.  0.  0.]]\n",
      "[[255.   1.   1.   0.]]\n",
      "[[ 1. 75.  1.  1.]]\n",
      "[[214.   1.   1.   1.]]\n",
      "[[121.   1.   0.   1.]]\n",
      "[[240.   1.   0.   1.]]\n",
      "[[172.   1.   0.   1.]]\n",
      "[[59.  1.  1.  1.]]\n",
      "[[7. 1. 1. 0.]]\n",
      "[[144.   1.   0.   1.]]\n",
      "[[196. 109.   1.   0.]]\n",
      "[[152.   1.   1.   1.]]\n",
      "[[188.   1.   1.   1.]]\n",
      "[[54.  1.  0.  1.]]\n",
      "[[98.  1.  1.  1.]]\n",
      "[[237.   1.   1.   0.]]\n",
      "[[34.  1.  0.  0.]]\n",
      "[[275.   1.   1.   0.]]\n",
      "[[300.  79.   1.   1.]]\n",
      "[[209.   1.   1.   0.]]\n",
      "[[270.   1.   0.   0.]]\n",
      "[[247.   1.   1.   1.]]\n",
      "[[93.  1.  0.  1.]]\n",
      "[[196.   1.   0.   1.]]\n",
      "[[20.  1.  0.  1.]]\n",
      "[[265. 109.   1.   0.]]\n",
      "[[138.   1.   0.   0.]]\n",
      "[[165.   1.   1.   1.]]\n",
      "[[73.  1.  0.  1.]]\n",
      "[[137.   1.   1.   1.]]\n",
      "[[136.  82.   1.   1.]]\n",
      "[[47.  1.  1.  1.]]\n",
      "[[202.   1.   0.   0.]]\n",
      "[[270.   1.   1.   1.]]\n",
      "[[87.  1.  1.  1.]]\n",
      "[[243.   1.   1.   0.]]\n",
      "[[208.   1.   0.   1.]]\n",
      "[[118.   1.   1.   0.]]\n",
      "[[86.  1.  0.  0.]]\n",
      "[[188.   1.   0.   0.]]\n",
      "[[232.   1.   0.   0.]]\n",
      "[[107.   1.   1.   0.]]\n",
      "[[4. 1. 0. 1.]]\n",
      "[[132.   1.   1.   0.]]\n",
      "[[75. 87.  0.  0.]]\n",
      "[[15.  1.  0.  0.]]\n",
      "[[70.  1.  1.  0.]]\n",
      "[[158.   1.   1.   0.]]\n",
      "[[29. 91.  1.  1.]]\n",
      "[[172.   1.   1.   1.]]\n",
      "[[37.  1.  1.  1.]]\n",
      "[[221.   1.   0.   1.]]\n",
      "[[58.  1.  0.  0.]]\n",
      "[[63.  1.  1.  0.]]\n",
      "[[180.   1.   1.   1.]]\n",
      "[[253.   1.   0.   1.]]\n",
      "[[280.   1.   0.   0.]]\n",
      "[[47.  1.  0.  0.]]\n",
      "[[184.   1.   1.   0.]]\n",
      "[[97.  1.  0.  0.]]\n",
      "[[226.   1.   1.   1.]]\n",
      "[[25.  1.  1.  1.]]\n",
      "[[128.   1.   0.   1.]]\n",
      "[[82.  1.  1.  0.]]\n",
      "[[163.   1.   0.   0.]]\n",
      "[[115.   1.   1.   1.]]\n",
      "[[280.   1.   1.   1.]]\n",
      "[[29.  1.  0.  1.]]\n",
      "[[199.   1.   1.   0.]]\n",
      "[[265.   1.   0.   1.]]\n",
      "[[229.   1.   1.   0.]]\n",
      "[[14.  1.  1.  1.]]\n",
      "[[126.   1.   0.   0.]]\n",
      "[[142.   1.   0.   0.]]\n",
      "[[62.  1.  0.  1.]]\n",
      "[[245.   1.   0.   1.]]\n",
      "[[78.  1.  1.  1.]]\n",
      "[[261.   1.   1.   1.]]\n",
      "[[9. 1. 0. 0.]]\n",
      "[[261.   1.   0.   0.]]\n",
      "[[206.   1.   0.   0.]]\n",
      "[[170.   1.   0.   0.]]\n",
      "[[199.   1.   1.   1.]]\n",
      "[[190.  53.   0.   0.]]\n",
      "[[177.   1.   0.   1.]]\n",
      "[[285.   1.   1.   0.]]\n",
      "[[180.  11.   1.   1.]]\n",
      "[[136.  54.   0.   0.]]\n",
      "[[104.   1.   1.   1.]]\n",
      "[[78.  1.  0.  0.]]\n",
      "[[44.  1.  1.  0.]]\n",
      "[[112.   1.   1.   0.]]\n",
      "[[235.   1.   0.   1.]]\n",
      "[[284.   1.   0.   1.]]\n",
      "[[254.   1.   1.   1.]]\n",
      "[[55.  1.  1.  0.]]\n",
      "[[229.   1.   0.   1.]]\n",
      "[[162.   1.   1.   0.]]\n",
      "[[240.   1.   0.   0.]]\n",
      "[[146.   1.   0.   0.]]\n",
      "[[138. 107.   1.   0.]]\n",
      "[[180.   1.   0.   0.]]\n",
      "[[107.   1.   0.   1.]]\n",
      "[[89.  1.  0.  1.]]\n",
      "[[154.   1.   0.   1.]]\n",
      "[[174.   1.   0.   0.]]\n",
      "[[116.   1.   0.   1.]]\n",
      "[[125.   1.   0.   1.]]\n",
      "[[29.  1.  1.  0.]]\n",
      "[[284.  68.   1.   0.]]\n",
      "[[239.   1.   1.   1.]]\n",
      "[[148.   1.   1.   1.]]\n",
      "[[143.   1.   1.   1.]]\n",
      "[[288.   1.   0.   0.]]\n",
      "[[127.   1.   1.   1.]]\n",
      "[[37.  1.  0.  1.]]\n",
      "[[218.   1.   1.   1.]]\n",
      "[[2. 1. 1. 0.]]\n",
      "[[110.   1.   0.   0.]]\n",
      "[[22.  1.  1.  0.]]\n",
      "[[68.  1.  0.  0.]]\n",
      "[[4. 1. 1. 1.]]\n",
      "[[15.  1.  0.  1.]]\n",
      "[[84. 50.  0.  1.]]\n",
      "[[242.   1.   1.   1.]]\n",
      "[[290.   2.   1.   0.]]\n",
      "[[168.   1.   0.   1.]]\n",
      "[[11.  1.  1.  0.]]\n",
      "[[130.   1.   0.   0.]]\n",
      "[[ 82. 109.   1.   0.]]\n",
      "[[211.   1.   0.   0.]]\n",
      "[[51.  1.  0.  1.]]\n",
      "[[73.  1.  1.  1.]]\n",
      "[[21.  1.  1.  1.]]\n",
      "[[8. 1. 0. 1.]]\n",
      "[[191.   1.   0.   1.]]\n",
      "[[96.  1.  1.  1.]]\n",
      "[[101.   1.   1.   0.]]\n",
      "[[270.   1.   1.   0.]]\n",
      "[[266.   1.   1.   1.]]\n",
      "[[257.   1.   1.   1.]]\n",
      "[[233.  87.   1.   0.]]\n",
      "[[203.   1.   1.   0.]]\n",
      "[[223.   1.   1.   1.]]\n",
      "[[250.   1.   0.   1.]]\n",
      "[[42.  1.  0.  0.]]\n",
      "[[156.   1.   1.   1.]]\n",
      "[[214.   1.   1.   0.]]\n",
      "[[89.  1.  1.  0.]]\n",
      "[[175.   1.   1.   1.]]\n",
      "[[139.   1.   1.   0.]]\n",
      "[[153.   1.   1.   0.]]\n",
      "[[140.   1.   0.   1.]]\n",
      "[[73.  1.  0.  0.]]\n",
      "[[197.   1.   0.   0.]]\n",
      "[[196.   1.   1.   1.]]\n",
      "[[171.   1.   1.   0.]]\n",
      "[[ 24. 109.   1.   0.]]\n",
      "[[258.   1.   1.   0.]]\n",
      "[[42.  1.  1.  1.]]\n",
      "[[196.  87.   1.   0.]]\n",
      "[[45.  1.  0.  1.]]\n",
      "[[119.   1.   1.   1.]]\n",
      "[[32.  1.  0.  0.]]\n",
      "[[250.   1.   1.   0.]]\n",
      "[[20.  3.  0.  1.]]\n",
      "[[ 1. 48.  1.  0.]]\n",
      "[[78. 14.  1.  0.]]\n",
      "[[90.  1.  1.  1.]]\n",
      "[[161.   1.   1.   1.]]\n",
      "[[157.   1.   0.   0.]]\n",
      "[[280.  92.   0.   0.]]\n",
      "[[101.   1.   0.   1.]]\n",
      "[[217.   1.   0.   1.]]\n",
      "[[92.  1.  0.  0.]]\n",
      "[[ 6. 90.  0.  0.]]\n",
      "[[225.   1.   1.   0.]]\n",
      "[[54.  1.  1.  1.]]\n",
      "[[25.  1.  0.  1.]]\n",
      "[[186.   1.   0.   1.]]\n",
      "[[83.  1.  1.  1.]]\n",
      "[[277.   1.   1.   1.]]\n",
      "[[53.  1.  0.  0.]]\n",
      "[[133.   1.   1.   1.]]\n",
      "[[118.   1.   0.   0.]]\n",
      "[[273.   1.   0.   0.]]\n",
      "[[279.   1.   1.   0.]]\n",
      "[[58.  1.  0.  1.]]\n",
      "[[66.  1.  0.  1.]]\n",
      "[[254.   1.   0.   0.]]\n",
      "[[184.   1.   1.   1.]]\n",
      "[[219.  49.   0.   0.]]\n",
      "[[6. 1. 0. 0.]]\n",
      "[[286.   1.   1.   1.]]\n",
      "[[112.   1.   0.   1.]]\n",
      "[[64.  1.  1.  1.]]\n",
      "[[21.  1.  0.  0.]]\n",
      "[[33.  1.  1.  0.]]\n",
      "[[204.   1.   0.   1.]]\n",
      "[[235.   1.   1.   1.]]\n",
      "[[283.   1.   1.   0.]]\n",
      "[[110.  45.   1.   1.]]\n",
      "[[236.   1.   0.   0.]]\n",
      "[[154.  96.   0.   0.]]\n",
      "[[288.   4.   1.   1.]]\n",
      "[[300.  73.   0.   0.]]\n",
      "[[229.  16.   0.   0.]]\n",
      "[[246.  58.   0.   1.]]\n",
      "[[113.   9.   1.   1.]]\n",
      "[[300.   4.   0.   0.]]\n",
      "[[51. 82.  0.  0.]]\n",
      "[[39. 42.  0.  1.]]\n",
      "[[154.  62.   1.   0.]]\n",
      "[[115.  89.   0.   0.]]\n",
      "[[211.   6.   0.   1.]]\n",
      "[[241.  35.   0.   0.]]\n",
      "[[198.  37.   1.   0.]]\n",
      "[[63. 39.  0.  0.]]\n",
      "[[266.  56.   0.   0.]]\n",
      "[[177.  94.   1.   0.]]\n",
      "[[289.   1.   1.   0.]]\n",
      "[[158.   8.   1.   0.]]\n",
      "[[247. 109.   1.   1.]]\n",
      "[[76. 68.  1.  0.]]\n",
      "[[94. 91.  1.  1.]]\n",
      "[[243.   7.   1.   0.]]\n",
      "[[214.  96.   1.   1.]]\n",
      "[[122.  66.   0.   1.]]\n",
      "[[222.  33.   1.   1.]]\n",
      "[[173.  52.   1.   1.]]\n",
      "[[131.  36.   0.   1.]]\n",
      "[[19. 41.  0.  1.]]\n",
      "[[190.  70.   1.   0.]]\n",
      "[[300.  36.   0.   1.]]\n",
      "[[280. 109.   1.   1.]]\n",
      "[[235.  68.   0.   0.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[62.  5.  0.  0.]]\n",
      "[[92.  7.  1.  1.]]\n",
      "[[84. 33.  1.  0.]]\n",
      "[[39. 25.  1.  0.]]\n",
      "[[18. 76.  0.  1.]]\n",
      "[[194.   1.   0.   0.]]\n",
      "[[123. 109.   1.   1.]]\n",
      "[[181.  34.   0.   1.]]\n",
      "[[122.   1.   1.   0.]]\n",
      "[[76.  1.  0.  1.]]\n",
      "[[290.   2.   0.   0.]]\n",
      "[[96.  1.  1.  0.]]\n",
      "[[39. 73.  1.  1.]]\n",
      "[[70.  1.  0.  1.]]\n",
      "[[282.  49.   0.   1.]]\n",
      "[[1. 6. 1. 0.]]\n",
      "[[278.   1.   0.   1.]]\n",
      "[[1. 4. 0. 0.]]\n",
      "[[ 1. 12.  1.  0.]]\n",
      "[[262.  38.   1.   1.]]\n",
      "[[291.   2.   1.   0.]]\n",
      "[[ 1. 34.  1.  1.]]\n",
      "[[40.  7.  1.  1.]]\n",
      "[[154.  79.   1.   1.]]\n",
      "[[249.  92.   0.   0.]]\n",
      "[[180. 109.   1.   0.]]\n",
      "[[ 68. 109.   0.   1.]]\n",
      "[[288.   1.   0.   1.]]\n",
      "[[ 38. 109.   0.   0.]]\n",
      "[[ 9. 61.  1.  1.]]\n",
      "[[212. 109.   1.   1.]]\n",
      "[[300.  94.   1.   0.]]\n",
      "[[52. 30.  0.  0.]]\n",
      "[[205.  57.   1.   1.]]\n",
      "[[211.   1.   1.   1.]]\n",
      "[[98. 56.  0.  0.]]\n",
      "[[250.  73.   1.   1.]]\n",
      "[[117.  30.   1.   1.]]\n",
      "[[277.   1.   0.   0.]]\n",
      "[[129.   5.   1.   1.]]\n",
      "[[146.  31.   0.   0.]]\n",
      "[[193.   5.   0.   0.]]\n",
      "[[6. 1. 1. 1.]]\n",
      "[[267.   1.   0.   0.]]\n",
      "[[135.   1.   1.   0.]]\n",
      "[[234.  49.   1.   1.]]\n",
      "[[60. 94.  1.  1.]]\n",
      "[[37.  1.  0.  0.]]\n",
      "[[213.   1.   0.   1.]]\n",
      "[[88. 76.  0.  1.]]\n",
      "[[163.   1.   0.   1.]]\n",
      "[[63. 74.  0.  1.]]\n",
      "[[97. 39.  1.  0.]]\n",
      "[[284.   1.   1.   1.]]\n",
      "[[224.   1.   0.   1.]]\n",
      "[[169.   1.   1.   1.]]\n",
      "[[69. 53.  1.  0.]]\n",
      "[[222.   1.   0.   0.]]\n",
      "[[1. 7. 1. 1.]]\n",
      "[[19. 27.  1.  0.]]\n",
      "[[279.  34.   0.   0.]]\n",
      "[[270.  71.   0.   1.]]\n",
      "[[139.  68.   0.   0.]]\n",
      "[[ 95. 109.   1.   0.]]\n",
      "[[122.  51.   1.   0.]]\n",
      "[[151. 109.   1.   0.]]\n",
      "[[168.  26.   0.   0.]]\n",
      "[[130.  95.   0.   0.]]\n",
      "[[70. 27.  0.  1.]]\n",
      "[[44. 94.  1.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[1. 1. 1. 0.]]\n",
      "[[81.  1.  1.  0.]]\n",
      "[[52.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[70.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[147.  46.   1.   1.]]\n",
      "[[41. 57.  0.  1.]]\n",
      "[[149.   1.   1.   0.]]\n",
      "[[256.   1.   0.   1.]]\n",
      "[[209.  31.   0.   1.]]\n",
      "[[207.   1.   1.   1.]]\n",
      "[[187.   1.   1.   0.]]\n",
      "[[160.   1.   0.   0.]]\n",
      "[[123.   1.   0.   0.]]\n",
      "[[251.  46.   0.   0.]]\n",
      "[[136.   1.   0.   1.]]\n",
      "[[265.  95.   1.   1.]]\n",
      "[[18.  1.  0.  0.]]\n",
      "[[220.  83.   0.   0.]]\n",
      "[[28.  1.  1.  1.]]\n",
      "[[234.   1.   1.   0.]]\n",
      "[[223.  62.   1.   1.]]\n",
      "[[182.  81.   0.   1.]]\n",
      "[[130.   1.   1.   1.]]\n",
      "[[ 14. 101.   0.   1.]]\n",
      "[[185.   1.   0.   0.]]\n",
      "[[48.  1.  0.  1.]]\n",
      "[[165.  87.   1.   0.]]\n",
      "[[149.   1.   0.   1.]]\n",
      "[[285.  81.   0.   1.]]\n",
      "[[181.   1.   1.   0.]]\n",
      "[[190.   1.   1.   0.]]\n",
      "[[100.   1.   0.   0.]]\n",
      "[[121.  78.   1.   1.]]\n",
      "[[273.   1.   1.   1.]]\n",
      "[[85.  1.  0.  1.]]\n",
      "[[193.   1.   1.   1.]]\n",
      "[[174.   1.   1.   0.]]\n",
      "[[178.   1.   1.   1.]]\n",
      "[[201.   1.   1.   1.]]\n",
      "[[39.  1.  0.  1.]]\n",
      "[[246.   1.   0.   0.]]\n",
      "[[101.   1.   1.   1.]]\n",
      "[[10.  1.  1.  1.]]\n",
      "[[228.   1.   0.   0.]]\n",
      "[[264.   1.   0.   0.]]\n",
      "[[257.   1.   0.   0.]]\n",
      "[[23.  1.  0.  1.]]\n",
      "[[131.   1.   0.   1.]]\n",
      "[[61.  1.  1.  1.]]\n",
      "[[123.   1.   1.   1.]]\n",
      "[[66.  1.  1.  0.]]\n",
      "[[112.   1.   1.   1.]]\n",
      "[[202.  76.   0.   1.]]\n",
      "[[243.   1.   0.   0.]]\n",
      "[[218.   1.   0.   0.]]\n",
      "[[285.   1.   0.   0.]]\n",
      "[[134.   1.   0.   0.]]\n",
      "[[18.  1.  1.  1.]]\n",
      "[[181.   1.   0.   1.]]\n",
      "[[182.   1.   0.   0.]]\n",
      "[[33.  1.  0.  1.]]\n",
      "[[262.   1.   1.   0.]]\n",
      "[[216.   1.   1.   0.]]\n",
      "[[196.   1.   1.   0.]]\n",
      "[[52. 46.  1.  1.]]\n",
      "[[85.  1.  1.  0.]]\n",
      "[[179.  63.   0.   0.]]\n",
      "[[48.  1.  1.  0.]]\n",
      "[[200.   1.   0.   1.]]\n",
      "[[128.   1.   1.   0.]]\n",
      "[[111.  60.   1.   1.]]\n",
      "[[ 1. 63.  0.  0.]]\n",
      "[[96.  1.  0.  1.]]\n",
      "[[3. 1. 0. 0.]]\n",
      "[[252.   1.   1.   0.]]\n",
      "[[61.  1.  0.  0.]]\n",
      "[[237.  99.   0.   1.]]\n",
      "[[262.   1.   0.   1.]]\n",
      "[[165.   1.   1.   0.]]\n",
      "[[231.   1.   1.   0.]]\n",
      "[[83.  1.  0.  0.]]\n",
      "[[264.   1.   1.   1.]]\n",
      "[[246.   1.   1.   0.]]\n",
      "[[19.  1.  1.  0.]]\n",
      "[[28.  1.  0.  0.]]\n",
      "[[251.   1.   1.   1.]]\n",
      "[[28. 49.  1.  1.]]\n",
      "[[104.   1.   0.   1.]]\n",
      "[[106.   1.   0.   0.]]\n",
      "[[29. 34.  1.  0.]]\n",
      "[[44.  1.  0.  0.]]\n",
      "[[272.  44.   0.   0.]]\n",
      "[[81. 97.  1.  1.]]\n",
      "[[251.   1.   0.   0.]]\n",
      "[[190.  98.   0.   1.]]\n",
      "[[26.  1.  1.  0.]]\n",
      "[[143.   1.   1.   0.]]\n",
      "[[190.  26.   1.   0.]]\n",
      "[[153.   1.   0.   0.]]\n",
      "[[166.   1.   0.   0.]]\n",
      "[[94.  1.  0.  0.]]\n",
      "[[238.   1.   0.   1.]]\n",
      "[[59.  1.  1.  0.]]\n",
      "[[206.   1.   1.   0.]]\n",
      "[[247.   1.   0.   1.]]\n",
      "[[75.  1.  1.  1.]]\n",
      "[[88. 63.  0.  1.]]\n",
      "[[191.   1.   0.   0.]]\n",
      "[[269.   1.   0.   1.]]\n",
      "[[140.   1.   1.   1.]]\n",
      "[[44.  1.  1.  1.]]\n",
      "[[105.  98.   1.   1.]]\n",
      "[[10. 33.  1.  0.]]\n",
      "[[212.   1.   1.   0.]]\n",
      "[[79.  1.  0.  1.]]\n",
      "[[108.   1.   0.   0.]]\n",
      "[[151.   1.   0.   1.]]\n",
      "[[36.  1.  1.  0.]]\n",
      "[[254.  27.   0.   0.]]\n",
      "[[ 1. 98.  1.  0.]]\n",
      "[[3. 6. 1. 0.]]\n",
      "[[227.   4.   1.   1.]]\n",
      "[[234.  25.   0.   1.]]\n",
      "[[104.  83.   0.   0.]]\n",
      "[[116.   1.   1.   0.]]\n",
      "[[143.  93.   1.   1.]]\n",
      "[[230.   1.   0.   0.]]\n",
      "[[272.   1.   0.   1.]]\n",
      "[[258.  64.   1.   0.]]\n",
      "[[290.  41.   0.   0.]]\n",
      "[[158.   1.   1.   1.]]\n",
      "[[50.  1.  1.  1.]]\n",
      "[[300.  25.   1.   0.]]\n",
      "[[161.  53.   0.   0.]]\n",
      "[[241.   1.   1.   0.]]\n",
      "[[171.   4.   0.   0.]]\n",
      "[[146.   4.   0.   1.]]\n",
      "[[134.  24.   1.   0.]]\n",
      "[[290. 101.   0.   0.]]\n",
      "[[268.   1.   1.   0.]]\n",
      "[[203.  98.   0.   0.]]\n",
      "[[75. 40.  1.  1.]]\n",
      "[[232.   1.   0.   1.]]\n",
      "[[155.   1.   1.   0.]]\n",
      "[[145.   1.   1.   1.]]\n",
      "[[13.  1.  0.  0.]]\n",
      "[[14.  1.  1.  0.]]\n",
      "[[30. 80.  0.  0.]]\n",
      "[[110.   1.   0.   1.]]\n",
      "[[219.  23.   1.   0.]]\n",
      "[[16.  1.  1.  1.]]\n",
      "[[174.   1.   0.   1.]]\n",
      "[[155.  24.   1.   1.]]\n",
      "[[147.   1.   0.   1.]]\n",
      "[[177.   1.   0.   0.]]\n",
      "[[207.  45.   0.   0.]]\n",
      "[[16. 52.  0.  0.]]\n",
      "[[151.   1.   1.   0.]]\n",
      "[[204.   1.   0.   0.]]\n",
      "[[225.  97.   0.   0.]]\n",
      "[[ 11. 109.   1.   0.]]\n",
      "[[222.   1.   1.   0.]]\n",
      "[[99.  1.  0.  1.]]\n",
      "[[168.   1.   0.   0.]]\n",
      "[[73.  1.  1.  0.]]\n",
      "[[193.   1.   0.   1.]]\n",
      "[[191.   1.   1.   1.]]\n",
      "[[57.  1.  1.  1.]]\n",
      "[[35.  1.  1.  1.]]\n",
      "[[206.   1.   0.   1.]]\n",
      "[[186.   1.   1.   1.]]\n",
      "[[155.   1.   0.   0.]]\n",
      "[[291.  60.   0.   1.]]\n",
      "[[104.   1.   1.   0.]]\n",
      "[[189.   1.   0.   1.]]\n",
      "[[18. 88.  1.  0.]]\n",
      "[[89.  1.  0.  0.]]\n",
      "[[160.   1.   1.   0.]]\n",
      "[[171.  40.   0.   0.]]\n",
      "[[220.   1.   1.   1.]]\n",
      "[[29. 18.  1.  0.]]\n",
      "[[239.   1.   1.   0.]]\n",
      "[[17.  1.  0.  1.]]\n",
      "[[51. 70.  0.  1.]]\n",
      "[[107.  34.   0.   1.]]\n",
      "[[230.   1.   1.   1.]]\n",
      "[[165.   1.   0.   1.]]\n",
      "[[41.  1.  1.  0.]]\n",
      "[[71.  1.  0.  0.]]\n",
      "[[245.   1.   1.   1.]]\n",
      "[[120.   1.   0.   0.]]\n",
      "[[56.  1.  0.  0.]]\n",
      "[[274.  82.   1.   0.]]\n",
      "[[87.  1.  0.  1.]]\n",
      "[[267.  28.   0.   0.]]\n",
      "[[93.  1.  1.  1.]]\n",
      "[[113.   1.   0.   0.]]\n",
      "[[142.   1.   0.   1.]]\n",
      "[[227.  75.   0.   1.]]\n",
      "[[255. 102.   1.   1.]]\n",
      "[[210.   1.   0.   1.]]\n",
      "[[86. 23.  0.  1.]]\n",
      "[[114.   1.   1.   0.]]\n",
      "[[161.   1.   0.   1.]]\n",
      "[[11.  1.  0.  0.]]\n",
      "[[242.  80.   0.   0.]]\n",
      "[[80.  1.  1.  1.]]\n",
      "[[1. 1. 0. 1.]]\n",
      "[[282.   1.   0.   0.]]\n",
      "[[167.  99.   0.   1.]]\n",
      "[[81.  1.  0.  0.]]\n",
      "[[166.  63.   1.   1.]]\n",
      "[[2. 1. 0. 1.]]\n",
      "[[209.   1.   1.   1.]]\n",
      "[[118. 100.   1.   0.]]\n",
      "[[28.  4.  1.  0.]]\n",
      "[[ 9. 71.  1.  0.]]\n",
      "[[201.   1.   1.   0.]]\n",
      "[[120.  39.   0.   0.]]\n",
      "[[ 91. 100.   0.   0.]]\n",
      "[[30.  1.  0.  0.]]\n",
      "[[66. 63.  0.  1.]]\n",
      "[[144.   1.   0.   0.]]\n",
      "[[70. 99.  0.  0.]]\n",
      "[[260.   1.   1.   0.]]\n",
      "[[292.  88.   1.   0.]]\n",
      "[[121.   1.   1.   1.]]\n",
      "[[79.  1.  1.  0.]]\n",
      "[[40.  1.  0.  0.]]\n",
      "[[157.   1.   0.   1.]]\n",
      "[[202.  65.   0.   0.]]\n",
      "[[ 31. 100.   0.   0.]]\n",
      "[[ 1. 32.  1.  0.]]\n",
      "[[130.   1.   1.   0.]]\n",
      "[[135.   1.   1.   1.]]\n",
      "[[242.   1.   0.   1.]]\n",
      "[[179.   1.   0.   1.]]\n",
      "[[167.   1.   1.   1.]]\n",
      "[[60.  1.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "## define the domain of the considered parameters\n",
    "n_estimators = tuple(np.arange(1,301,1, dtype= np.int))\n",
    "# print(n_estimators)\n",
    "max_depth = tuple(np.arange(1,110,1, dtype= np.int))\n",
    "# max_features = ('log2', 'sqrt', None)\n",
    "max_features = (0, 1)\n",
    "# criterion = ('gini', 'entropy')\n",
    "criterion = (0, 1)\n",
    "\n",
    "\n",
    "# define the dictionary for GPyOpt\n",
    "domain = [{'n_estimators': 'var_1',  'type': 'discrete',     'domain': n_estimators},\n",
    "          {'max_depth': 'var_2',     'type': 'discrete',     'domain': max_depth},\n",
    "          {'max_features': 'var_3',  'type': 'categorical',  'domain': max_features},\n",
    "          {'criterion': 'var_4',     'type': 'categorical',  'domain': criterion}]\n",
    "\n",
    "\n",
    "## we have to define the function we want to maximize --> validation accuracy, \n",
    "## note it should take a 2D ndarray but it is ok that it assumes only one point\n",
    "## in this setting\n",
    "def objective_function(x): \n",
    "    print(x)\n",
    "    # we have to handle the categorical variables that is convert 0/1 to labels\n",
    "    # log2/sqrt and gini/entropy\n",
    "    \n",
    "    param = x[0]\n",
    "    \n",
    "    if param[2] == 0:\n",
    "        var_3 = \"log2\"\n",
    "    else:\n",
    "        var_3 = \"sqrt\"\n",
    "    \n",
    "    if param[3] == 0:\n",
    "        var_4 = \"gini\"\n",
    "    else:\n",
    "        var_4 = \"entropy\"\n",
    "        \n",
    "        \n",
    "#fit the model\n",
    "    model = RandomForestClassifier(n_estimators = int(param[0]), criterion = var_4, max_depth = int(param[1]), max_features = var_3)\n",
    "    model.fit(Xcattrain, ytrain)\n",
    "    forestPreds = model.predict(Xcattest)\n",
    "    accuracy = len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "opt = GPyOpt.methods.BayesianOptimization(f = objective_function,   # function to optimize\n",
    "                                              domain = domain,         # box-constrains of the problem\n",
    "                                              acquisition_type = \"EI\",      # Select acquisition function MPI, EI, LCB\n",
    "                                             )\n",
    "opt.acquisition.exploration_weight=.1\n",
    "\n",
    "opt.run_optimization(max_iter = 100) \n",
    "\n",
    "\n",
    "x_best = opt.X[np.argmin(opt.Y)]\n",
    "print(\"The best parameters obtained: n_estimators=\" + str(x_best[0]) + \", max_depth=\" + str(x_best[1]) + \", max_features=\" + str(\n",
    "    x_best[2])  + \", criterion=\" + str(\n",
    "    x_best[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
