{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import GPyOpt\n",
    "s = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "#data = pd.read_csv(\"/home/oskar/Desktop/fagprojekt/compas/compas-scores-raw.csv\")\n",
    "url = \"https://raw.githubusercontent.com/oskarwiese/fagprojekt/master/compas/compas-scores-raw.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/oskarwiese/fagprojekt/master/compas_propublica/compas-scores-two-years.csv\"\n",
    "new_data = pd.read_csv(url)\n",
    "# Til at se p√• dataen \n",
    "#print(data.head)\n",
    "#print(data.columns)\n",
    "\n",
    "# Check if there are any missing values\n",
    "print(np.count_nonzero(data[\"IsDeleted\"] == 1))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'name', 'first', 'last', 'compas_screening_date', 'sex', 'dob',\n",
      "       'age', 'age_cat', 'race', 'juv_fel_count', 'decile_score',\n",
      "       'juv_misd_count', 'juv_other_count', 'priors_count',\n",
      "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
      "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas',\n",
      "       'c_charge_degree', 'c_charge_desc', 'is_recid', 'r_case_number',\n",
      "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
      "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
      "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
      "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
      "       'decile_score.1', 'score_text', 'screening_date',\n",
      "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
      "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
      "       'start', 'end', 'event', 'two_year_recid'],\n",
      "      dtype='object')\n",
      "0        NaN\n",
      "1       (F3)\n",
      "2       (M1)\n",
      "3        NaN\n",
      "4        NaN\n",
      "        ... \n",
      "7209     NaN\n",
      "7210     NaN\n",
      "7211     NaN\n",
      "7212     NaN\n",
      "7213    (M2)\n",
      "Name: r_charge_degree, Length: 7214, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(new_data.columns)\n",
    "print(new_data[\"r_charge_degree\"])\n",
    "def is_plot():\n",
    "    sb.countplot(x = \"score_text\", hue = \"race\", data = new_data)\n",
    "    plt.show()\n",
    "\n",
    "    sb.countplot(x = \"two_year_recid\", hue = \"race\", data = new_data)\n",
    "    plt.show()\n",
    "    sb.countplot(x = \"is_recid\", hue = \"race\", data = new_data)\n",
    "    plt.show()\n",
    "    sb.countplot(x = \"is_violent_recid\", hue = \"race\", data = new_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots():\n",
    "    # Show distribution of different ethnicities and sexes\n",
    "    chart = sb.countplot(x = \"Ethnic_Code_Text\", data = data)\n",
    "    chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    chart.set(xlabel='Ethnicity', ylabel='Count')\n",
    "    plt.show()\n",
    "    \n",
    "    chart = sb.countplot(x = \"Sex_Code_Text\", data = data)\n",
    "    chart.set(xlabel='Sex', ylabel='Count')\n",
    "    plt.show()\n",
    "    \n",
    "    sb.countplot(x = \"Language\", data = data)\n",
    "    plt.show()\n",
    "    \n",
    "    # Showing the distribution of the raw and decile values\n",
    "    plt.xlabel(\"Raw value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Visualization of the values\")\n",
    "    plt.hist(data[\"RawScore\"])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.xlabel(\"Decile value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Visualization of the decile values\")\n",
    "    plt.hist(data[\"DecileScore\"])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #sb.countplot(x = \"RawScore\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Indication that some black people might get higher sentences that white people\n",
    "    sb.countplot(x = \"DecileScore\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    plt.show()\n",
    "    \n",
    "    sb.countplot(x = \"ScoreText\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"Agency_Text\", \"Sex_Code_Text\", \"Ethnic_Code_Text\", \"ScaleSet_ID\", \"AssessmentReason\", \"Language\", \"LegalStatus\", \"CustodyStatus\", \"MaritalStatus\", \"RecSupervisionLevel\"]\n",
    "new_categoricals = [\"c_charge_degree\", \"race\", \"age_cat\", \"sex\", \"is_recid\"] # \"two_year_recid\"\n",
    "# Changing date of birth into age,as this should work better in a neural network\n",
    "new_numericals = [\"age\", \"priors_count\"] # \"days_b_screening_arrest\"\n",
    "\n",
    "if s == 1:\n",
    "    ages = [None] * len(data[\"DateOfBirth\"])\n",
    "    for i in range(len(data[\"DateOfBirth\"])):\n",
    "        ages[i] = 20 +(100 - int(data[\"DateOfBirth\"][i].split(\"/\")[2]))\n",
    "    data[\"DateOfBirth\"] = ages\n",
    "    numericals = [\"DateOfBirth\"]\n",
    "    s+=1\n",
    "else:\n",
    "    pass\n",
    "\n",
    "outputs = [\"ScoreText\"]\n",
    "new_outputs = [\"score_text\"]\n",
    "\n",
    "data = data.dropna(axis = 0, how = 'any')\n",
    "data[outputs] = data[outputs].replace('Low',0)\n",
    "data[outputs] = data[outputs].replace('Medium',1)\n",
    "data[outputs] = data[outputs].replace('High',1)\n",
    "data[outputs] = data[outputs].astype(\"category\")\n",
    "\n",
    "\n",
    "new_data[new_outputs] = new_data[new_outputs].replace('Low',0)\n",
    "new_data[new_outputs] = new_data[new_outputs].replace('Medium',1)\n",
    "new_data[new_outputs] = new_data[new_outputs].replace('High',1)\n",
    "new_data[new_outputs] = new_data[new_outputs].astype(\"category\")\n",
    "\n",
    "\n",
    "for category in categoricals:\n",
    "    data[category] = data[category].astype(\"category\")\n",
    "    \n",
    "for new_category in new_categoricals:\n",
    "    new_data[new_category] = new_data[new_category].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparing data for pytorch\n",
    "Xcat = []\n",
    "for i in range(len(categoricals)):\n",
    "    Xcat.append(data[categoricals[i]].cat.codes.values)\n",
    "Xcat = torch.tensor(Xcat , dtype = torch.int64).T\n",
    "\n",
    "\n",
    "new_Xcat = []\n",
    "for i in range(len(new_categoricals)):\n",
    "    new_Xcat.append(new_data[new_categoricals[i]].cat.codes.values)\n",
    "new_Xcat = torch.tensor(new_Xcat , dtype = torch.int64).T\n",
    "\n",
    "#Converting the numerical values to a tensor\n",
    "Xnum = np.stack([data[col].values for col in numericals], 1)\n",
    "Xnum = torch.tensor(Xnum, dtype=torch.float)\n",
    "\n",
    "\n",
    "new_Xnum = np.stack([new_data[col].values for col in new_numericals], 1)\n",
    "new_Xnum = torch.tensor(new_Xnum, dtype=torch.float)\n",
    "\n",
    "# Converting the output to tensor\n",
    "y = torch.tensor(data[outputs].values).flatten()\n",
    "new_y = torch.tensor(new_data[new_outputs].values).flatten()\n",
    "\n",
    "# Calculation of embedding sizes for the categorical values in the format (unique categorical values, embedding size (dimension of encoding))\n",
    "categorical_column_sizes = [len(data[column].cat.categories) for column in categoricals]\n",
    "categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]\n",
    "\n",
    "\n",
    "new_categorical_column_sizes = [len(new_data[column].cat.categories) for column in new_categoricals]\n",
    "new_categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in new_categorical_column_sizes]\n",
    "\n",
    "# Train-test split\n",
    "totalnumber = len(Xnum)\n",
    "testnumber = int(totalnumber * 0.2)\n",
    "\n",
    "\n",
    "\n",
    "new_totalnumber = len(new_Xnum)\n",
    "new_testnumber = int(new_totalnumber * 0.2)\n",
    "\n",
    "Xcattrain = Xcat[:totalnumber - testnumber]\n",
    "Xcattest = Xcat[totalnumber - testnumber:totalnumber]\n",
    "Xnumtrain = Xnum[:totalnumber - testnumber]\n",
    "Xnumtest = Xnum[totalnumber - testnumber:totalnumber]\n",
    "ytrain = y[:totalnumber - testnumber]\n",
    "ytest = y[totalnumber - testnumber:totalnumber]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_Xcattrain = new_Xcat[:new_totalnumber - new_testnumber]\n",
    "new_Xcattest = new_Xcat[new_totalnumber - new_testnumber:new_totalnumber]\n",
    "new_Xnumtrain = new_Xnum[:new_totalnumber - new_testnumber]\n",
    "new_Xnumtest = new_Xnum[new_totalnumber - new_testnumber:new_totalnumber]\n",
    "new_ytrain = new_y[:new_totalnumber - new_testnumber]\n",
    "new_ytest = new_y[new_totalnumber - new_testnumber:new_totalnumber]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols + num_numerical_cols\n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = torch.cat([x, x_numerical], 1)\n",
    "        x = self.layers(x)\n",
    "        return nn.functional.softmax(x, dim = -1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(4, 2)\n",
      "    (1): Embedding(2, 1)\n",
      "    (2): Embedding(9, 5)\n",
      "    (3): Embedding(2, 1)\n",
      "    (4): Embedding(1, 1)\n",
      "    (5): Embedding(2, 1)\n",
      "    (6): Embedding(5, 3)\n",
      "    (7): Embedding(6, 3)\n",
      "    (8): Embedding(7, 4)\n",
      "    (9): Embedding(4, 2)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.6, inplace=False)\n",
      "  (batch_norm_num): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=10, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "    (4): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.6, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.6, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.6, inplace=False)\n",
      "    (16): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define and show the model\n",
    "#model = Model(categorical_embedding_sizes, 1, 2, [8,16,32,64,128], p=0.6)\n",
    "model = Model(categorical_embedding_sizes, 1, 2, [10,20,20,10], p=0.6)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimization\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001 , weight_decay = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4355, 0.5645],\n",
      "        [0.7099, 0.2901],\n",
      "        [0.0156, 0.9844],\n",
      "        ...,\n",
      "        [0.6057, 0.3943],\n",
      "        [0.8905, 0.1095],\n",
      "        [0.3582, 0.6418]], grad_fn=<SoftmaxBackward>)\n",
      "epoch:   1 loss: 0.68714494\n",
      "tensor([[0.7941, 0.2059],\n",
      "        [0.7968, 0.2032],\n",
      "        [0.6633, 0.3367],\n",
      "        ...,\n",
      "        [0.4477, 0.5523],\n",
      "        [0.9412, 0.0588],\n",
      "        [0.4467, 0.5533]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1924, 0.8076],\n",
      "        [0.4937, 0.5063],\n",
      "        [0.7391, 0.2609],\n",
      "        ...,\n",
      "        [0.6050, 0.3950],\n",
      "        [0.9794, 0.0206],\n",
      "        [0.5274, 0.4726]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3225, 0.6775],\n",
      "        [0.5261, 0.4739],\n",
      "        [0.6587, 0.3413],\n",
      "        ...,\n",
      "        [0.0811, 0.9189],\n",
      "        [0.3868, 0.6132],\n",
      "        [0.5051, 0.4949]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5400, 0.4600],\n",
      "        [0.7700, 0.2300],\n",
      "        [0.8101, 0.1899],\n",
      "        ...,\n",
      "        [0.6388, 0.3612],\n",
      "        [0.6138, 0.3862],\n",
      "        [0.5650, 0.4350]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7891, 0.2109],\n",
      "        [0.2746, 0.7254],\n",
      "        [0.8102, 0.1898],\n",
      "        ...,\n",
      "        [0.6953, 0.3047],\n",
      "        [0.5616, 0.4384],\n",
      "        [0.5376, 0.4624]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8068, 0.1932],\n",
      "        [0.4753, 0.5247],\n",
      "        [0.5969, 0.4031],\n",
      "        ...,\n",
      "        [0.8792, 0.1208],\n",
      "        [0.5597, 0.4403],\n",
      "        [0.5791, 0.4209]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6120, 0.3880],\n",
      "        [0.4976, 0.5024],\n",
      "        [0.6875, 0.3125],\n",
      "        ...,\n",
      "        [0.1116, 0.8884],\n",
      "        [0.1012, 0.8988],\n",
      "        [0.9915, 0.0085]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[9.7615e-01, 2.3848e-02],\n",
      "        [5.7827e-01, 4.2173e-01],\n",
      "        [6.4679e-01, 3.5321e-01],\n",
      "        ...,\n",
      "        [1.5121e-02, 9.8488e-01],\n",
      "        [6.4347e-01, 3.5653e-01],\n",
      "        [1.0000e+00, 1.8848e-09]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.0000e+00, 7.1576e-09],\n",
      "        [2.7316e-02, 9.7268e-01],\n",
      "        [8.5555e-01, 1.4445e-01],\n",
      "        ...,\n",
      "        [4.9725e-01, 5.0275e-01],\n",
      "        [6.6534e-01, 3.3466e-01],\n",
      "        [7.9840e-01, 2.0160e-01]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6351, 0.3649],\n",
      "        [0.1144, 0.8856],\n",
      "        [0.6108, 0.3892],\n",
      "        ...,\n",
      "        [0.6739, 0.3261],\n",
      "        [0.7347, 0.2653],\n",
      "        [0.5714, 0.4286]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6218, 0.3782],\n",
      "        [0.9024, 0.0976],\n",
      "        [0.7472, 0.2528],\n",
      "        ...,\n",
      "        [0.9237, 0.0763],\n",
      "        [0.4333, 0.5667],\n",
      "        [0.6205, 0.3795]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5022, 0.4978],\n",
      "        [0.5725, 0.4275],\n",
      "        [0.7243, 0.2757],\n",
      "        ...,\n",
      "        [0.5359, 0.4641],\n",
      "        [0.6542, 0.3458],\n",
      "        [0.7245, 0.2755]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.0034, 0.9966],\n",
      "        [0.4950, 0.5050],\n",
      "        [0.0296, 0.9704],\n",
      "        ...,\n",
      "        [0.5762, 0.4238],\n",
      "        [0.6022, 0.3978],\n",
      "        [0.6670, 0.3330]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.9923, 0.0077],\n",
      "        [0.4217, 0.5783],\n",
      "        [0.5874, 0.4126],\n",
      "        ...,\n",
      "        [0.6620, 0.3380],\n",
      "        [0.4862, 0.5138],\n",
      "        [0.1115, 0.8885]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.0213, 0.9787],\n",
      "        [0.5829, 0.4171],\n",
      "        [0.1729, 0.8271],\n",
      "        ...,\n",
      "        [0.4602, 0.5398],\n",
      "        [0.0058, 0.9942],\n",
      "        [0.4055, 0.5945]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7243, 0.2757],\n",
      "        [0.6052, 0.3948],\n",
      "        [0.6101, 0.3899],\n",
      "        ...,\n",
      "        [0.5351, 0.4649],\n",
      "        [0.6152, 0.3848],\n",
      "        [0.7372, 0.2628]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6318, 0.3682],\n",
      "        [0.5475, 0.4525],\n",
      "        [0.8587, 0.1413],\n",
      "        ...,\n",
      "        [0.6221, 0.3779],\n",
      "        [0.5274, 0.4726],\n",
      "        [0.5882, 0.4118]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.9932, 0.0068],\n",
      "        [0.4649, 0.5351],\n",
      "        [0.6915, 0.3085],\n",
      "        ...,\n",
      "        [0.1399, 0.8601],\n",
      "        [0.4380, 0.5620],\n",
      "        [0.1329, 0.8671]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5615, 0.4385],\n",
      "        [0.6218, 0.3782],\n",
      "        [0.6405, 0.3595],\n",
      "        ...,\n",
      "        [0.6926, 0.3074],\n",
      "        [0.3924, 0.6076],\n",
      "        [0.7650, 0.2350]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4788, 0.5212],\n",
      "        [0.3997, 0.6003],\n",
      "        [0.6799, 0.3201],\n",
      "        ...,\n",
      "        [0.6662, 0.3338],\n",
      "        [0.9968, 0.0032],\n",
      "        [0.4733, 0.5267]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7729, 0.2271],\n",
      "        [0.6805, 0.3195],\n",
      "        [0.7602, 0.2398],\n",
      "        ...,\n",
      "        [0.6428, 0.3572],\n",
      "        [0.5248, 0.4752],\n",
      "        [0.5203, 0.4797]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-8c7ca3267576>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0msingle_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "aggregated_losses = []\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(Xcattrain, Xnumtrain)\n",
    "    single_loss = loss_function(y_pred, ytrain)\n",
    "    aggregated_losses.append(single_loss)\n",
    "    \n",
    "    if i % 25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "    \n",
    "\n",
    "\n",
    "# Plot the loss over epocs\n",
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val = model(Xcattest, Xnumtest)\n",
    "    loss = loss_function(y_val, ytest)\n",
    "\n",
    "\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "\n",
    "print(confusion_matrix(ytest,y_val))\n",
    "print(classification_report(ytest,y_val))\n",
    "print(accuracy_score(ytest, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(2, 1)\n",
      "    (1): Embedding(6, 3)\n",
      "    (2): Embedding(3, 2)\n",
      "    (3): Embedding(2, 1)\n",
      "    (4): Embedding(2, 1)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.6, inplace=False)\n",
      "  (batch_norm_num): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "    (4): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.6, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.6, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.6, inplace=False)\n",
      "    (16): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define and show the model\n",
    "#model = Model(categorical_embedding_sizes, 1, 2, [8,16,32,64,128], p=0.6)\n",
    "model = Model(new_categorical_embedding_sizes, 2, 2, [10,20,20,10], p=0.6)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimization\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001 , weight_decay = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 0.70964843\n",
      "epoch:  26 loss: 0.69665974\n",
      "epoch:  51 loss: 0.68599802\n",
      "epoch:  76 loss: 0.67100930\n",
      "epoch: 101 loss: 0.65128976\n",
      "epoch: 126 loss: 0.63406003\n",
      "epoch: 151 loss: 0.61295533\n",
      "epoch: 176 loss: 0.60434514\n",
      "epoch: 201 loss: 0.59097612\n",
      "epoch: 226 loss: 0.58402157\n",
      "epoch: 251 loss: 0.58015555\n",
      "epoch: 276 loss: 0.57855314\n",
      "epoch: 300 loss: 0.5732506514\n",
      "[[590 180]\n",
      " [192 480]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76       770\n",
      "           1       0.73      0.71      0.72       672\n",
      "\n",
      "    accuracy                           0.74      1442\n",
      "   macro avg       0.74      0.74      0.74      1442\n",
      "weighted avg       0.74      0.74      0.74      1442\n",
      "\n",
      "0.7420249653259362\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3hcxbn48e+rVe+9V9tykY2rsLEN2BSDCQQTSMBcQkuAGxISktwkF25ygUuSm4TkElL8I5BQQw+hmFCMMRiMjXFvcpMsuUiyepesPr8/ztGqeGXLWOtVeT/Po8e7c+as3uOV9tXMnJkRYwxKKaVUX16eDkAppdTQpAlCKaWUS5oglFJKuaQJQimllEuaIJRSSrnk7ekABkt0dLRJT0/3dBhKKTWsbN68ucIYE+Pq2IhJEOnp6WzatMnTYSil1LAiIof6O6ZdTEoppVzSBKGUUsolTRBKKaVc0gShlFLKJU0QSimlXNIEoZRSyiVNEEoppVwa9QmipqmV36/cz96SOk+HopRSQ8qoTxAAj64+wEsbjng6DKWUGlJGfYIID/TlkslxvL61iOa2DgBqm9ro6NSNlJRSo9uoTxAA152dQu2xNlbtKSOvrIG5v17FX9fkezospZTyKE0QwLyx0YQH+vDRvjL+45VtNLV28MHuUk+HpZRSHjViFus7HQ4vYe6YKJZvK6a1o5Mx0UFsPVJDfXMbIf4+ng5PKaU8QlsQtnljo2jt6CTYz5v/viKLjk7DZwcqWfZRHhsPVnk6PKWUOuO0BWGbNy4agMvPSmDeuCgCfBy8sa2Id3aWEBfqx/SUcL48LZErpiYC8Pznh0iNDGTe2GgE8PISD0avlFKDTxOEbUx0EL+55iwWjI/Fz9vBOWMieWdnCQCldS2syCmlpK6FK6YmcqSqiZ++vguA8zKjSYkM5H+/cpYnw1dKqUGnXUw2EeG6s1OJD/MH4NxMa4OlED9vnvvmHC6cGEtuaT3tHZ088WmB87z1+ZVs0i4opdQIpC2IfpyXaXU5zc6I5NzMaKqbWvlwbxm7j9bxxrYiZ722DsPBiiY6Og0O7WZSSo0g2oLoR2ZsMEumJ3L97FQAstMjAHhl0xFqmtq4ekaSs25rRyeF1U0eiVMppdzFrQlCRBaLyD4RyRORe1wc/72IbLO/9otITY9jN4tIrv11szvjdEVE+MPSGVycFQdAQlgA6VGBvLKxEIAbzknrVT+/vPFMh6iUUm7ltgQhIg5gGXAZkAVcLyJZPesYY35gjJlujJkO/Al4zT43ErgfmAPMBu4XkQh3xTpQ156dQmtHJxGBPsxMDSchzJ+shFAAfvXuHt7PKfFwhEopNXjc2YKYDeQZY/KNMa3AS8CSE9S/HnjRfnwpsNIYU2WMqQZWAovdGOuAXJudgq/Di1lpEYgIv79uOv937TQA9pc28N0XtzrrPvlpAQ+/v89ToSql1Glz5yB1EtBzidRCrBbBcUQkDcgAPjzBuUkuzrsDuAMgNTX19CM+iehgPx67cRbJEQEAnDMmCoCrZyTx2tYiAnwdzrovbzxCaX0zl06Jp7apzTnPQimlhgt3tiBc3dLT3xKpS4FXjTEdp3KuMeZxY0y2MSY7JibmC4Z5ai6YGEtmXEivsoevm86PL51ATVMbTa3ttLZ3cqC8gZqmNv7975u568WtdOrqsEqpYcadLYhCIKXH82SguJ+6S4Hv9Dl3YZ9zVw9ibIOuq1Vx2zObaO80tNsJobD6GAD7SuuZZI9XKKXUcODOFsRGIFNEMkTEFysJLO9bSUQmABHAZz2KVwCXiEiEPTh9iV02ZCVHBAKw7kAlGwqOnzj34d4yao+1caSqic8OVJ7p8JRS6pS5rQVhjGkXkbuwPtgdwJPGmBwReRDYZIzpShbXAy8ZY0yPc6tE5OdYSQbgQWPMkJ6unBIZ0Ou5w0tIiQiguKaZID8Hv12xj5c3HiE1MpBtR2rYcf8lun6TUmpIc+tMamPMO8A7fcru6/P8gX7OfRJ40m3BDbKYYD/8fbxobuskPNCH6GA/vjYrmaO1zSSE+fOPzYXklTVwuMqaUFdQ2cjOwlre2XmUx2/K9nD0Sil1PF1qY5CICMkRgTS2tPOn62fQ1mGYOzbKefzW+RnM/dUqKhtbAcgpruPpdQfZdqSGmqZWwgN9ASiqOcaHe0q5cW66Jy5DKaWcNEEMopvnpoEI2emRxx3z9fbiWwvG8nlBFZ/sL2f13jK2HbEmjh8ob2RWmpUgXt54hD+uymVRVrxz4UCllPIETRCD6GR/9d9+/hhuP38Ml/9xDa9t7V7wb21eBS9vPIzDy4vW9k4ACioaNUEopTxKE4QHzEyNIKe4jrljoth8qJqHV+53HhsTEwRYCaJnF5VSSp1pupqrB/x48QTe/t65vHD7HDKirYSQFG7dBdW16F9BRQP1zW0UVDRy72s7nS0LpZQ6UzRBeECovw+TE8MQEcbGWgniOxeM61Xnr2sKmPO/q/jzh3m8uOGwc7xCKaXOFE0QHjY7PZKk8ACumZVEYI+1nACaWjt4bau1vPjWw9WeCE8pNYppgvCwW+ZnsOYnF+Dn7XCOP0iP+XNd0wdP1IIoqNC9KJRSg08TxBDQNaN6THQwAA9dM5XFk+O5YY61Qm1qZCBbD7tOEBsKqrjgd6t5d+fRMxOsUmrU0LuYhpCsxFBW5JRwxdREvpadQmF1E5FBvoQF+PCLt/dQUNHoHNTusia3HIDHPsln8ZR4RHT5DqXU4NAWxBByy7x03r37POe+EskRgfzHJRP48rREfB1ePPFpPq3tnby36yhNre2szavgswOVeInVBaUD2UqpwaQtiCHE38fBmJjg48rjQv25emYSr2wqxM/bwROfFhAR6EN1UxsA189O4ZVNhazcXcqMVI/vzKqUGiG0BTFM3LlwLMYYnvi0gNgQP1rbO5kYb21ctCgrjuy0CGtJcTtpKKXU6dIEMUykRQVxw5w0AO77chY7H7iUt757Lk/fejYLx8dy4cRY9pbUM+3B93k/p8TD0SqlRgJNEMPIjy6dwG+uOYvLpiTg5SX4OLxYOCEWLy/hS2clkBjmT7CfN39ff8jToSqlRgDpsU/PsJadnW02bdrk6TA87g8f5PL7D/aTHBFATVMby26YyYLxZ2a/bqXU8CMim40xLjel0RbECHPd2SmMiw1mcmIoAb4O/vxhLg8szyG3tB6A2qY2OjtHxh8FSin30gQxwsSH+fPBDxfw2I3Z3HROGhsPVvP0uoP8+NUdFNccY+6vV/H4mnyqG1tp7+hkybK1vLLxiKfDVkoNQZogRrCls1MZFxvM4snxbDtSw41PfE5Tawd//SSfGT9fyQNv5bD9SA2r95d5OlSl1BDk1gQhIotFZJ+I5InIPf3UuVZEdotIjoi80KP8Ibtsj4j8UXSK8CmLCfHjgx8u4P/dMJMrpyVyoLyRMTFBzm1Pn1t/GID9pQ2eDFMpNUS5baKciDiAZcAioBDYKCLLjTG7e9TJBO4F5htjqkUk1i6fB8wHptpVPwUWAKvdFe9I5uUlPHLddBZlxTF/XDSPfXyAlXtKnXtPHKxopLW9E19vbVAqpbq58xNhNpBnjMk3xrQCLwFL+tS5HVhmjKkGMMZ09XUYwB/wBfwAH6DUjbGOeF5ewpenJRIZ5Mu9X5rEN+ZnOI+1dxpdEVYpdRx3JogkoOfoZ6Fd1tN4YLyIrBWR9SKyGMAY8xnwEXDU/lphjNnT9xuIyB0isklENpWXl7vlIkaq6SnhgLVSLMD+0nq9u0kp1Ys7E4SrMYO+n0DeQCawELge+JuIhIvIOGASkIyVVC4UkfOPezFjHjfGZBtjsmNi9F7/UzEhPoSYED+Wzk7B4SVsOljF5PtX8MdVuZ4OTSk1RLhzsb5CIKXH82Sg2EWd9caYNqBARPbRnTDWG2MaAETkXeAc4BM3xjuq+Di8WPOTC/B1eLGzsJZnPrNmX//5ozxumpuGn7fDuaqsUmp0cmcLYiOQKSIZIuILLAWW96nzBnABgIhEY3U55QOHgQUi4i0iPlgD1Md1ManT4+/jwMtL+NGlE3DYmxbFhfpx1bK1ZN3/HpsP6TanSo1mbksQxph24C5gBdaH+yvGmBwReVBErrSrrQAqRWQ31pjDj40xlcCrwAFgJ7Ad2G6MectdsY52Y2OCef62OSw9O4UjVcc4WNmEMfDdF7bQ1Nru6fCUUh6iazEppze3FXH3S9sAuOeyifz63b1cl53CL78yBW+H3gKr1Eh0orWYdMMg5TTW3qzI20u4ZV46NU1t/OXjAzS0tvPn62fodqZKjTKaIJRT137XExNC8PdxcM9lEwnx9+a3K/ZxTkYkN85N92yASqkzSvsNlFOQnzcT40M4L7P7luFvLxzLrLQI511OSqnRQxOE6mX5Xefyo0smOJ+LCJdNiSevrIHbntnIQ+/t9WB0SqkzSROE6sXX28t5y2uXhRNiAfhgTxn/b/UBRsqNDUqpE9MEoU5qbEwQKZEBzucHyhu4+6Wt/Ogf2zVZKDWC6SC1OikR4bGvZ7O/tJ7vv7yNix/untA+b2wUV89M9mB0Sil30RaEGpCsxFAun5rgfH7fFVnMSovgP/+5gze3FXkwMqWUu2iCUAPm4/BiTkYk0cF+3DwvnSduziYrMYxfvL2HktpmNhRU6YqwSo0g2sWkTsmz35yNMeDwEsIDffnqrGT++41d3PjE5+SWNbBwQgxP3zrb02EqpQaBtiDUKfHzduDv073K69npEQDkllnblq7eV05lQ4tHYlNKDS5NEOq0jI8NIcTfaoj+7PJJAGwoqPJkSEqpQaIJQp0WLy9hTkYUCWH+3Dg3jQAfB5/lVzqPH65s4pWNR3RsQqlhSMcg1Gn7369MobG1Az9vB9npEby2pYgth6tJiwrC20t4c1sx6w5U8MjSGZ4OVSl1CrQFoU5bbKi/c6G/7188ngUTYthVVMfbO46SZ49NvLGtmPrmNk+GqZQ6RZog1KCalRbBsn+byV9vspaXzymuIyzAB4CDFU2eDE0pdYo0QSi3mBAX4nx88aQ4APIrGjwVjlLqC9AEodwiOSKAAPt22AsnxiIC+eWNHo5KKXUqNEEot/DyEsbHWTvUTUoIISk8gPwKTRBKDSduTRAislhE9olInojc00+da0Vkt4jkiMgLPcpTReR9EdljH093Z6xq8I2PC8HHIaREBjImJpgC7WJSalhx222uIuIAlgGLgEJgo4gsN8bs7lEnE7gXmG+MqRaR2B4v8SzwS2PMShEJBjrdFatyj7suHMeirDh8HF6MiQ5ifX4lP3h5G02t7fzp+pn4emsDVqmhzJ2/obOBPGNMvjGmFXgJWNKnzu3AMmNMNYAxpgxARLIAb2PMSru8wRijt8AMM2lRQVwyOR6Am+elMyUxlJW7S1mRU8qb24rYUVjD7c9uoqap1cORKqVccWeCSAKO9HheaJf1NB4YLyJrRWS9iCzuUV4jIq+JyFYR+a3dIulFRO4QkU0isqm8vNwtF6EGR0Z0EK99ez47H7iESQmh/OXjA7y44Qgrd5dy90vbdOMhpYYgdyYIcVHW91PAG8gEFgLXA38TkXC7/DzgR8DZwBjgluNezJjHjTHZxpjsmJiYwYtcuY2IcPPcNA6UN7J6XxkAH+8vp0AHsJUactyZIAqBlB7Pk4FiF3XeNMa0GWMKgH1YCaMQ2Gp3T7UDbwAz3RirOoPmj4sG4GhtM1kJoUD3arBKqaHDnQliI5ApIhki4gssBZb3qfMGcAGAiERjdS3l2+dGiEhXs+BCYDdqREiJDCQp3Nrjesn0RADnkhxKqaHDbQnC/sv/LmAFsAd4xRiTIyIPisiVdrUVQKWI7AY+An5sjKk0xnRgdS+tEpGdWN1Vf3VXrOrMO2dMFABzxkSRGOZPXlmDjkMoNcS4dTVXY8w7wDt9yu7r8dgAP7S/+p67EpjqzviU51w9M4mDlY1MjA9hbGwwr28tYkNBFX//5mzGxAR7OjylFDqTWnnI/HHR/PPOefj7OEiNDASgqOYYv3x7j4cjU0p10QShPG5CvLWw38IJMazaW8bGg7ojnVJDgSYI5XHXz05lxffP59EbZhEZ5Mtv3t3LH1fl0tLecVzdxpZ2vvviVo7WHvNApEqNLpoglMf5OLyYEB9CgK+Dm+ems+lQNQ+v3M+qPWXH1d1yuJq3thezLq/SxSsppQaTJgg1pPz7gjE8fO00fBzCB7tLAThY0Uj2L1ZyoLzBuWR4RUOLJ8NUalTQPanVkOLv4+Dqmcl8mlvBR/vKaO/oZMPBKioaWtlVVOuccV3ZqOs3KeVu2oJQQ9JFk+KobmpjR1EtuaX1gDXzumtPiYp6bUEo5W6aINSQdHZ6BADbDtewr9SaZV1S20x+ufW4QlsQSrmdJgg1JMWG+hMf6s/2whpnC+JgZSNFNdbdS9qCUMr9dAxCDVnTUsL4NLfCOd6w+WA1xkCQr4PKRk0QSrmbtiDUkDUtJdyZHKKD/ahvaQfg/PExlNa1cN+buyiu0fkQSrmLJgg1ZJ03zlrM9yszkrg2OxmAqCBfZqVZ4xPPfnaIt7b3XUFeKTVYtItJDVlnJYeR98vL8HZ48cLnhwGYlRZBTIifs87OotrjzlubV8GUpDDCAnzOWKxKjUTaglBDmrfD+hFNCPMH4Oz0SKKCuhNETnEdAO0dnRhjqG1q4+tPfM7znx8688EqNcJoglDDwtTkMLLTIrh0cjyxoVaC8PYSCioaKa45xoyfr2RFTilHqpswBoqqdWxCqdM1oC4mERkLFBpjWkRkIdY+Dc8aY2rcGZxSXaKC/Xj1znnO5+/efR4ldc3c+tRGlm8vpr65na2Hq52bDpXUNnsqVKVGjIG2IP4JdIjIOOAJIAN4wW1RKXUSkxJCmZIYBsC7u0oAa55Eod1yKKnTBKHU6Rpogui0txD9CvCIMeYHQIL7wlLq5GJC/IgM8mX7Eashe7CiiSPVTYC2IJQaDANNEG0icj1wM/Avu0xvEVEelxnbvT3poapGDldZCaKysdXlfhJKqYEbaIK4FZgL/NIYUyAiGcBzJztJRBaLyD4RyRORe/qpc62I7BaRHBF5oc+xUBEpEpE/DzBONcp07UYH0NzWyeZD1c7nZXU621qp0zGgQWpjzG7gewAiEgGEGGN+faJzRMQBLAMWAYXARhFZbr9WV51M4F5gvjGmWkRi+7zMz4GPB3oxavTJjLMSRHyoPyV1zdQ3t5MZG0xuWQNHa5tJsfe7VkqdugG1IERktf3XfCSwHXhKRB4+yWmzgTxjTL4xphV4CVjSp87twDJjTDWAMca5hZiIzALigPcHdilqNJpgJ4gF42OcZZdPtYbHdKBaqdMz0JnUYcaYOhG5DXjKGHO/iOw4yTlJwJEezwuBOX3qjAcQkbWAA3jAGPOeiHgB/wfcCFzU3zcQkTuAOwBSU1MHeClqJJmcGMpZSWFce3YyZ2dEkhoZyIT4EB75IJd7/7kDh4gzYSilTs1AxyC8RSQBuJbuQeqTERdlpu/rApnAQuB64G8iEg58G3jHGHOEEzDGPG6MyTbGZMfExJyoqhqhgvy8eeu75zIrLZKvzkpmdkYkYQE+3HdFFg4vYdWeUk+HqNSwNdAE8SCwAjhgjNkoImOA3JOcUwik9HieDPRdWa0QeNMY02aMKQD2YSWMucBdInIQ+B1wk4iccMxDqZ6+cW4GM1Ij2FNST2NLO4sf+YSXNhz2dFhKDSsDShDGmH8YY6YaY+60n+cbY645yWkbgUwRyRARX2ApsLxPnTeACwBEJBqryynfGHODMSbVGJMO/Ahr1rbLu6CU6s/EhBDyyur5xdt72FtSzzOf6fpMSp2KgQ5SJ4vI6yJSJiKlIvJPEUk+0Tn2xLq7sFoee4BXjDE5IvKgiFxpV1sBVIrIbuAj4MfGmMovfjlKdZsUH0pbh+FFu+Xg63DV66mU6s9AB6mfwlpa42v286/bZYtOdJIx5h3gnT5l9/V4bIAf2l/9vcbTwNMDjFMpp4kJ3XMkLsmKY+PBKpf18ssbCPB1kBAWcKZCU2pYGOgYRIwx5iljTLv99TSgo8JqSBsTbc2yPndcNLPSIqhuaqOuua1Xndb2Tq57fD3fe3Gry9d4ZdMRHv/kgNtjVWooGmgLokJEvg68aD+/HtCuIDWk+Xp7se6eC4kO9nPezXS4somy+mZSIwMZFxvCOzuPUl7fQnl9CwUVjWREB/V6jX9uLqSysZU7zh/riUtQyqMG2oL4BtYtriXAUeCrWMtvKDWkJYYH4Ovt5ZxRvaOwlm/9fQs//9ceAJ7//BCJYf54Cby2pfC48ysbW6nv0+pQarQY6F1Mh40xVxpjYowxscaYq4Cr3RybUoMmNcpKEH/5+ACtHZ18ll9JbVMb247UcOX0JM5KCmPL4erjzqtoaKGhuf1Mh6vUkHA6O8r1O7Cs1FAT6u9DRnQQh6uaCPR10NreyRNrC2jrMExLDiM21J/KhtZe57R1dFLT1EZjawcdnX3neCo18p1OgtB7BtWw8sLtc7hpbhqPXDedIF8Hf1xlzfWcmhJOdLAvFQ29V3+tauxOGA0t2opQo8/pJAj9k0oNKwlhATy4ZAqXTI7nmlnd03gSw/yJDvajqrG1V0uhvL47YWiCUKPRCROEiNSLSJ2Lr3og8QzFqNSg+/bCcYC1K52IEB3sR6eB7724lV/8y1qRvmeLQsch1Gh0wttcjTEhJzqu1HAVH+bPC7fNITbUD4CoYF8A3t55lLSoQH52RVavMQm9k0mNRgOdB6HUiDNvXLTzcXSwn/Px4aomjrV29GpB1GsXkxqFTmcMQqkRo2eCMAb2ldZT1mMMol67mNQopC0IpYBou4upy1XL1vZ6rmMQajTSFoRSQFiADz4uVnudGG8NwzW0tFFUc4yc4tozHZpSHqMJQilARIgK8iM+1N9ZtuYnF/DcbXMQsbqYvv/SVq57bD01Ta0neCWlRg7tYlLKFh/mT7CfN/d+aSLtHca5flOwnzefF1Sx8aC1FMcfVuVy72WT8PXWv6/UyKYJQinb7742DV+Hl3Pdpi4hft5sKKjC38eLc8fF8NTag2w7UsPr357voUiVOjP0TyClbONig49LDgCBftbfUYuy4nn06zNZMj2RvNKGMx2eUmecJgilTiKvzEoGl06Ow8fhRUZ0EPUt7bqAnxrxNEEoNUALxlubKIYF+AA6u1qNfG5NECKyWET2iUieiNzTT51rRWS3iOSIyAt22XQR+cwu2yEi17kzTqVO5A9Lp/PjSycQ4m8lhlD737pj1tyIN7YW8eU/fUppXbPHYlTKHdyWIETEASwDLgOygOtFJKtPnUzgXmC+MWYy8H37UBNwk122GHhERMLdFatSJ7JkehLfuWCc83mo3YLo2t/6XzuOsrOolpue2EBHp2H59mLm//pD3ttVwoW/W01zW4dH4lbqdLmzBTEbyDPG5BtjWoGXgCV96twOLDPGVAMYY8rsf/cbY3Ltx8VAGRDjxliVGrBQf2vQuvaYlSBa2q0EsK+0nk/zKthQUElRzTGeWltAfkUjhdVNHotVqdPhzgSRBBzp8bzQLutpPDBeRNaKyHoRWdz3RURkNuALHHBbpEqdAmcLwk4QR2ubuWhiLBGBPryy8QiHKq2EsOFgFQAltS2uX0ipIc6d8yBc7TjX97YPbyATWAgkA2tEZIoxpgZARBKAvwM3G2M6j/sGIncAdwCkpqYOXuRKnUBYny6mktpmzsuMJiUykBc+P0x4oHXc2D/tJTo2oYYpd7YgCoGUHs+TgWIXdd40xrQZYwqAfVgJAxEJBd4GfmaMWe/qGxhjHjfGZBtjsmNitAdKnRndLYh26prbaGhpJyHMn/Myo2nt6Oy1CixASe0xT4Sp1GlzZ4LYCGSKSIaI+AJLgeV96rwBXAAgItFYXU75dv3XgWeNMf9wY4xKnbIgXwdeYo1BFNdYH/4JYQHMTI1w1vH26m5AawtCDVduSxDGmHbgLmAFsAd4xRiTIyIPisiVdrUVQKWI7AY+An5sjKkErgXOB24RkW3213R3xarUqRARQgN8+PNHeSx+ZA0ACWH+RAT5MjYmCIC5Y6MAiArypaS2hf2l9dz2zCaOteodTWr4cOtaTMaYd4B3+pTd1+OxAX5of/Ws8xzwnDtjU+p0hAX4UNPUPVEuPsxaBTY7LZID5Y389PJJ7Cup5/WtRZTUHeO9XSV8sKeUzYeqOTczur+XVWpI0ZnUSn0BwX69/7aKDbESxI1z07hz4VgmxIWwZHoS8aH+lNS2sOdoHQCPr8ln3q9WUV6vdzapoU9Xc1XqC6hqtPaEuHRyHNHBfs6lv6ckhTElKcxZLy7Un4qGFnYUWhsNfbK/HIB/7Shm3YFKHr52mnOGtlJDjSYIpb6Ao7XWwPMPFo1nYnxov/Uy44IBKKo5hkj3ra//89ZuANbmVbJ4Srx7g1XqC9IuJqW+gMggaw/rcTHBJ6y3KCvO+fiCCbEA+Pt0/9p1Gl0RVg1dmiCU+gJeu3Mez35jNt6OE/8K+Xk7+NqsZADuuyKL52+bw6Ks7hZDZaNuX6qGLu1iUuoLSI8OIj06aEB1f3X1Wdw6P8N5zo7CWt7abs0ZrWzQwWo1dGkLQik383Z4kZXYPU5xwcQYxttjE1XaglBDmCYIpc6wifGhvP+DBYyJDqKysZX88gZm/nwlB8p1G1M1tGiCUMpDooJ9qWpoZfOhaqoaW9lyqNrTISnViyYIpTwkMsiXqsZWCioaAThY2UhxzTHO/c2HPLr6AEbvcFIepglCKQ+JDPKjsrGlO0FUNLE2r4LC6mP85r29rLYn1SnlKZoglPKQqCBfqpvanGMPBRWN7CisxdtLEIHNB0/c5WSM4X/eymGTvTGRUoNNb3NVykMig3zp6DTsL7USxMHKRrwdwtnpkVQ2tpBTXHvC8xta2nlq7UFqmtrITo88EyGrUUZbEEp5SFSwr/PxxPgQmlo72FFYy9SUMCYnhrHbXuCvP10bE205rIPbyj00QSjlIT3XcLp0cvfs6hkpEUxODKW0roWKhhbqm9s4539XsXpfWa/zy+qsBHGosokKnXCn3EC7mJTykAnxIex44APq4W4AABmzSURBVBJyS+uZmhxOWIAP0SF+XDwplo32+MOOwhpiQ/wpqWtm5e5SFtrrOQGU1XfvVLf1cE2vdZ+UGgyaIJTyoFB/H2alWeMH3zg3w1k+I9VKGK9tKWLJ9CQAPsuv5JpH1xHq780PF01w7inhJbDpUBUPvbeXa2Yl860FY8/8hagRSbuYlBqC/H0cXJudzHu7SthRWANAfnkjmw9Vs+1IDdf/dT0f7y/Hz9uLs5LDWb6tmNyyBt7ZedTDkauRRBOEUkPU0tmptHcaXvj8sLNscmIo79x9HsYY1uRWEBvqx8zUcOf+FDuLaqntsRWqUqdDE4RSQ9SY6CCC/bypbGwlPNCH2BA/vr1wHAlhAcxMiwCsrU5n2Y/B2pDos/xKT4WsRhi3JggRWSwi+0QkT0Tu6afOtSKyW0RyROSFHuU3i0iu/XWzO+NUaigSEeeqr+lRQWz46cVcPjUBgNn2vAc/by9mploJYsH4GAJ8HKzvkyCeWlvA+zklZzByNVK4bZBaRBzAMmARUAhsFJHlxpjdPepkAvcC840x1SISa5dHAvcD2YABNtvn6g3falSZEB/ClsM1JIT59yqfnWEliMNVTSSGB3D1zCQum5LAsdYOtttjFmBNpuva3vTgry8/c4GrEcGdLYjZQJ4xJt8Y0wq8BCzpU+d2YFnXB78xputG70uBlcaYKvvYSmCxG2NVakgaHxcCQFxo7wQxLSUcgKVnpwDw8LXTWZQVx9TkMHYX19HW0QnAJ7qekzoN7rzNNQk40uN5ITCnT53xACKyFnAADxhj3uvn3KS+30BE7gDuAEhNTR20wJUaKibYCaJvC8Lfx0HuLy/D20t6lZ+VHEZLeyf7S+uZnBjGyt2lAM6uKqVOhTtbEOKirO/6xd5AJrAQuB74m4iED/BcjDGPG2OyjTHZMTExpxmuUkPPWclhTIwP4eyM49da8nF4IdL7V2VastWy2FlYS2en4WO7BdHY0sEfPsjl0dUH3B+0GjHc2YIoBFJ6PE8Gil3UWW+MaQMKRGQfVsIoxEoaPc9d7bZIlRqiQvx9eO/75w+4flpUIGEBPmw7UsOM1AiqGlsJ8fOmrrmN33+wH7C2PO25zIdS/XFnC2IjkCkiGSLiCywFlvep8wZwAYCIRGN1OeUDK4BLRCRCRCKAS+wypdQJiAgzU8PZfKiazwusu5kWZcXR0NLurPPLt/cA8Pznh3rNsVCqL7clCGNMO3AX1gf7HuAVY0yOiDwoIlfa1VYAlSKyG/gI+LExptIYUwX8HCvJbAQetMuUUieRnR5JblkD7+0qISk8gEkJoXRtThfk62BNbgWPfXyAn76+i/96fSd/WpXLnc9tdg5sn4qc4lrufmkr7V/gXDX0yUjZ1jA7O9ts2rTJ02Eo5XEbCqq49rHPAPjarGTOTo/kJ//cAcB/Lp7IX9fkU9XYetx5iWH+LJgQw6+unjrg7/XIB/t55INc1t1zIYnhAYNzAeqMEpHNxphsV8d0JrVSI8zU5DDn4x8sGk+If/dQY3JEAH+7OZvffnUqP79qirM8MsiX4tpm3tjad5gQ6pu7d73r62hNs12n3eVxNbzpaq5KjTD+Pg5+ftUUMqKCSAwPcO55DVYimJkawczUCPLKuj/0H7pmKnnlDfz63b3UNrXh6+2Fv48XHZ2GX769h5c2HmH5XfOZat8l1aW49hhgJRE18miCUGoEuvGcNOfjni2IyKDuXezSowLxdXjR2tHJlKQwmts7ALjz+c2sO1DJ/HFRlNW10HUn7Z3PbeGfd84jvsecjKKargShLYiRSLuYlBrhQv19nI97JghvhxdjYoKIDvYlLtSPJHsMYd0B6+6ntXmV5JY1UFDRyNTkMGqaWrn5yQ10dFrjlsYYZxdTnd2CWJdXwfxff0jtMW1RjASaIJQa4Xq2IMIDfXodu2luOredNwYRITki0Fk+b2yUc+Ohtg7Dtdkp/O/VZ7GvtJ5P8yoAqGlq41ib1eroakH88JXtFNUcY19JvVuvSZ0ZmiCUGuFC7BZEiJ83ft6OXsf+bU6qMxFEB/vi5219JHwtO5mfXDrBmVwmJYSyeEo8EYE+vLLJWgWna/wBYPfROu5/cxcldVaLoqap911SLe0dLPsoj2Y7oajhQROEUiNc14BzRI/uJVdEhKQIq5spKyEMLy8hOy0CEZgYH4Kft4OrZiTxfk4JBRWNFNd074n9ysYjPPPZIefziobeCeLNbcX8dsU+/vxhHgCd9kZITa06djGUaYJQahQI9fc5aYIASAoPwNcemwC4dX4G3144liA/qyVx54Kx+Hs7uPe1Haw7YHU1eXsJ7fa4RNcttl37Zfe1q7gWgM2Hq/mv13fy0oYjLuupoUEThFKjQHigD9EDSBBXTkvkprlp+Disj4bzx8fw40snOo/HhvrzsysmsT6/iqfWHmTJ9ETnBLnkiACW33UuEYE+lDc093rdri6nw1VNAOw5WgfAJ7mulyPv7DT87I2dbD9S4/K4OjP0NlelRoH/uXIKoQEn/3X/WnbKSetcd3YqcaH+rM2r4D8umcA1j64DIDbED4CYED8q6ls5WnuMB9/azSWT45xdTocrm2hu62DPUWsQe31+Jc1tHfj79B4bKa1v5rn1hwnx93HufaHOPE0QSo0Cc8dGDerrLZwQy8IJsUD3XVJdmxpFB/tR3tDCa1uKeHdXCe/uKmGs3WXV3mn45jMbOVjRRICPg2NtHWw8WMV5mb2X6z9UabU0+uuqUmeGdjEppU5L111SPVsQ5fUtfLS3zJk8DpQ3clZSGD9cNJ6NBdUU1RzjymmJiMCWQ8d3Ix2qtGZ/l2mC8ChNEEqp09KVBGLtFkRMsB+Hq5rYcriar5+ThsPe9S4mxI/vXZTJ9bOtbqysxFDGxgSzs8hVgrBaEGV1zccdU2eOJgil1GkJddGCAOg0cElWnHOGdnSwNUh+75cm8Z+LJ3L1zCSmJoWxvbAWYwybD1XT3NbB458cYK09Ga+iQVsQnqRjEEqp09J3DKJrtvbCCTFMTwknLSqQw1VNRAVbicPfx8GdC63JeVOTw3htaxGr9pRx27ObWDA+xrlNKkBlYyvtHZ14O/RvWU/QBKGUOi3dXUxWAlg8OYGapjZunpeOiJAWFciaXIhycZvtWfbqsPcvzwHolRwAjLEm3fVcILCz0+Dl5Wrb+m5dg9sxIX6U1DbT1tFJSmSgy7q1x9oor29mXGzIQC53VNG0rJQ6LVOSwhgbE0Sq/QEcFujDvy8Y67x1NS3SuoOpq+upp2nJYczOiKSo5pizK+rccdGAtVUqWB/2/9pRTF6ZdWvs5X/6lG/9fTP9bXZWXHOMxY98wi1PbQDgnF+t4ryHPuo3/t+u2MtVy9bR0q7LgPSlLQil1GmZNzaaVf+xsN/jaVFW4ogOPj5BeDu8+NvN2fxpVS7Xz07l0dUHuHV+BuPjgtlVXMfK3aVsPlTFA2/tJjLIl2e/MZs9R+vYc7SOZz87xM3z0o97zXte20llYyuVja18tK/spPGvz6+ioaWdrYdrOGfM4N4OPNxpglBKudXCCbE8uGQyczIiXR4P9ffhp5dnAfDbr01zlne1OJ5YWwBAY0s7P319p/P4/72/j4KKRsbEBNHQ0s7YmGBmpISzJrecW+al88Lnh7n1qY3O+i3tHcctVljV2OrcOGldXsVxCeLdnUf5/Qf7+ce35hEW0Hsl3FNhjOHljUe4bEoCYYFf/HXONLd2MYnIYhHZJyJ5InKPi+O3iEi5iGyzv27rcewhEckRkT0i8kcROXGno1JqSPL19uKmuemnPNAcE+xHoK+DI1XHmJEazszUCLYXWms5/WHpdOpb2nl63UHuezOHh97bxw9f3saTaw9iDNwwJ5Ubzknt9XpldcffEbX5UDUAQb4O1tr7YHTp7DT87v197C9t4PnPDx13rGu5kIEoqGjkntd28vrWwgGfMxS4LUGIiANYBlwGZAHXi0iWi6ovG2Om219/s8+dB8wHpgJTgLOBBe6KVSk19Ph6e/H0rbOZNzaKby8cx5SkUABE4NLJ8fz+2un8/Zuz+e8rsrj/y1m0dxr+8vEBxscFkxkXwv1fnsyb35nPzy6fBEBpnzkVnZ2G17cW4uMQls5OZfuRGhpauleX/XBvGQfKG4kM8uXJTw/2GqN4c3sRl/1hDTsKB7ZWVNfKt4XVx05Sc2hxZxfTbCDPGJMPICIvAUuA3QM41wD+gC8ggA9Q6qY4lVJD1OyMSF64/RzA6mICa8VZfx9r6XHAuUzH5MQw1udXsnBC97Id01LC8bX3uCjt04L4w6pc3tlZwvcvzuTs9Eie+LSADQWVXDjRGhxfvr2YqCBfHvrqVL75zCY+2lvG4ikJAKzYZX0cfbCn7Lh9ul05au+d0bVF63Dhzi6mJKDnWr6Fdllf14jIDhF5VURSAIwxnwEfAUftrxXGmD19TxSRO0Rkk4hsKi93vSqkUmpk6GpBZEQHuTw+OyOS712UedwHdrw9P6NrM6Pl24t5YHkOT60t4NLJcdx9USaz0iLw9fZibV4leWUNPLr6AB/tK+PCidaaU7Ehfry6uQiwxjLW2KvQrh7AIDjA0Vrre3cliM5Ow4Nv7ea9XUdP5b/gjHNngnA1ZtD3vrS3gHRjzFTgA+AZABEZB0wCkrGSyoUicv5xL2bM48aYbGNMdkxMTN/DSqkRJCM6mLAAHybGn9p8hfBAH3y9vSita6a5rYMH38rh6XUHqWtud2636u/jIDstgrV5Ffzpw1x+895e6pvbuTgrDoeX8JUZSazeV0ZtUxuPfZxPY2sH2WkR7CisPeGCgi3tHdz35i62HLbGOorsLqb8ikaeXFvAt57bwoqcki/+n+Jm7kwQhUDPtYOTgeKeFYwxlcaYrv/dvwKz7MdfAdYbYxqMMQ3Au8A5boxVKTXEObyEt+46l7svHn9K54kI8aH+vLvrKD99fRcVDa186ax4rpqeSHZahLPe/HHR7C2pZ5u9B0VqZCDnZUY7j7V3Gh5asZeHV+5nUVYcP1ls7ZPR9eHvyuf5VTz72SFW77NaHJWNrZx1/wp+u2Kvs86GgqoBX8vOwlpyS8/cft/uHIPYCGSKSAZQBCwF/q1nBRFJMMZ0tbGuBLq6kQ4Dt4vIr7BaIguAR9wYq1JqGEiNcj0b+mQ6Og1FNcc4UlXI9JRwlv3bTPreGDnPXhL9UGUTd5w/hnsvm+isMynB6t56dXMhwX7e/OXrs2ht78RLIKeolksnx1NS29xrxjfApoPdH/4i1szw+pZ2VuSU4uftRVyov7PrayC+/OdPAdjwXxc5F0d0J7e1IIwx7cBdwAqsD/5XjDE5IvKgiFxpV/uefSvrduB7wC12+avAAWAnsB3Ybox5y12xKqVGtlvnp3PF1AS23beI1+6cd1xyADgrKYwQe2vVqclhverEhPgRHexHS3sn01LCcHgJAb4OMmND2FlUy/LtxZzzq1U8Zc/Z6LLpUHfrYmJ8aK9jExNCSQoPoKR2YAmi5/7dD7yVM6BzTpdbJ8oZY94B3ulTdl+Px/cC97o4rwP4d3fGppQaPW47b8xJ63g7vJgzJooP9pQyzcWdSZMSQliT28KMlO5uqSlJYbyfU8L+0gZE4Jdv76G1vZMJ8SE8t/4Q6w5UEhviR1l9C3MyItlztI4Qf2/qm9uZkhjKsdYOPu/TxbQur4K06CDn0iNd9pdaE/rSogJZubuUqsZWIgewjezp0LWYlFLK9vVzUrlyWiLJEQHHHevqZpreYwvUKUmh1Le0U1RzjEdvmMn542P41bt7ueWpjc6FB3+waDzBft5cPCmON74zn7X3XMjU5DAWZcURH+ZPaV0znZ3W/TvHWju45amN/G7FPsDakrWgwto8aa89Me/eyybR1mH4xb92O8dL3EWX2lBKKVvPrVT7WjA+hn9tLyY7vbsFsSgrjlV7yvjmeRlcMCGWxVMS2FVUS15ZAxdNiqWptYPYED+uy07ptQLt8rvOBeBwVRPtnYaKxhZiQ/zZcria1o5O517d33x6I+Nig3njO/PZW1JPoK+DS7LiOCvJWib9QHkDb9qv5Q6aIJRSagDmj4tm3b0X9SpLjgjkudvm9CqbkhTGlKQwoHs71v4WCnLO0ahtJsDH4dwo6WhtM//YXEhjawfbC2tZk1vBpkNVTIgPwctL+Ps3Z/PdF7eSU1zH7uI6Anwd/c4POR2aIJRSykO67nr65+ZCXrUTQrCfNw0t7Tz8/j4CfBwE+Tm46Ulr6fKfXzUFgPBAX+aNjWZNbgX/9fpOOjoNb3138FsSOgahlFIe0pUgnvnsEPFh/vg6vPj2BWOJD/WnuqmN8zKjefVb87j7okwev3EWN56T5jy3a/+NbUdqyEoIdfn6p0tbEEop5SHRQX5MSQolJSKQ33x1KkG+3ghwbXYK6/MrmZEaQVJ4AD9YdPzkwNQeO+RNTtIEoZRSI4qXl/Cv7553XHl0sB9XTE084bk9E4S7WhDaxaSUUsNQWKAPof7eiFiT7txBWxBKKTVMpUYF0thiDWy7gyYIpZQapu66IJOOzr6LZA8eTRBKKTVMLZ4S79bX1zEIpZRSLmmCUEop5ZImCKWUUi5pglBKKeWSJgillFIuaYJQSinlkiYIpZRSLmmCUEop5ZIY475ZeGeSiJQDh07jJaKBikEKx9NGyrWMlOsAvZahSq8F0owxMa4OjJgEcbpEZJMxJtvTcQyGkXItI+U6QK9lqNJrOTHtYlJKKeWSJgillFIuaYLo9rinAxhEI+VaRsp1gF7LUKXXcgI6BqGUUsolbUEopZRySROEUkopl0Z9ghCRxSKyT0TyROQeT8dzqkTkoIjsFJFtIrLJLosUkZUikmv/G+HpOF0RkSdFpExEdvUocxm7WP5ov087RGSm5yI/Xj/X8oCIFNnvzTYR+VKPY/fa17JPRC71TNSuiUiKiHwkIntEJEdE7rbLh9V7c4LrGHbvi4j4i8gGEdluX8v/2OUZIvK5/Z68LCK+drmf/TzPPp7+hb6xMWbUfgEO4AAwBvAFtgNZno7rFK/hIBDdp+wh4B778T3AbzwdZz+xnw/MBHadLHbgS8C7gADnAJ97Ov4BXMsDwI9c1M2yf9b8gAz7Z9Dh6WvoEV8CMNN+HALst2MeVu/NCa5j2L0v9v9tsP3YB/jc/r9+BVhql/8FuNN+/G3gL/bjpcDLX+T7jvYWxGwgzxiTb4xpBV4Clng4psGwBHjGfvwMcJUHY+mXMeYToKpPcX+xLwGeNZb1QLiIJJyZSE+un2vpzxLgJWNMizGmAMjD+lkcEowxR40xW+zH9cAeIIlh9t6c4Dr6M2TfF/v/tsF+6mN/GeBC4FW7vO970vVevQpcJCJyqt93tCeIJOBIj+eFnPgHaCgywPsisllE7rDL4owxR8H6JQFiPRbdqesv9uH6Xt1ld7s82aOrb9hci901MQPrL9Zh+970uQ4Yhu+LiDhEZBtQBqzEauHUGGPa7So943Vei328Fog61e852hOEq4w63O77nW+MmQlcBnxHRM73dEBuMhzfq0eBscB04Cjwf3b5sLgWEQkG/gl83xhTd6KqLsqGzPW4uI5h+b4YYzqMMdOBZKyWzSRX1ex/B+VaRnuCKARSejxPBoo9FMsXYowptv8tA17H+sEp7Wri2/+WeS7CU9Zf7MPuvTLGlNq/1J3AX+nurhjy1yIiPlgfqs8bY16zi4fde+PqOobz+wJgjKkBVmONQYSLiLd9qGe8zmuxj4cx8C5Qp9GeIDYCmfadAL5YgznLPRzTgIlIkIiEdD0GLgF2YV3DzXa1m4E3PRPhF9Jf7MuBm+w7Zs4Baru6O4aqPv3wX8F6b8C6lqX2nSYZQCaw4UzH1x+7r/oJYI8x5uEeh4bVe9PfdQzH90VEYkQk3H4cAFyMNabyEfBVu1rf96Trvfoq8KGxR6xPiadH5z39hXUHxn6s/ryfejqeU4x9DNZdF9uBnK74sfoaVwG59r+Rno61n/hfxGrit2H9xfPN/mLHajIvs9+nnUC2p+MfwLX83Y51h/0Lm9Cj/k/ta9kHXObp+Ptcy7lY3RE7gG3215eG23tzgusYdu8LMBXYase8C7jPLh+DlcTygH8Afna5v/08zz4+5ot8X11qQymllEujvYtJKaVUPzRBKKWUckkThFJKKZc0QSillHJJE4RSSimXNEEoNQSIyEIR+Zen41CqJ00QSimlXNIEodQpEJGv2+vybxORx+wF1BpE5P9EZIuIrBKRGLvudBFZby8K93qP/RPGicgH9tr+W0RkrP3ywSLyqojsFZHnv8jqm0oNJk0QSg2QiEwCrsNaIHE60AHcAAQBW4y1aOLHwP32Kc8C/2mMmYo1c7er/HlgmTFmGjAPawY2WKuNfh9rX4IxwHy3X5RSJ+B98ipKKdtFwCxgo/3HfQDWgnWdwMt2neeA10QkDAg3xnxslz8D/MNeOyvJGPM6gDGmGcB+vQ3GmEL7+TYgHfjU/ZellGuaIJQaOAGeMcbc26tQ5L/71DvR+jUn6jZq6fG4A/39VB6mXUxKDdwq4KsiEgvOPZrTsH6PulbU/DfgU2NMLVAtIufZ5TcCHxtrP4JCEbnKfg0/EQk8o1eh1ADpXyhKDZAxZreI/AxrBz8vrJVbvwM0ApNFZDPWzl3X2afcDPzFTgD5wK12+Y3AYyLyoP0aXzuDl6HUgOlqrkqdJhFpMMYEezoOpQabdjEppZRySVsQSimlXNIWhFJKKZc0QSillHJJE4RSSimXNEEopZRySROEUkopl/4/4R0u8b+SZ+0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 300\n",
    "aggregated_losses = []\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(new_Xcattrain, new_Xnumtrain)\n",
    "    single_loss = loss_function(y_pred, new_ytrain)\n",
    "    aggregated_losses.append(single_loss)\n",
    "    \n",
    "    if i % 25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "    \n",
    "\n",
    "\n",
    "# Plot the loss over epocs\n",
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val = model(new_Xcattest, new_Xnumtest)\n",
    "    loss = loss_function(y_val, new_ytest)\n",
    "\n",
    "\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "\n",
    "print(confusion_matrix(new_ytest,y_val))\n",
    "print(classification_report(new_ytest,y_val))\n",
    "print(accuracy_score(new_ytest, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times low scoretext is predicted:  1954\n",
      "Times medium scoretext is predicted:  534\n",
      "Times high scoretext is predicted:  639\n",
      "Accuracy of the random forest model:  0.7547169811320755\n"
     ]
    }
   ],
   "source": [
    "# Define the model and fit it to the data\n",
    "forestModel = RandomForestClassifier(n_estimators = 53, max_depth = 14, max_features = \"log2\", criterion = \"entropy\")\n",
    "forestModel.fit(Xcattrain, ytrain)\n",
    "\n",
    "# Predict on the test set\n",
    "forestPreds = forestModel.predict(Xcattest)\n",
    "forestProbs = forestModel.predict_proba(Xcattest)[:, 1]\n",
    "\n",
    "print(\"Times low scoretext is predicted: \", len(forestPreds[forestPreds == 0]))\n",
    "print(\"Times medium scoretext is predicted: \", len(forestPreds[forestPreds == 1]))\n",
    "print(\"Times high scoretext is predicted: \", len(forestPreds[forestPreds == 2]))\n",
    "\n",
    "print(\"Accuracy of the random forest model: \", len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted no recidivism:  803\n",
      "Predicted recidivism:  639\n",
      "Accuracy of the random forest model:  0.6678224687933426\n"
     ]
    }
   ],
   "source": [
    "# Define the model and fit it to the data\n",
    "forestModel = RandomForestClassifier(n_estimators = 53, max_depth = 14, max_features = \"log2\", criterion = \"entropy\")\n",
    "forestModel.fit(new_Xcattrain, new_ytrain)\n",
    "\n",
    "# Predict on the test set\n",
    "forestPreds = forestModel.predict(new_Xcattest)\n",
    "forestProbs = forestModel.predict_proba(new_Xcattest)[:, 1]\n",
    "\n",
    "print(\"Predicted no recidivism: \", len(forestPreds[forestPreds == 0]))\n",
    "print(\"Predicted recidivism: \", len(forestPreds[forestPreds == 1]))\n",
    "\n",
    "print(\"Accuracy of the random forest model: \", len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == new_ytest]) / len(forestPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baysian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[290.   2.   0.   1.]]\n",
      "[[170.  73.   0.   1.]]\n",
      "[[160.  40.   1.   1.]]\n",
      "[[27. 62.  0.  0.]]\n",
      "[[55. 59.  1.  0.]]\n",
      "[[290.   2.   0.   1.]]\n",
      "[[289.   2.   0.   1.]]\n",
      "[[300.  54.   1.   0.]]\n",
      "[[249.   1.   0.   0.]]\n",
      "[[265.   1.   1.   0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[259.   1.   0.   1.]]\n",
      "[[232.   1.   1.   1.]]\n",
      "[[1. 1. 1. 1.]]\n",
      "[[32.  1.  1.  1.]]\n",
      "[[17.  1.  1.  0.]]\n",
      "[[65.  1.  0.  0.]]\n",
      "[[51.  1.  1.  0.]]\n",
      "[[289.   1.   1.   1.]]\n",
      "[[275.   1.   0.   1.]]\n",
      "[[215.   1.   0.   0.]]\n",
      "[[225.   1.   0.   0.]]\n",
      "[[92.  1.  1.  0.]]\n",
      "[[115.   1.   0.   0.]]\n",
      "[[103.   1.   0.   0.]]\n",
      "[[177.   1.   1.   0.]]\n",
      "[[193.   1.   1.   0.]]\n",
      "[[146.   1.   1.   0.]]\n",
      "[[109.   1.   1.   1.]]\n",
      "[[11.  1.  0.  1.]]\n",
      "[[1. 1. 0. 0.]]\n",
      "[[159.   1.   0.   1.]]\n",
      "[[133.   1.   0.   1.]]\n",
      "[[82.  1.  0.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[68.  1.  1.  1.]]\n",
      "[[184.   1.   0.   1.]]\n",
      "[[150.   1.   0.   0.]]\n",
      "[[204.   1.   1.   1.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[76.  1.  1.  0.]]\n",
      "[[263.   8.   0.   1.]]\n",
      "[[108. 109.   0.   0.]]\n",
      "[[229. 109.   0.   0.]]\n",
      "[[  1. 109.   1.   1.]]\n",
      "[[300. 109.   0.   1.]]\n",
      "[[ 1. 23.  1.  0.]]\n",
      "[[101.  25.   0.   0.]]\n",
      "[[ 53. 109.   0.   1.]]\n",
      "[[300.  13.   0.   1.]]\n",
      "[[201.  21.   1.   0.]]\n",
      "[[164. 109.   1.   1.]]\n",
      "[[51. 16.  0.  1.]]\n",
      "[[262.  82.   1.   0.]]\n",
      "[[139.  13.   0.   0.]]\n",
      "[[103.  71.   1.   1.]]\n",
      "[[125.   1.   1.   0.]]\n",
      "[[213.  72.   1.   1.]]\n",
      "[[167.   1.   1.   0.]]\n",
      "[[42.  1.  0.  1.]]\n",
      "[[39.  1.  1.  0.]]\n",
      "[[25.  1.  0.  0.]]\n",
      "[[255.   1.   1.   0.]]\n",
      "[[ 1. 75.  1.  1.]]\n",
      "[[214.   1.   1.   1.]]\n",
      "[[121.   1.   0.   1.]]\n",
      "[[240.   1.   0.   1.]]\n",
      "[[172.   1.   0.   1.]]\n",
      "[[59.  1.  1.  1.]]\n",
      "[[7. 1. 1. 0.]]\n",
      "[[144.   1.   0.   1.]]\n",
      "[[196. 109.   1.   0.]]\n",
      "[[152.   1.   1.   1.]]\n",
      "[[188.   1.   1.   1.]]\n",
      "[[54.  1.  0.  1.]]\n",
      "[[98.  1.  1.  1.]]\n",
      "[[237.   1.   1.   0.]]\n",
      "[[34.  1.  0.  0.]]\n",
      "[[275.   1.   1.   0.]]\n",
      "[[300.  79.   1.   1.]]\n",
      "[[209.   1.   1.   0.]]\n",
      "[[270.   1.   0.   0.]]\n",
      "[[247.   1.   1.   1.]]\n",
      "[[93.  1.  0.  1.]]\n",
      "[[196.   1.   0.   1.]]\n",
      "[[20.  1.  0.  1.]]\n",
      "[[265. 109.   1.   0.]]\n",
      "[[138.   1.   0.   0.]]\n",
      "[[165.   1.   1.   1.]]\n",
      "[[73.  1.  0.  1.]]\n",
      "[[137.   1.   1.   1.]]\n",
      "[[136.  82.   1.   1.]]\n",
      "[[47.  1.  1.  1.]]\n",
      "[[202.   1.   0.   0.]]\n",
      "[[270.   1.   1.   1.]]\n",
      "[[87.  1.  1.  1.]]\n",
      "[[243.   1.   1.   0.]]\n",
      "[[208.   1.   0.   1.]]\n",
      "[[118.   1.   1.   0.]]\n",
      "[[86.  1.  0.  0.]]\n",
      "[[188.   1.   0.   0.]]\n",
      "[[232.   1.   0.   0.]]\n",
      "[[107.   1.   1.   0.]]\n",
      "[[4. 1. 0. 1.]]\n",
      "[[132.   1.   1.   0.]]\n",
      "[[75. 87.  0.  0.]]\n",
      "[[15.  1.  0.  0.]]\n",
      "[[70.  1.  1.  0.]]\n",
      "[[158.   1.   1.   0.]]\n",
      "[[29. 91.  1.  1.]]\n",
      "[[172.   1.   1.   1.]]\n",
      "[[37.  1.  1.  1.]]\n",
      "[[221.   1.   0.   1.]]\n",
      "[[58.  1.  0.  0.]]\n",
      "[[63.  1.  1.  0.]]\n",
      "[[180.   1.   1.   1.]]\n",
      "[[253.   1.   0.   1.]]\n",
      "[[280.   1.   0.   0.]]\n",
      "[[47.  1.  0.  0.]]\n",
      "[[184.   1.   1.   0.]]\n",
      "[[97.  1.  0.  0.]]\n",
      "[[226.   1.   1.   1.]]\n",
      "[[25.  1.  1.  1.]]\n",
      "[[128.   1.   0.   1.]]\n",
      "[[82.  1.  1.  0.]]\n",
      "[[163.   1.   0.   0.]]\n",
      "[[115.   1.   1.   1.]]\n",
      "[[280.   1.   1.   1.]]\n",
      "[[29.  1.  0.  1.]]\n",
      "[[199.   1.   1.   0.]]\n",
      "[[265.   1.   0.   1.]]\n",
      "[[229.   1.   1.   0.]]\n",
      "[[14.  1.  1.  1.]]\n",
      "[[126.   1.   0.   0.]]\n",
      "[[142.   1.   0.   0.]]\n",
      "[[62.  1.  0.  1.]]\n",
      "[[245.   1.   0.   1.]]\n",
      "[[78.  1.  1.  1.]]\n",
      "[[261.   1.   1.   1.]]\n",
      "[[9. 1. 0. 0.]]\n",
      "[[261.   1.   0.   0.]]\n",
      "[[206.   1.   0.   0.]]\n",
      "[[170.   1.   0.   0.]]\n",
      "[[199.   1.   1.   1.]]\n",
      "[[190.  53.   0.   0.]]\n",
      "[[177.   1.   0.   1.]]\n",
      "[[285.   1.   1.   0.]]\n",
      "[[180.  11.   1.   1.]]\n",
      "[[136.  54.   0.   0.]]\n",
      "[[104.   1.   1.   1.]]\n",
      "[[78.  1.  0.  0.]]\n",
      "[[44.  1.  1.  0.]]\n",
      "[[112.   1.   1.   0.]]\n",
      "[[235.   1.   0.   1.]]\n",
      "[[284.   1.   0.   1.]]\n",
      "[[254.   1.   1.   1.]]\n",
      "[[55.  1.  1.  0.]]\n",
      "[[229.   1.   0.   1.]]\n",
      "[[162.   1.   1.   0.]]\n",
      "[[240.   1.   0.   0.]]\n",
      "[[146.   1.   0.   0.]]\n",
      "[[138. 107.   1.   0.]]\n",
      "[[180.   1.   0.   0.]]\n",
      "[[107.   1.   0.   1.]]\n",
      "[[89.  1.  0.  1.]]\n",
      "[[154.   1.   0.   1.]]\n",
      "[[174.   1.   0.   0.]]\n",
      "[[116.   1.   0.   1.]]\n",
      "[[125.   1.   0.   1.]]\n",
      "[[29.  1.  1.  0.]]\n",
      "[[284.  68.   1.   0.]]\n",
      "[[239.   1.   1.   1.]]\n",
      "[[148.   1.   1.   1.]]\n",
      "[[143.   1.   1.   1.]]\n",
      "[[288.   1.   0.   0.]]\n",
      "[[127.   1.   1.   1.]]\n",
      "[[37.  1.  0.  1.]]\n",
      "[[218.   1.   1.   1.]]\n",
      "[[2. 1. 1. 0.]]\n",
      "[[110.   1.   0.   0.]]\n",
      "[[22.  1.  1.  0.]]\n",
      "[[68.  1.  0.  0.]]\n",
      "[[4. 1. 1. 1.]]\n",
      "[[15.  1.  0.  1.]]\n",
      "[[84. 50.  0.  1.]]\n",
      "[[242.   1.   1.   1.]]\n",
      "[[290.   2.   1.   0.]]\n",
      "[[168.   1.   0.   1.]]\n",
      "[[11.  1.  1.  0.]]\n",
      "[[130.   1.   0.   0.]]\n",
      "[[ 82. 109.   1.   0.]]\n",
      "[[211.   1.   0.   0.]]\n",
      "[[51.  1.  0.  1.]]\n",
      "[[73.  1.  1.  1.]]\n",
      "[[21.  1.  1.  1.]]\n",
      "[[8. 1. 0. 1.]]\n",
      "[[191.   1.   0.   1.]]\n",
      "[[96.  1.  1.  1.]]\n",
      "[[101.   1.   1.   0.]]\n",
      "[[270.   1.   1.   0.]]\n",
      "[[266.   1.   1.   1.]]\n",
      "[[257.   1.   1.   1.]]\n",
      "[[233.  87.   1.   0.]]\n",
      "[[203.   1.   1.   0.]]\n",
      "[[223.   1.   1.   1.]]\n",
      "[[250.   1.   0.   1.]]\n",
      "[[42.  1.  0.  0.]]\n",
      "[[156.   1.   1.   1.]]\n",
      "[[214.   1.   1.   0.]]\n",
      "[[89.  1.  1.  0.]]\n",
      "[[175.   1.   1.   1.]]\n",
      "[[139.   1.   1.   0.]]\n",
      "[[153.   1.   1.   0.]]\n",
      "[[140.   1.   0.   1.]]\n",
      "[[73.  1.  0.  0.]]\n",
      "[[197.   1.   0.   0.]]\n",
      "[[196.   1.   1.   1.]]\n",
      "[[171.   1.   1.   0.]]\n",
      "[[ 24. 109.   1.   0.]]\n",
      "[[258.   1.   1.   0.]]\n",
      "[[42.  1.  1.  1.]]\n",
      "[[196.  87.   1.   0.]]\n",
      "[[45.  1.  0.  1.]]\n",
      "[[119.   1.   1.   1.]]\n",
      "[[32.  1.  0.  0.]]\n",
      "[[250.   1.   1.   0.]]\n",
      "[[20.  3.  0.  1.]]\n",
      "[[ 1. 48.  1.  0.]]\n",
      "[[78. 14.  1.  0.]]\n",
      "[[90.  1.  1.  1.]]\n",
      "[[161.   1.   1.   1.]]\n",
      "[[157.   1.   0.   0.]]\n",
      "[[280.  92.   0.   0.]]\n",
      "[[101.   1.   0.   1.]]\n",
      "[[217.   1.   0.   1.]]\n",
      "[[92.  1.  0.  0.]]\n",
      "[[ 6. 90.  0.  0.]]\n",
      "[[225.   1.   1.   0.]]\n",
      "[[54.  1.  1.  1.]]\n",
      "[[25.  1.  0.  1.]]\n",
      "[[186.   1.   0.   1.]]\n",
      "[[83.  1.  1.  1.]]\n",
      "[[277.   1.   1.   1.]]\n",
      "[[53.  1.  0.  0.]]\n",
      "[[133.   1.   1.   1.]]\n",
      "[[118.   1.   0.   0.]]\n",
      "[[273.   1.   0.   0.]]\n",
      "[[279.   1.   1.   0.]]\n",
      "[[58.  1.  0.  1.]]\n",
      "[[66.  1.  0.  1.]]\n",
      "[[254.   1.   0.   0.]]\n",
      "[[184.   1.   1.   1.]]\n",
      "[[219.  49.   0.   0.]]\n",
      "[[6. 1. 0. 0.]]\n",
      "[[286.   1.   1.   1.]]\n",
      "[[112.   1.   0.   1.]]\n",
      "[[64.  1.  1.  1.]]\n",
      "[[21.  1.  0.  0.]]\n",
      "[[33.  1.  1.  0.]]\n",
      "[[204.   1.   0.   1.]]\n",
      "[[235.   1.   1.   1.]]\n",
      "[[283.   1.   1.   0.]]\n",
      "[[110.  45.   1.   1.]]\n",
      "[[236.   1.   0.   0.]]\n",
      "[[154.  96.   0.   0.]]\n",
      "[[288.   4.   1.   1.]]\n",
      "[[300.  73.   0.   0.]]\n",
      "[[229.  16.   0.   0.]]\n",
      "[[246.  58.   0.   1.]]\n",
      "[[113.   9.   1.   1.]]\n",
      "[[300.   4.   0.   0.]]\n",
      "[[51. 82.  0.  0.]]\n",
      "[[39. 42.  0.  1.]]\n",
      "[[154.  62.   1.   0.]]\n",
      "[[115.  89.   0.   0.]]\n",
      "[[211.   6.   0.   1.]]\n",
      "[[241.  35.   0.   0.]]\n",
      "[[198.  37.   1.   0.]]\n",
      "[[63. 39.  0.  0.]]\n",
      "[[266.  56.   0.   0.]]\n",
      "[[177.  94.   1.   0.]]\n",
      "[[289.   1.   1.   0.]]\n",
      "[[158.   8.   1.   0.]]\n",
      "[[247. 109.   1.   1.]]\n",
      "[[76. 68.  1.  0.]]\n",
      "[[94. 91.  1.  1.]]\n",
      "[[243.   7.   1.   0.]]\n",
      "[[214.  96.   1.   1.]]\n",
      "[[122.  66.   0.   1.]]\n",
      "[[222.  33.   1.   1.]]\n",
      "[[173.  52.   1.   1.]]\n",
      "[[131.  36.   0.   1.]]\n",
      "[[19. 41.  0.  1.]]\n",
      "[[190.  70.   1.   0.]]\n",
      "[[300.  36.   0.   1.]]\n",
      "[[280. 109.   1.   1.]]\n",
      "[[235.  68.   0.   0.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[62.  5.  0.  0.]]\n",
      "[[92.  7.  1.  1.]]\n",
      "[[84. 33.  1.  0.]]\n",
      "[[39. 25.  1.  0.]]\n",
      "[[18. 76.  0.  1.]]\n",
      "[[194.   1.   0.   0.]]\n",
      "[[123. 109.   1.   1.]]\n",
      "[[181.  34.   0.   1.]]\n",
      "[[122.   1.   1.   0.]]\n",
      "[[76.  1.  0.  1.]]\n",
      "[[290.   2.   0.   0.]]\n",
      "[[96.  1.  1.  0.]]\n",
      "[[39. 73.  1.  1.]]\n",
      "[[70.  1.  0.  1.]]\n",
      "[[282.  49.   0.   1.]]\n",
      "[[1. 6. 1. 0.]]\n",
      "[[278.   1.   0.   1.]]\n",
      "[[1. 4. 0. 0.]]\n",
      "[[ 1. 12.  1.  0.]]\n",
      "[[262.  38.   1.   1.]]\n",
      "[[291.   2.   1.   0.]]\n",
      "[[ 1. 34.  1.  1.]]\n",
      "[[40.  7.  1.  1.]]\n",
      "[[154.  79.   1.   1.]]\n",
      "[[249.  92.   0.   0.]]\n",
      "[[180. 109.   1.   0.]]\n",
      "[[ 68. 109.   0.   1.]]\n",
      "[[288.   1.   0.   1.]]\n",
      "[[ 38. 109.   0.   0.]]\n",
      "[[ 9. 61.  1.  1.]]\n",
      "[[212. 109.   1.   1.]]\n",
      "[[300.  94.   1.   0.]]\n",
      "[[52. 30.  0.  0.]]\n",
      "[[205.  57.   1.   1.]]\n",
      "[[211.   1.   1.   1.]]\n",
      "[[98. 56.  0.  0.]]\n",
      "[[250.  73.   1.   1.]]\n",
      "[[117.  30.   1.   1.]]\n",
      "[[277.   1.   0.   0.]]\n",
      "[[129.   5.   1.   1.]]\n",
      "[[146.  31.   0.   0.]]\n",
      "[[193.   5.   0.   0.]]\n",
      "[[6. 1. 1. 1.]]\n",
      "[[267.   1.   0.   0.]]\n",
      "[[135.   1.   1.   0.]]\n",
      "[[234.  49.   1.   1.]]\n",
      "[[60. 94.  1.  1.]]\n",
      "[[37.  1.  0.  0.]]\n",
      "[[213.   1.   0.   1.]]\n",
      "[[88. 76.  0.  1.]]\n",
      "[[163.   1.   0.   1.]]\n",
      "[[63. 74.  0.  1.]]\n",
      "[[97. 39.  1.  0.]]\n",
      "[[284.   1.   1.   1.]]\n",
      "[[224.   1.   0.   1.]]\n",
      "[[169.   1.   1.   1.]]\n",
      "[[69. 53.  1.  0.]]\n",
      "[[222.   1.   0.   0.]]\n",
      "[[1. 7. 1. 1.]]\n",
      "[[19. 27.  1.  0.]]\n",
      "[[279.  34.   0.   0.]]\n",
      "[[270.  71.   0.   1.]]\n",
      "[[139.  68.   0.   0.]]\n",
      "[[ 95. 109.   1.   0.]]\n",
      "[[122.  51.   1.   0.]]\n",
      "[[151. 109.   1.   0.]]\n",
      "[[168.  26.   0.   0.]]\n",
      "[[130.  95.   0.   0.]]\n",
      "[[70. 27.  0.  1.]]\n",
      "[[44. 94.  1.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[1. 1. 1. 0.]]\n",
      "[[81.  1.  1.  0.]]\n",
      "[[52.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[70.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[147.  46.   1.   1.]]\n",
      "[[41. 57.  0.  1.]]\n",
      "[[149.   1.   1.   0.]]\n",
      "[[256.   1.   0.   1.]]\n",
      "[[209.  31.   0.   1.]]\n",
      "[[207.   1.   1.   1.]]\n",
      "[[187.   1.   1.   0.]]\n",
      "[[160.   1.   0.   0.]]\n",
      "[[123.   1.   0.   0.]]\n",
      "[[251.  46.   0.   0.]]\n",
      "[[136.   1.   0.   1.]]\n",
      "[[265.  95.   1.   1.]]\n",
      "[[18.  1.  0.  0.]]\n",
      "[[220.  83.   0.   0.]]\n",
      "[[28.  1.  1.  1.]]\n",
      "[[234.   1.   1.   0.]]\n",
      "[[223.  62.   1.   1.]]\n",
      "[[182.  81.   0.   1.]]\n",
      "[[130.   1.   1.   1.]]\n",
      "[[ 14. 101.   0.   1.]]\n",
      "[[185.   1.   0.   0.]]\n",
      "[[48.  1.  0.  1.]]\n",
      "[[165.  87.   1.   0.]]\n",
      "[[149.   1.   0.   1.]]\n",
      "[[285.  81.   0.   1.]]\n",
      "[[181.   1.   1.   0.]]\n",
      "[[190.   1.   1.   0.]]\n",
      "[[100.   1.   0.   0.]]\n",
      "[[121.  78.   1.   1.]]\n",
      "[[273.   1.   1.   1.]]\n",
      "[[85.  1.  0.  1.]]\n",
      "[[193.   1.   1.   1.]]\n",
      "[[174.   1.   1.   0.]]\n",
      "[[178.   1.   1.   1.]]\n",
      "[[201.   1.   1.   1.]]\n",
      "[[39.  1.  0.  1.]]\n",
      "[[246.   1.   0.   0.]]\n",
      "[[101.   1.   1.   1.]]\n",
      "[[10.  1.  1.  1.]]\n",
      "[[228.   1.   0.   0.]]\n",
      "[[264.   1.   0.   0.]]\n",
      "[[257.   1.   0.   0.]]\n",
      "[[23.  1.  0.  1.]]\n",
      "[[131.   1.   0.   1.]]\n",
      "[[61.  1.  1.  1.]]\n",
      "[[123.   1.   1.   1.]]\n",
      "[[66.  1.  1.  0.]]\n",
      "[[112.   1.   1.   1.]]\n",
      "[[202.  76.   0.   1.]]\n",
      "[[243.   1.   0.   0.]]\n",
      "[[218.   1.   0.   0.]]\n",
      "[[285.   1.   0.   0.]]\n",
      "[[134.   1.   0.   0.]]\n",
      "[[18.  1.  1.  1.]]\n",
      "[[181.   1.   0.   1.]]\n",
      "[[182.   1.   0.   0.]]\n",
      "[[33.  1.  0.  1.]]\n",
      "[[262.   1.   1.   0.]]\n",
      "[[216.   1.   1.   0.]]\n",
      "[[196.   1.   1.   0.]]\n",
      "[[52. 46.  1.  1.]]\n",
      "[[85.  1.  1.  0.]]\n",
      "[[179.  63.   0.   0.]]\n",
      "[[48.  1.  1.  0.]]\n",
      "[[200.   1.   0.   1.]]\n",
      "[[128.   1.   1.   0.]]\n",
      "[[111.  60.   1.   1.]]\n",
      "[[ 1. 63.  0.  0.]]\n",
      "[[96.  1.  0.  1.]]\n",
      "[[3. 1. 0. 0.]]\n",
      "[[252.   1.   1.   0.]]\n",
      "[[61.  1.  0.  0.]]\n",
      "[[237.  99.   0.   1.]]\n",
      "[[262.   1.   0.   1.]]\n",
      "[[165.   1.   1.   0.]]\n",
      "[[231.   1.   1.   0.]]\n",
      "[[83.  1.  0.  0.]]\n",
      "[[264.   1.   1.   1.]]\n",
      "[[246.   1.   1.   0.]]\n",
      "[[19.  1.  1.  0.]]\n",
      "[[28.  1.  0.  0.]]\n",
      "[[251.   1.   1.   1.]]\n",
      "[[28. 49.  1.  1.]]\n",
      "[[104.   1.   0.   1.]]\n",
      "[[106.   1.   0.   0.]]\n",
      "[[29. 34.  1.  0.]]\n",
      "[[44.  1.  0.  0.]]\n",
      "[[272.  44.   0.   0.]]\n",
      "[[81. 97.  1.  1.]]\n",
      "[[251.   1.   0.   0.]]\n",
      "[[190.  98.   0.   1.]]\n",
      "[[26.  1.  1.  0.]]\n",
      "[[143.   1.   1.   0.]]\n",
      "[[190.  26.   1.   0.]]\n",
      "[[153.   1.   0.   0.]]\n",
      "[[166.   1.   0.   0.]]\n",
      "[[94.  1.  0.  0.]]\n",
      "[[238.   1.   0.   1.]]\n",
      "[[59.  1.  1.  0.]]\n",
      "[[206.   1.   1.   0.]]\n",
      "[[247.   1.   0.   1.]]\n",
      "[[75.  1.  1.  1.]]\n",
      "[[88. 63.  0.  1.]]\n",
      "[[191.   1.   0.   0.]]\n",
      "[[269.   1.   0.   1.]]\n",
      "[[140.   1.   1.   1.]]\n",
      "[[44.  1.  1.  1.]]\n",
      "[[105.  98.   1.   1.]]\n",
      "[[10. 33.  1.  0.]]\n",
      "[[212.   1.   1.   0.]]\n",
      "[[79.  1.  0.  1.]]\n",
      "[[108.   1.   0.   0.]]\n",
      "[[151.   1.   0.   1.]]\n",
      "[[36.  1.  1.  0.]]\n",
      "[[254.  27.   0.   0.]]\n",
      "[[ 1. 98.  1.  0.]]\n",
      "[[3. 6. 1. 0.]]\n",
      "[[227.   4.   1.   1.]]\n",
      "[[234.  25.   0.   1.]]\n",
      "[[104.  83.   0.   0.]]\n",
      "[[116.   1.   1.   0.]]\n",
      "[[143.  93.   1.   1.]]\n",
      "[[230.   1.   0.   0.]]\n",
      "[[272.   1.   0.   1.]]\n",
      "[[258.  64.   1.   0.]]\n",
      "[[290.  41.   0.   0.]]\n",
      "[[158.   1.   1.   1.]]\n",
      "[[50.  1.  1.  1.]]\n",
      "[[300.  25.   1.   0.]]\n",
      "[[161.  53.   0.   0.]]\n",
      "[[241.   1.   1.   0.]]\n",
      "[[171.   4.   0.   0.]]\n",
      "[[146.   4.   0.   1.]]\n",
      "[[134.  24.   1.   0.]]\n",
      "[[290. 101.   0.   0.]]\n",
      "[[268.   1.   1.   0.]]\n",
      "[[203.  98.   0.   0.]]\n",
      "[[75. 40.  1.  1.]]\n",
      "[[232.   1.   0.   1.]]\n",
      "[[155.   1.   1.   0.]]\n",
      "[[145.   1.   1.   1.]]\n",
      "[[13.  1.  0.  0.]]\n",
      "[[14.  1.  1.  0.]]\n",
      "[[30. 80.  0.  0.]]\n",
      "[[110.   1.   0.   1.]]\n",
      "[[219.  23.   1.   0.]]\n",
      "[[16.  1.  1.  1.]]\n",
      "[[174.   1.   0.   1.]]\n",
      "[[155.  24.   1.   1.]]\n",
      "[[147.   1.   0.   1.]]\n",
      "[[177.   1.   0.   0.]]\n",
      "[[207.  45.   0.   0.]]\n",
      "[[16. 52.  0.  0.]]\n",
      "[[151.   1.   1.   0.]]\n",
      "[[204.   1.   0.   0.]]\n",
      "[[225.  97.   0.   0.]]\n",
      "[[ 11. 109.   1.   0.]]\n",
      "[[222.   1.   1.   0.]]\n",
      "[[99.  1.  0.  1.]]\n",
      "[[168.   1.   0.   0.]]\n",
      "[[73.  1.  1.  0.]]\n",
      "[[193.   1.   0.   1.]]\n",
      "[[191.   1.   1.   1.]]\n",
      "[[57.  1.  1.  1.]]\n",
      "[[35.  1.  1.  1.]]\n",
      "[[206.   1.   0.   1.]]\n",
      "[[186.   1.   1.   1.]]\n",
      "[[155.   1.   0.   0.]]\n",
      "[[291.  60.   0.   1.]]\n",
      "[[104.   1.   1.   0.]]\n",
      "[[189.   1.   0.   1.]]\n",
      "[[18. 88.  1.  0.]]\n",
      "[[89.  1.  0.  0.]]\n",
      "[[160.   1.   1.   0.]]\n",
      "[[171.  40.   0.   0.]]\n",
      "[[220.   1.   1.   1.]]\n",
      "[[29. 18.  1.  0.]]\n",
      "[[239.   1.   1.   0.]]\n",
      "[[17.  1.  0.  1.]]\n",
      "[[51. 70.  0.  1.]]\n",
      "[[107.  34.   0.   1.]]\n",
      "[[230.   1.   1.   1.]]\n",
      "[[165.   1.   0.   1.]]\n",
      "[[41.  1.  1.  0.]]\n",
      "[[71.  1.  0.  0.]]\n",
      "[[245.   1.   1.   1.]]\n",
      "[[120.   1.   0.   0.]]\n",
      "[[56.  1.  0.  0.]]\n",
      "[[274.  82.   1.   0.]]\n",
      "[[87.  1.  0.  1.]]\n",
      "[[267.  28.   0.   0.]]\n",
      "[[93.  1.  1.  1.]]\n",
      "[[113.   1.   0.   0.]]\n",
      "[[142.   1.   0.   1.]]\n",
      "[[227.  75.   0.   1.]]\n",
      "[[255. 102.   1.   1.]]\n",
      "[[210.   1.   0.   1.]]\n",
      "[[86. 23.  0.  1.]]\n",
      "[[114.   1.   1.   0.]]\n",
      "[[161.   1.   0.   1.]]\n",
      "[[11.  1.  0.  0.]]\n",
      "[[242.  80.   0.   0.]]\n",
      "[[80.  1.  1.  1.]]\n",
      "[[1. 1. 0. 1.]]\n",
      "[[282.   1.   0.   0.]]\n",
      "[[167.  99.   0.   1.]]\n",
      "[[81.  1.  0.  0.]]\n",
      "[[166.  63.   1.   1.]]\n",
      "[[2. 1. 0. 1.]]\n",
      "[[209.   1.   1.   1.]]\n",
      "[[118. 100.   1.   0.]]\n",
      "[[28.  4.  1.  0.]]\n",
      "[[ 9. 71.  1.  0.]]\n",
      "[[201.   1.   1.   0.]]\n",
      "[[120.  39.   0.   0.]]\n",
      "[[ 91. 100.   0.   0.]]\n",
      "[[30.  1.  0.  0.]]\n",
      "[[66. 63.  0.  1.]]\n",
      "[[144.   1.   0.   0.]]\n",
      "[[70. 99.  0.  0.]]\n",
      "[[260.   1.   1.   0.]]\n",
      "[[292.  88.   1.   0.]]\n",
      "[[121.   1.   1.   1.]]\n",
      "[[79.  1.  1.  0.]]\n",
      "[[40.  1.  0.  0.]]\n",
      "[[157.   1.   0.   1.]]\n",
      "[[202.  65.   0.   0.]]\n",
      "[[ 31. 100.   0.   0.]]\n",
      "[[ 1. 32.  1.  0.]]\n",
      "[[130.   1.   1.   0.]]\n",
      "[[135.   1.   1.   1.]]\n",
      "[[242.   1.   0.   1.]]\n",
      "[[179.   1.   0.   1.]]\n",
      "[[167.   1.   1.   1.]]\n",
      "[[60.  1.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "## define the domain of the considered parameters\n",
    "n_estimators = tuple(np.arange(1,301,1, dtype= np.int))\n",
    "# print(n_estimators)\n",
    "max_depth = tuple(np.arange(1,110,1, dtype= np.int))\n",
    "# max_features = ('log2', 'sqrt', None)\n",
    "max_features = (0, 1)\n",
    "# criterion = ('gini', 'entropy')\n",
    "criterion = (0, 1)\n",
    "\n",
    "\n",
    "# define the dictionary for GPyOpt\n",
    "domain = [{'n_estimators': 'var_1',  'type': 'discrete',     'domain': n_estimators},\n",
    "          {'max_depth': 'var_2',     'type': 'discrete',     'domain': max_depth},\n",
    "          {'max_features': 'var_3',  'type': 'categorical',  'domain': max_features},\n",
    "          {'criterion': 'var_4',     'type': 'categorical',  'domain': criterion}]\n",
    "\n",
    "\n",
    "## we have to define the function we want to maximize --> validation accuracy, \n",
    "## note it should take a 2D ndarray but it is ok that it assumes only one point\n",
    "## in this setting\n",
    "def objective_function(x): \n",
    "    print(x)\n",
    "    # we have to handle the categorical variables that is convert 0/1 to labels\n",
    "    # log2/sqrt and gini/entropy\n",
    "    \n",
    "    param = x[0]\n",
    "    \n",
    "    if param[2] == 0:\n",
    "        var_3 = \"log2\"\n",
    "    else:\n",
    "        var_3 = \"sqrt\"\n",
    "    \n",
    "    if param[3] == 0:\n",
    "        var_4 = \"gini\"\n",
    "    else:\n",
    "        var_4 = \"entropy\"\n",
    "        \n",
    "        \n",
    "#fit the model\n",
    "    model = RandomForestClassifier(n_estimators = int(param[0]), criterion = var_4, max_depth = int(param[1]), max_features = var_3)\n",
    "    model.fit(Xcattrain, ytrain)\n",
    "    forestPreds = model.predict(Xcattest)\n",
    "    accuracy = len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "opt = GPyOpt.methods.BayesianOptimization(f = objective_function,   # function to optimize\n",
    "                                              domain = domain,         # box-constrains of the problem\n",
    "                                              acquisition_type = \"EI\",      # Select acquisition function MPI, EI, LCB\n",
    "                                             )\n",
    "opt.acquisition.exploration_weight=.1\n",
    "\n",
    "opt.run_optimization(max_iter = 100) \n",
    "\n",
    "\n",
    "x_best = opt.X[np.argmin(opt.Y)]\n",
    "print(\"The best parameters obtained: n_estimators=\" + str(x_best[0]) + \", max_depth=\" + str(x_best[1]) + \", max_features=\" + str(\n",
    "    x_best[2])  + \", criterion=\" + str(\n",
    "    x_best[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
