{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "import GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, norm_type):\n",
    "    if norm_type == \"minmax\":\n",
    "        for i in range(data.size()[1]):\n",
    "            data[:,i] = (data[:,i]-data[:,i].min()) / (data[:,i].max()-data[:,i].min())\n",
    "        return data\n",
    "    elif norm_type == \"zscore\":\n",
    "        for i in range(data.size()[1]):\n",
    "            data[:,i] = (data[:,i]-data[:,i].mean()) / (data[:,i].std())\n",
    "        return data\n",
    "    elif norm_type == None:\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(\"Please choose a correct normalization type\")\n",
    "#Xnumtrain = torch.tensor(np.vstack([(Xnumtrain[:,i]-Xnumtrain[:,i].min()) / (Xnumtrain[:,i].max()-Xnumtrain[:,i].min()) for i in range(Xnumtrain.size()[1]) if \"Tue elsker det her\"])).view(-1,2)\n",
    "\n",
    "def dataprep(data, norm_type = \"zscore\"):\n",
    "    for category in categoricals:\n",
    "        data[category] = data[category].astype(\"category\")\n",
    "\n",
    "    catdata = []\n",
    "    for i in range(len(categoricals)):\n",
    "        catdata.append(data[categoricals[i]].cat.codes.values)\n",
    "    catdata = torch.tensor(catdata, dtype = torch.int64).T\n",
    "    \n",
    "    \n",
    "    numdata = np.stack([data[col].values for col in numericals], 1)\n",
    "    numdata = torch.tensor(numdata, dtype=torch.float)\n",
    "\n",
    "    normalize(numdata, norm_type)\n",
    "    return catdata, numdata\n",
    "\n",
    "def permutation(data, cat):\n",
    "    perm_data = data.copy()\n",
    "    rand_num = np.random.randint(0 , len(data[cat]) , len(data[cat]))\n",
    "    perm_data[cat][np.arange(len(data[cat]))] = data[cat][rand_num] \n",
    "    df = perm_data\n",
    "    return df\n",
    "\n",
    "def reset_data():\n",
    "    return pd.read_csv(\"https://raw.githubusercontent.com/oskarwiese/fagprojekt/master/compas_propublica/compas-scores-two-years.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/oskarwiese/fagprojekt/master/compas_propublica/compas-scores-two-years.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "#print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.20it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def is_plot():\n",
    "    \n",
    "    path = r\"C:\\Users\\ejer\\Desktop\\Fagprojekt\\fagprojekt\\report\\imgs\"\n",
    "    \n",
    "    pred_plot_race = sb.countplot(x = \"score_text\", hue = \"race\", data = data)\n",
    "    pred_plot_race.set_title(\"Predicted risk of recidivism according to race\")\n",
    "    pred_plot_race.figure.savefig(path + \"\\\\predicted_recid_race.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    pred_plot_sex = sb.countplot(x = \"score_text\", hue = \"sex\", data = data)\n",
    "    pred_plot_sex.set_title(\"Predicted risk of recidivism according to sex\")\n",
    "    pred_plot_sex.figure.savefig(path + \"\\\\predicted_recid_sex.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    true_plot = sb.countplot(x = \"is_recid\", hue = \"race\", data = data)\n",
    "    true_plot.set_title(\"True recidivism scores\")\n",
    "    true_plot.figure.savefig(path + \"\\\\true_recid.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    df = data[(data[\"race\"] == \"African-American\") | (data[\"race\"] == \"Caucasian\")]\n",
    "    df = df[df[\"priors_count\"] < 25]\n",
    "    \n",
    "    priors_plot = sb.countplot(x = \"priors_count\", hue = \"race\", data = df)\n",
    "    priors_plot.set_title(\"Number of previous felonies according to race\")\n",
    "    priors_plot.figure.savefig(path + \"\\\\proirs.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    c_charge_degree = sb.countplot(x = \"c_charge_degree\", hue = \"race\", data = df)\n",
    "    c_charge_degree.set_title(\"Severity of the crimes for each race\")\n",
    "    c_charge_degree.figure.savefig(path + \"\\\\c_charge_degree.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    c_charge_degree = sb.countplot(x = \"score_text\", hue = \"c_charge_degree\", data = df)\n",
    "    c_charge_degree.set_title(\"Relation between crime severity and predicted recidivism\")\n",
    "    c_charge_degree.figure.savefig(path + \"\\\\charge_degree_score.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#is_plot()\n",
    "\n",
    "palette = {\"Caucasian\": \"C0\", \"African-American\": \"C1\", \"Hispanic\": \"C2\", \"Other\": \"C3\" , \"Asian\": \"C4\" ,\"Native American\": \"C5\"}\n",
    "for i in tqdm(range(3)):\n",
    "    #TODO: der skal indsættes punkter for den rigtige data1fordelling. \n",
    "    data = permutation(data, \"race\")\n",
    "    path = r\"C:\\Users\\ejer\\Desktop\\Fagprojekt\\fagprojekt\\report\\imgs\"\n",
    "    pred_plot_race = sb.countplot(x = \"score_text\" ,hue = \"race\", data = data, palette = palette ,alpha=.06,  hue_order = [ \"African-American\",\"Caucasian\", \"Hispanic\", \"Other\" , \"Asian\",\"Native American\"])\n",
    "    pred_plot_race.set_title(\"Predicted risk of recidivism according to race\")\n",
    "    pred_plot_race.set_xticklabels(pred_plot_race.get_xticklabels(), horizontalalignment ='right')\n",
    "    pred_plot_race.get_legend().remove()\n",
    "    data = reset_data()\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"c_charge_degree\", \"race\", \"age_cat\", \"sex\"] # \"r_charge_degree\"    \"two_year_recid\"    \"is_recid\", \"is_violent_recid\"\n",
    "numericals = [\"age\", \"priors_count\", \"juv_fel_count\", \"juv_misd_count\"] # \"days_b_screening_arrest\"\n",
    "outputs = [\"score_text\"]\n",
    "data = data[categoricals + numericals + outputs + [\"two_year_recid\"]]\n",
    "\n",
    "# Making the output binary\n",
    "data[outputs] = data[outputs].replace('Low',0)\n",
    "data[outputs] = data[outputs].replace('Medium',1)\n",
    "data[outputs] = data[outputs].replace('High',1)\n",
    "data[outputs] = data[outputs].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcat, Xnum = dataprep(data, norm_type = \"zscore\")\n",
    "\n",
    "# Converting the output to tensor\n",
    "y = torch.tensor(data[outputs].values).flatten()\n",
    "\n",
    "# Calculation of embedding sizes for the categorical values in the format (unique categorical values, embedding size (dimension of encoding))\n",
    "categorical_column_sizes = [len(data[column].cat.categories) for column in categoricals]\n",
    "categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]\n",
    "\n",
    "# Train-test split\n",
    "totalnumber = len(Xnum)\n",
    "testnumber = int(totalnumber * 0.2)\n",
    "\n",
    "Xcattrain = Xcat[:totalnumber - testnumber]\n",
    "Xcattest = Xcat[totalnumber - testnumber:totalnumber]\n",
    "Xnumtrain = Xnum[:totalnumber - testnumber]\n",
    "Xnumtest = Xnum[totalnumber - testnumber:totalnumber]\n",
    "ytrain = y[:totalnumber - testnumber]\n",
    "ytest = y[totalnumber - testnumber:totalnumber]\n",
    "\n",
    "\n",
    "# Make sure that we dont validate on training data to compare if the algorithm is biased\n",
    "\n",
    "\n",
    "normalize(Xnumtrain, \"zscore\");\n",
    "normalize(Xnumtest, \"zscore\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols + num_numerical_cols\n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = torch.cat([x, x_numerical], 1)\n",
    "        x = self.layers(x)\n",
    "        return nn.functional.softmax(x, dim = -1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(2, 1)\n",
      "    (1): Embedding(6, 3)\n",
      "    (2): Embedding(3, 2)\n",
      "    (3): Embedding(2, 1)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (batch_norm_num): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=16, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "    (16): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.5, inplace=False)\n",
      "    (20): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define and show the model\n",
    "model = Model(categorical_embedding_sizes, 4, 2, [16,32,64,128,64], p=0.5)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimization\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 0.71161085\n",
      "epoch:  26 loss: 0.66686159\n",
      "epoch:  51 loss: 0.63189709\n"
     ]
    }
   ],
   "source": [
    "epochs = 600\n",
    "aggregated_losses = []\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(Xcattrain, Xnumtrain)\n",
    "    single_loss = loss_function(y_pred, ytrain)\n",
    "    aggregated_losses.append(single_loss)\n",
    "    \n",
    "    if i % 25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "    \n",
    "\n",
    "\n",
    "# Plot the loss over epocs\n",
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val = model(Xcattest, Xnumtest)\n",
    "    loss = loss_function(y_val, ytest)\n",
    "\n",
    "\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "\n",
    "print(confusion_matrix(ytest,y_val))\n",
    "print(classification_report(ytest,y_val))\n",
    "neural_acc = accuracy_score(ytest, y_val)\n",
    "print(neural_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep black and white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[totalnumber - testnumber:totalnumber]\n",
    "black_data = df[df[\"race\"]==\"African-American\"]\n",
    "white_data = df[df[\"race\"]==\"Caucasian\"]\n",
    "\n",
    "Xcat_white, Xnum_white = dataprep(white_data, norm_type = \"zscore\")\n",
    "Xcat_black, Xnum_black = dataprep(black_data, norm_type = \"zscore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few more plots of blacks and whites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n",
      "450\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307\n",
      "168\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots of the distribution of black and whites in each category of scoretext\n",
    "sb.countplot(x = \"score_text\", data = black_data)\n",
    "print(len(black_data[black_data[\"score_text\"] == 0]))\n",
    "print(len(black_data[black_data[\"score_text\"] == 1]))\n",
    "plt.show()\n",
    "sb.countplot(x = \"score_text\", data = white_data)\n",
    "print(len(white_data[white_data[\"score_text\"] == 0]))\n",
    "print(len(white_data[white_data[\"score_text\"] == 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix for black/white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confussion matrix for the white:\n",
      "[[297   0]\n",
      " [178   0]]\n",
      "[[1.        0.       ]\n",
      " [0.5993266 0.       ]]\n",
      "\n",
      "Confussion matrix for the black:\n",
      "[[349   0]\n",
      " [395   0]]\n",
      "[[1.         0.        ]\n",
      " [1.13180516 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_val_white = model(Xcat_white, Xnum_white)\n",
    "y_val_white = np.argmax(y_val_white.detach().numpy(), axis = 1)\n",
    "y_val_black = model(Xcat_black, Xnum_black)\n",
    "y_val_black = np.argmax(y_val_black.detach().numpy(), axis = 1)\n",
    "\n",
    "# Ground truth of recidivism from dataset\n",
    "y_white = torch.tensor(white_data[\"two_year_recid\"].values).flatten()\n",
    "y_black = torch.tensor(black_data[\"two_year_recid\"].values).flatten()\n",
    "\n",
    "print(\"Confussion matrix for the white:\")\n",
    "conf_white = confusion_matrix( y_white, y_val_white)\n",
    "print(conf_white)\n",
    "print(conf_white / conf_white.astype(np.float).sum(axis=1))\n",
    "print()\n",
    "print(\"Confussion matrix for the black:\")\n",
    "conf_black = confusion_matrix( y_black, y_val_black)\n",
    "print(conf_black)\n",
    "print(conf_black / conf_black.astype(np.float).sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted no recidivism:  696\n",
      "Predicted recidivism:  746\n",
      "Accuracy of the random forest model:  0.6615811373092927\n"
     ]
    }
   ],
   "source": [
    "# Define the model and fit it to the data\n",
    "def RandomForest(datatrain, datatest, ytrain, ytest, n_estimators = 53, max_depth = 14, max_features = \"log2\", criterion = \"entropy\", show_acc = True):\n",
    "    forestModel = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, max_features = max_features, criterion = criterion)\n",
    "    forestModel.fit(datatrain, ytrain)\n",
    "\n",
    "    # Predict on the test set\n",
    "    forestPreds = forestModel.predict(datatest)\n",
    "\n",
    "    forestProbs = forestModel.predict_proba(datatest)[:, 1]\n",
    "\n",
    "    if show_acc:\n",
    "        print(\"Predicted no recidivism: \", len(forestPreds[forestPreds == 0]))\n",
    "        print(\"Predicted recidivism: \", len(forestPreds[forestPreds == 1]))\n",
    "\n",
    "        print(\"Accuracy of the random forest model: \", len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds))\n",
    "\n",
    "RandomForest(Xcattrain, Xcattest, ytrain, ytest, n_estimators = 53, max_depth = 14, max_features = \"log2\", criterion = \"entropy\", show_acc = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baysian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[290.   2.   0.   1.]]\n",
      "[[170.  73.   0.   1.]]\n",
      "[[160.  40.   1.   1.]]\n",
      "[[27. 62.  0.  0.]]\n",
      "[[55. 59.  1.  0.]]\n",
      "[[290.   2.   0.   1.]]\n",
      "[[289.   2.   0.   1.]]\n",
      "[[300.  54.   1.   0.]]\n",
      "[[249.   1.   0.   0.]]\n",
      "[[265.   1.   1.   0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[259.   1.   0.   1.]]\n",
      "[[232.   1.   1.   1.]]\n",
      "[[1. 1. 1. 1.]]\n",
      "[[32.  1.  1.  1.]]\n",
      "[[17.  1.  1.  0.]]\n",
      "[[65.  1.  0.  0.]]\n",
      "[[51.  1.  1.  0.]]\n",
      "[[289.   1.   1.   1.]]\n",
      "[[275.   1.   0.   1.]]\n",
      "[[215.   1.   0.   0.]]\n",
      "[[225.   1.   0.   0.]]\n",
      "[[92.  1.  1.  0.]]\n",
      "[[115.   1.   0.   0.]]\n",
      "[[103.   1.   0.   0.]]\n",
      "[[177.   1.   1.   0.]]\n",
      "[[193.   1.   1.   0.]]\n",
      "[[146.   1.   1.   0.]]\n",
      "[[109.   1.   1.   1.]]\n",
      "[[11.  1.  0.  1.]]\n",
      "[[1. 1. 0. 0.]]\n",
      "[[159.   1.   0.   1.]]\n",
      "[[133.   1.   0.   1.]]\n",
      "[[82.  1.  0.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[68.  1.  1.  1.]]\n",
      "[[184.   1.   0.   1.]]\n",
      "[[150.   1.   0.   0.]]\n",
      "[[204.   1.   1.   1.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[76.  1.  1.  0.]]\n",
      "[[263.   8.   0.   1.]]\n",
      "[[108. 109.   0.   0.]]\n",
      "[[229. 109.   0.   0.]]\n",
      "[[  1. 109.   1.   1.]]\n",
      "[[300. 109.   0.   1.]]\n",
      "[[ 1. 23.  1.  0.]]\n",
      "[[101.  25.   0.   0.]]\n",
      "[[ 53. 109.   0.   1.]]\n",
      "[[300.  13.   0.   1.]]\n",
      "[[201.  21.   1.   0.]]\n",
      "[[164. 109.   1.   1.]]\n",
      "[[51. 16.  0.  1.]]\n",
      "[[262.  82.   1.   0.]]\n",
      "[[139.  13.   0.   0.]]\n",
      "[[103.  71.   1.   1.]]\n",
      "[[125.   1.   1.   0.]]\n",
      "[[213.  72.   1.   1.]]\n",
      "[[167.   1.   1.   0.]]\n",
      "[[42.  1.  0.  1.]]\n",
      "[[39.  1.  1.  0.]]\n",
      "[[25.  1.  0.  0.]]\n",
      "[[255.   1.   1.   0.]]\n",
      "[[ 1. 75.  1.  1.]]\n",
      "[[214.   1.   1.   1.]]\n",
      "[[121.   1.   0.   1.]]\n",
      "[[240.   1.   0.   1.]]\n",
      "[[172.   1.   0.   1.]]\n",
      "[[59.  1.  1.  1.]]\n",
      "[[7. 1. 1. 0.]]\n",
      "[[144.   1.   0.   1.]]\n",
      "[[196. 109.   1.   0.]]\n",
      "[[152.   1.   1.   1.]]\n",
      "[[188.   1.   1.   1.]]\n",
      "[[54.  1.  0.  1.]]\n",
      "[[98.  1.  1.  1.]]\n",
      "[[237.   1.   1.   0.]]\n",
      "[[34.  1.  0.  0.]]\n",
      "[[275.   1.   1.   0.]]\n",
      "[[300.  79.   1.   1.]]\n",
      "[[209.   1.   1.   0.]]\n",
      "[[270.   1.   0.   0.]]\n",
      "[[247.   1.   1.   1.]]\n",
      "[[93.  1.  0.  1.]]\n",
      "[[196.   1.   0.   1.]]\n",
      "[[20.  1.  0.  1.]]\n",
      "[[265. 109.   1.   0.]]\n",
      "[[138.   1.   0.   0.]]\n",
      "[[165.   1.   1.   1.]]\n",
      "[[73.  1.  0.  1.]]\n",
      "[[137.   1.   1.   1.]]\n",
      "[[136.  82.   1.   1.]]\n",
      "[[47.  1.  1.  1.]]\n",
      "[[202.   1.   0.   0.]]\n",
      "[[270.   1.   1.   1.]]\n",
      "[[87.  1.  1.  1.]]\n",
      "[[243.   1.   1.   0.]]\n",
      "[[208.   1.   0.   1.]]\n",
      "[[118.   1.   1.   0.]]\n",
      "[[86.  1.  0.  0.]]\n",
      "[[188.   1.   0.   0.]]\n",
      "[[232.   1.   0.   0.]]\n",
      "[[107.   1.   1.   0.]]\n",
      "[[4. 1. 0. 1.]]\n",
      "[[132.   1.   1.   0.]]\n",
      "[[75. 87.  0.  0.]]\n",
      "[[15.  1.  0.  0.]]\n",
      "[[70.  1.  1.  0.]]\n",
      "[[158.   1.   1.   0.]]\n",
      "[[29. 91.  1.  1.]]\n",
      "[[172.   1.   1.   1.]]\n",
      "[[37.  1.  1.  1.]]\n",
      "[[221.   1.   0.   1.]]\n",
      "[[58.  1.  0.  0.]]\n",
      "[[63.  1.  1.  0.]]\n",
      "[[180.   1.   1.   1.]]\n",
      "[[253.   1.   0.   1.]]\n",
      "[[280.   1.   0.   0.]]\n",
      "[[47.  1.  0.  0.]]\n",
      "[[184.   1.   1.   0.]]\n",
      "[[97.  1.  0.  0.]]\n",
      "[[226.   1.   1.   1.]]\n",
      "[[25.  1.  1.  1.]]\n",
      "[[128.   1.   0.   1.]]\n",
      "[[82.  1.  1.  0.]]\n",
      "[[163.   1.   0.   0.]]\n",
      "[[115.   1.   1.   1.]]\n",
      "[[280.   1.   1.   1.]]\n",
      "[[29.  1.  0.  1.]]\n",
      "[[199.   1.   1.   0.]]\n",
      "[[265.   1.   0.   1.]]\n",
      "[[229.   1.   1.   0.]]\n",
      "[[14.  1.  1.  1.]]\n",
      "[[126.   1.   0.   0.]]\n",
      "[[142.   1.   0.   0.]]\n",
      "[[62.  1.  0.  1.]]\n",
      "[[245.   1.   0.   1.]]\n",
      "[[78.  1.  1.  1.]]\n",
      "[[261.   1.   1.   1.]]\n",
      "[[9. 1. 0. 0.]]\n",
      "[[261.   1.   0.   0.]]\n",
      "[[206.   1.   0.   0.]]\n",
      "[[170.   1.   0.   0.]]\n",
      "[[199.   1.   1.   1.]]\n",
      "[[190.  53.   0.   0.]]\n",
      "[[177.   1.   0.   1.]]\n",
      "[[285.   1.   1.   0.]]\n",
      "[[180.  11.   1.   1.]]\n",
      "[[136.  54.   0.   0.]]\n",
      "[[104.   1.   1.   1.]]\n",
      "[[78.  1.  0.  0.]]\n",
      "[[44.  1.  1.  0.]]\n",
      "[[112.   1.   1.   0.]]\n",
      "[[235.   1.   0.   1.]]\n",
      "[[284.   1.   0.   1.]]\n",
      "[[254.   1.   1.   1.]]\n",
      "[[55.  1.  1.  0.]]\n",
      "[[229.   1.   0.   1.]]\n",
      "[[162.   1.   1.   0.]]\n",
      "[[240.   1.   0.   0.]]\n",
      "[[146.   1.   0.   0.]]\n",
      "[[138. 107.   1.   0.]]\n",
      "[[180.   1.   0.   0.]]\n",
      "[[107.   1.   0.   1.]]\n",
      "[[89.  1.  0.  1.]]\n",
      "[[154.   1.   0.   1.]]\n",
      "[[174.   1.   0.   0.]]\n",
      "[[116.   1.   0.   1.]]\n",
      "[[125.   1.   0.   1.]]\n",
      "[[29.  1.  1.  0.]]\n",
      "[[284.  68.   1.   0.]]\n",
      "[[239.   1.   1.   1.]]\n",
      "[[148.   1.   1.   1.]]\n",
      "[[143.   1.   1.   1.]]\n",
      "[[288.   1.   0.   0.]]\n",
      "[[127.   1.   1.   1.]]\n",
      "[[37.  1.  0.  1.]]\n",
      "[[218.   1.   1.   1.]]\n",
      "[[2. 1. 1. 0.]]\n",
      "[[110.   1.   0.   0.]]\n",
      "[[22.  1.  1.  0.]]\n",
      "[[68.  1.  0.  0.]]\n",
      "[[4. 1. 1. 1.]]\n",
      "[[15.  1.  0.  1.]]\n",
      "[[84. 50.  0.  1.]]\n",
      "[[242.   1.   1.   1.]]\n",
      "[[290.   2.   1.   0.]]\n",
      "[[168.   1.   0.   1.]]\n",
      "[[11.  1.  1.  0.]]\n",
      "[[130.   1.   0.   0.]]\n",
      "[[ 82. 109.   1.   0.]]\n",
      "[[211.   1.   0.   0.]]\n",
      "[[51.  1.  0.  1.]]\n",
      "[[73.  1.  1.  1.]]\n",
      "[[21.  1.  1.  1.]]\n",
      "[[8. 1. 0. 1.]]\n",
      "[[191.   1.   0.   1.]]\n",
      "[[96.  1.  1.  1.]]\n",
      "[[101.   1.   1.   0.]]\n",
      "[[270.   1.   1.   0.]]\n",
      "[[266.   1.   1.   1.]]\n",
      "[[257.   1.   1.   1.]]\n",
      "[[233.  87.   1.   0.]]\n",
      "[[203.   1.   1.   0.]]\n",
      "[[223.   1.   1.   1.]]\n",
      "[[250.   1.   0.   1.]]\n",
      "[[42.  1.  0.  0.]]\n",
      "[[156.   1.   1.   1.]]\n",
      "[[214.   1.   1.   0.]]\n",
      "[[89.  1.  1.  0.]]\n",
      "[[175.   1.   1.   1.]]\n",
      "[[139.   1.   1.   0.]]\n",
      "[[153.   1.   1.   0.]]\n",
      "[[140.   1.   0.   1.]]\n",
      "[[73.  1.  0.  0.]]\n",
      "[[197.   1.   0.   0.]]\n",
      "[[196.   1.   1.   1.]]\n",
      "[[171.   1.   1.   0.]]\n",
      "[[ 24. 109.   1.   0.]]\n",
      "[[258.   1.   1.   0.]]\n",
      "[[42.  1.  1.  1.]]\n",
      "[[196.  87.   1.   0.]]\n",
      "[[45.  1.  0.  1.]]\n",
      "[[119.   1.   1.   1.]]\n",
      "[[32.  1.  0.  0.]]\n",
      "[[250.   1.   1.   0.]]\n",
      "[[20.  3.  0.  1.]]\n",
      "[[ 1. 48.  1.  0.]]\n",
      "[[78. 14.  1.  0.]]\n",
      "[[90.  1.  1.  1.]]\n",
      "[[161.   1.   1.   1.]]\n",
      "[[157.   1.   0.   0.]]\n",
      "[[280.  92.   0.   0.]]\n",
      "[[101.   1.   0.   1.]]\n",
      "[[217.   1.   0.   1.]]\n",
      "[[92.  1.  0.  0.]]\n",
      "[[ 6. 90.  0.  0.]]\n",
      "[[225.   1.   1.   0.]]\n",
      "[[54.  1.  1.  1.]]\n",
      "[[25.  1.  0.  1.]]\n",
      "[[186.   1.   0.   1.]]\n",
      "[[83.  1.  1.  1.]]\n",
      "[[277.   1.   1.   1.]]\n",
      "[[53.  1.  0.  0.]]\n",
      "[[133.   1.   1.   1.]]\n",
      "[[118.   1.   0.   0.]]\n",
      "[[273.   1.   0.   0.]]\n",
      "[[279.   1.   1.   0.]]\n",
      "[[58.  1.  0.  1.]]\n",
      "[[66.  1.  0.  1.]]\n",
      "[[254.   1.   0.   0.]]\n",
      "[[184.   1.   1.   1.]]\n",
      "[[219.  49.   0.   0.]]\n",
      "[[6. 1. 0. 0.]]\n",
      "[[286.   1.   1.   1.]]\n",
      "[[112.   1.   0.   1.]]\n",
      "[[64.  1.  1.  1.]]\n",
      "[[21.  1.  0.  0.]]\n",
      "[[33.  1.  1.  0.]]\n",
      "[[204.   1.   0.   1.]]\n",
      "[[235.   1.   1.   1.]]\n",
      "[[283.   1.   1.   0.]]\n",
      "[[110.  45.   1.   1.]]\n",
      "[[236.   1.   0.   0.]]\n",
      "[[154.  96.   0.   0.]]\n",
      "[[288.   4.   1.   1.]]\n",
      "[[300.  73.   0.   0.]]\n",
      "[[229.  16.   0.   0.]]\n",
      "[[246.  58.   0.   1.]]\n",
      "[[113.   9.   1.   1.]]\n",
      "[[300.   4.   0.   0.]]\n",
      "[[51. 82.  0.  0.]]\n",
      "[[39. 42.  0.  1.]]\n",
      "[[154.  62.   1.   0.]]\n",
      "[[115.  89.   0.   0.]]\n",
      "[[211.   6.   0.   1.]]\n",
      "[[241.  35.   0.   0.]]\n",
      "[[198.  37.   1.   0.]]\n",
      "[[63. 39.  0.  0.]]\n",
      "[[266.  56.   0.   0.]]\n",
      "[[177.  94.   1.   0.]]\n",
      "[[289.   1.   1.   0.]]\n",
      "[[158.   8.   1.   0.]]\n",
      "[[247. 109.   1.   1.]]\n",
      "[[76. 68.  1.  0.]]\n",
      "[[94. 91.  1.  1.]]\n",
      "[[243.   7.   1.   0.]]\n",
      "[[214.  96.   1.   1.]]\n",
      "[[122.  66.   0.   1.]]\n",
      "[[222.  33.   1.   1.]]\n",
      "[[173.  52.   1.   1.]]\n",
      "[[131.  36.   0.   1.]]\n",
      "[[19. 41.  0.  1.]]\n",
      "[[190.  70.   1.   0.]]\n",
      "[[300.  36.   0.   1.]]\n",
      "[[280. 109.   1.   1.]]\n",
      "[[235.  68.   0.   0.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[62.  5.  0.  0.]]\n",
      "[[92.  7.  1.  1.]]\n",
      "[[84. 33.  1.  0.]]\n",
      "[[39. 25.  1.  0.]]\n",
      "[[18. 76.  0.  1.]]\n",
      "[[194.   1.   0.   0.]]\n",
      "[[123. 109.   1.   1.]]\n",
      "[[181.  34.   0.   1.]]\n",
      "[[122.   1.   1.   0.]]\n",
      "[[76.  1.  0.  1.]]\n",
      "[[290.   2.   0.   0.]]\n",
      "[[96.  1.  1.  0.]]\n",
      "[[39. 73.  1.  1.]]\n",
      "[[70.  1.  0.  1.]]\n",
      "[[282.  49.   0.   1.]]\n",
      "[[1. 6. 1. 0.]]\n",
      "[[278.   1.   0.   1.]]\n",
      "[[1. 4. 0. 0.]]\n",
      "[[ 1. 12.  1.  0.]]\n",
      "[[262.  38.   1.   1.]]\n",
      "[[291.   2.   1.   0.]]\n",
      "[[ 1. 34.  1.  1.]]\n",
      "[[40.  7.  1.  1.]]\n",
      "[[154.  79.   1.   1.]]\n",
      "[[249.  92.   0.   0.]]\n",
      "[[180. 109.   1.   0.]]\n",
      "[[ 68. 109.   0.   1.]]\n",
      "[[288.   1.   0.   1.]]\n",
      "[[ 38. 109.   0.   0.]]\n",
      "[[ 9. 61.  1.  1.]]\n",
      "[[212. 109.   1.   1.]]\n",
      "[[300.  94.   1.   0.]]\n",
      "[[52. 30.  0.  0.]]\n",
      "[[205.  57.   1.   1.]]\n",
      "[[211.   1.   1.   1.]]\n",
      "[[98. 56.  0.  0.]]\n",
      "[[250.  73.   1.   1.]]\n",
      "[[117.  30.   1.   1.]]\n",
      "[[277.   1.   0.   0.]]\n",
      "[[129.   5.   1.   1.]]\n",
      "[[146.  31.   0.   0.]]\n",
      "[[193.   5.   0.   0.]]\n",
      "[[6. 1. 1. 1.]]\n",
      "[[267.   1.   0.   0.]]\n",
      "[[135.   1.   1.   0.]]\n",
      "[[234.  49.   1.   1.]]\n",
      "[[60. 94.  1.  1.]]\n",
      "[[37.  1.  0.  0.]]\n",
      "[[213.   1.   0.   1.]]\n",
      "[[88. 76.  0.  1.]]\n",
      "[[163.   1.   0.   1.]]\n",
      "[[63. 74.  0.  1.]]\n",
      "[[97. 39.  1.  0.]]\n",
      "[[284.   1.   1.   1.]]\n",
      "[[224.   1.   0.   1.]]\n",
      "[[169.   1.   1.   1.]]\n",
      "[[69. 53.  1.  0.]]\n",
      "[[222.   1.   0.   0.]]\n",
      "[[1. 7. 1. 1.]]\n",
      "[[19. 27.  1.  0.]]\n",
      "[[279.  34.   0.   0.]]\n",
      "[[270.  71.   0.   1.]]\n",
      "[[139.  68.   0.   0.]]\n",
      "[[ 95. 109.   1.   0.]]\n",
      "[[122.  51.   1.   0.]]\n",
      "[[151. 109.   1.   0.]]\n",
      "[[168.  26.   0.   0.]]\n",
      "[[130.  95.   0.   0.]]\n",
      "[[70. 27.  0.  1.]]\n",
      "[[44. 94.  1.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[1. 1. 1. 0.]]\n",
      "[[81.  1.  1.  0.]]\n",
      "[[52.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[70.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[147.  46.   1.   1.]]\n",
      "[[41. 57.  0.  1.]]\n",
      "[[149.   1.   1.   0.]]\n",
      "[[256.   1.   0.   1.]]\n",
      "[[209.  31.   0.   1.]]\n",
      "[[207.   1.   1.   1.]]\n",
      "[[187.   1.   1.   0.]]\n",
      "[[160.   1.   0.   0.]]\n",
      "[[123.   1.   0.   0.]]\n",
      "[[251.  46.   0.   0.]]\n",
      "[[136.   1.   0.   1.]]\n",
      "[[265.  95.   1.   1.]]\n",
      "[[18.  1.  0.  0.]]\n",
      "[[220.  83.   0.   0.]]\n",
      "[[28.  1.  1.  1.]]\n",
      "[[234.   1.   1.   0.]]\n",
      "[[223.  62.   1.   1.]]\n",
      "[[182.  81.   0.   1.]]\n",
      "[[130.   1.   1.   1.]]\n",
      "[[ 14. 101.   0.   1.]]\n",
      "[[185.   1.   0.   0.]]\n",
      "[[48.  1.  0.  1.]]\n",
      "[[165.  87.   1.   0.]]\n",
      "[[149.   1.   0.   1.]]\n",
      "[[285.  81.   0.   1.]]\n",
      "[[181.   1.   1.   0.]]\n",
      "[[190.   1.   1.   0.]]\n",
      "[[100.   1.   0.   0.]]\n",
      "[[121.  78.   1.   1.]]\n",
      "[[273.   1.   1.   1.]]\n",
      "[[85.  1.  0.  1.]]\n",
      "[[193.   1.   1.   1.]]\n",
      "[[174.   1.   1.   0.]]\n",
      "[[178.   1.   1.   1.]]\n",
      "[[201.   1.   1.   1.]]\n",
      "[[39.  1.  0.  1.]]\n",
      "[[246.   1.   0.   0.]]\n",
      "[[101.   1.   1.   1.]]\n",
      "[[10.  1.  1.  1.]]\n",
      "[[228.   1.   0.   0.]]\n",
      "[[264.   1.   0.   0.]]\n",
      "[[257.   1.   0.   0.]]\n",
      "[[23.  1.  0.  1.]]\n",
      "[[131.   1.   0.   1.]]\n",
      "[[61.  1.  1.  1.]]\n",
      "[[123.   1.   1.   1.]]\n",
      "[[66.  1.  1.  0.]]\n",
      "[[112.   1.   1.   1.]]\n",
      "[[202.  76.   0.   1.]]\n",
      "[[243.   1.   0.   0.]]\n",
      "[[218.   1.   0.   0.]]\n",
      "[[285.   1.   0.   0.]]\n",
      "[[134.   1.   0.   0.]]\n",
      "[[18.  1.  1.  1.]]\n",
      "[[181.   1.   0.   1.]]\n",
      "[[182.   1.   0.   0.]]\n",
      "[[33.  1.  0.  1.]]\n",
      "[[262.   1.   1.   0.]]\n",
      "[[216.   1.   1.   0.]]\n",
      "[[196.   1.   1.   0.]]\n",
      "[[52. 46.  1.  1.]]\n",
      "[[85.  1.  1.  0.]]\n",
      "[[179.  63.   0.   0.]]\n",
      "[[48.  1.  1.  0.]]\n",
      "[[200.   1.   0.   1.]]\n",
      "[[128.   1.   1.   0.]]\n",
      "[[111.  60.   1.   1.]]\n",
      "[[ 1. 63.  0.  0.]]\n",
      "[[96.  1.  0.  1.]]\n",
      "[[3. 1. 0. 0.]]\n",
      "[[252.   1.   1.   0.]]\n",
      "[[61.  1.  0.  0.]]\n",
      "[[237.  99.   0.   1.]]\n",
      "[[262.   1.   0.   1.]]\n",
      "[[165.   1.   1.   0.]]\n",
      "[[231.   1.   1.   0.]]\n",
      "[[83.  1.  0.  0.]]\n",
      "[[264.   1.   1.   1.]]\n",
      "[[246.   1.   1.   0.]]\n",
      "[[19.  1.  1.  0.]]\n",
      "[[28.  1.  0.  0.]]\n",
      "[[251.   1.   1.   1.]]\n",
      "[[28. 49.  1.  1.]]\n",
      "[[104.   1.   0.   1.]]\n",
      "[[106.   1.   0.   0.]]\n",
      "[[29. 34.  1.  0.]]\n",
      "[[44.  1.  0.  0.]]\n",
      "[[272.  44.   0.   0.]]\n",
      "[[81. 97.  1.  1.]]\n",
      "[[251.   1.   0.   0.]]\n",
      "[[190.  98.   0.   1.]]\n",
      "[[26.  1.  1.  0.]]\n",
      "[[143.   1.   1.   0.]]\n",
      "[[190.  26.   1.   0.]]\n",
      "[[153.   1.   0.   0.]]\n",
      "[[166.   1.   0.   0.]]\n",
      "[[94.  1.  0.  0.]]\n",
      "[[238.   1.   0.   1.]]\n",
      "[[59.  1.  1.  0.]]\n",
      "[[206.   1.   1.   0.]]\n",
      "[[247.   1.   0.   1.]]\n",
      "[[75.  1.  1.  1.]]\n",
      "[[88. 63.  0.  1.]]\n",
      "[[191.   1.   0.   0.]]\n",
      "[[269.   1.   0.   1.]]\n",
      "[[140.   1.   1.   1.]]\n",
      "[[44.  1.  1.  1.]]\n",
      "[[105.  98.   1.   1.]]\n",
      "[[10. 33.  1.  0.]]\n",
      "[[212.   1.   1.   0.]]\n",
      "[[79.  1.  0.  1.]]\n",
      "[[108.   1.   0.   0.]]\n",
      "[[151.   1.   0.   1.]]\n",
      "[[36.  1.  1.  0.]]\n",
      "[[254.  27.   0.   0.]]\n",
      "[[ 1. 98.  1.  0.]]\n",
      "[[3. 6. 1. 0.]]\n",
      "[[227.   4.   1.   1.]]\n",
      "[[234.  25.   0.   1.]]\n",
      "[[104.  83.   0.   0.]]\n",
      "[[116.   1.   1.   0.]]\n",
      "[[143.  93.   1.   1.]]\n",
      "[[230.   1.   0.   0.]]\n",
      "[[272.   1.   0.   1.]]\n",
      "[[258.  64.   1.   0.]]\n",
      "[[290.  41.   0.   0.]]\n",
      "[[158.   1.   1.   1.]]\n",
      "[[50.  1.  1.  1.]]\n",
      "[[300.  25.   1.   0.]]\n",
      "[[161.  53.   0.   0.]]\n",
      "[[241.   1.   1.   0.]]\n",
      "[[171.   4.   0.   0.]]\n",
      "[[146.   4.   0.   1.]]\n",
      "[[134.  24.   1.   0.]]\n",
      "[[290. 101.   0.   0.]]\n",
      "[[268.   1.   1.   0.]]\n",
      "[[203.  98.   0.   0.]]\n",
      "[[75. 40.  1.  1.]]\n",
      "[[232.   1.   0.   1.]]\n",
      "[[155.   1.   1.   0.]]\n",
      "[[145.   1.   1.   1.]]\n",
      "[[13.  1.  0.  0.]]\n",
      "[[14.  1.  1.  0.]]\n",
      "[[30. 80.  0.  0.]]\n",
      "[[110.   1.   0.   1.]]\n",
      "[[219.  23.   1.   0.]]\n",
      "[[16.  1.  1.  1.]]\n",
      "[[174.   1.   0.   1.]]\n",
      "[[155.  24.   1.   1.]]\n",
      "[[147.   1.   0.   1.]]\n",
      "[[177.   1.   0.   0.]]\n",
      "[[207.  45.   0.   0.]]\n",
      "[[16. 52.  0.  0.]]\n",
      "[[151.   1.   1.   0.]]\n",
      "[[204.   1.   0.   0.]]\n",
      "[[225.  97.   0.   0.]]\n",
      "[[ 11. 109.   1.   0.]]\n",
      "[[222.   1.   1.   0.]]\n",
      "[[99.  1.  0.  1.]]\n",
      "[[168.   1.   0.   0.]]\n",
      "[[73.  1.  1.  0.]]\n",
      "[[193.   1.   0.   1.]]\n",
      "[[191.   1.   1.   1.]]\n",
      "[[57.  1.  1.  1.]]\n",
      "[[35.  1.  1.  1.]]\n",
      "[[206.   1.   0.   1.]]\n",
      "[[186.   1.   1.   1.]]\n",
      "[[155.   1.   0.   0.]]\n",
      "[[291.  60.   0.   1.]]\n",
      "[[104.   1.   1.   0.]]\n",
      "[[189.   1.   0.   1.]]\n",
      "[[18. 88.  1.  0.]]\n",
      "[[89.  1.  0.  0.]]\n",
      "[[160.   1.   1.   0.]]\n",
      "[[171.  40.   0.   0.]]\n",
      "[[220.   1.   1.   1.]]\n",
      "[[29. 18.  1.  0.]]\n",
      "[[239.   1.   1.   0.]]\n",
      "[[17.  1.  0.  1.]]\n",
      "[[51. 70.  0.  1.]]\n",
      "[[107.  34.   0.   1.]]\n",
      "[[230.   1.   1.   1.]]\n",
      "[[165.   1.   0.   1.]]\n",
      "[[41.  1.  1.  0.]]\n",
      "[[71.  1.  0.  0.]]\n",
      "[[245.   1.   1.   1.]]\n",
      "[[120.   1.   0.   0.]]\n",
      "[[56.  1.  0.  0.]]\n",
      "[[274.  82.   1.   0.]]\n",
      "[[87.  1.  0.  1.]]\n",
      "[[267.  28.   0.   0.]]\n",
      "[[93.  1.  1.  1.]]\n",
      "[[113.   1.   0.   0.]]\n",
      "[[142.   1.   0.   1.]]\n",
      "[[227.  75.   0.   1.]]\n",
      "[[255. 102.   1.   1.]]\n",
      "[[210.   1.   0.   1.]]\n",
      "[[86. 23.  0.  1.]]\n",
      "[[114.   1.   1.   0.]]\n",
      "[[161.   1.   0.   1.]]\n",
      "[[11.  1.  0.  0.]]\n",
      "[[242.  80.   0.   0.]]\n",
      "[[80.  1.  1.  1.]]\n",
      "[[1. 1. 0. 1.]]\n",
      "[[282.   1.   0.   0.]]\n",
      "[[167.  99.   0.   1.]]\n",
      "[[81.  1.  0.  0.]]\n",
      "[[166.  63.   1.   1.]]\n",
      "[[2. 1. 0. 1.]]\n",
      "[[209.   1.   1.   1.]]\n",
      "[[118. 100.   1.   0.]]\n",
      "[[28.  4.  1.  0.]]\n",
      "[[ 9. 71.  1.  0.]]\n",
      "[[201.   1.   1.   0.]]\n",
      "[[120.  39.   0.   0.]]\n",
      "[[ 91. 100.   0.   0.]]\n",
      "[[30.  1.  0.  0.]]\n",
      "[[66. 63.  0.  1.]]\n",
      "[[144.   1.   0.   0.]]\n",
      "[[70. 99.  0.  0.]]\n",
      "[[260.   1.   1.   0.]]\n",
      "[[292.  88.   1.   0.]]\n",
      "[[121.   1.   1.   1.]]\n",
      "[[79.  1.  1.  0.]]\n",
      "[[40.  1.  0.  0.]]\n",
      "[[157.   1.   0.   1.]]\n",
      "[[202.  65.   0.   0.]]\n",
      "[[ 31. 100.   0.   0.]]\n",
      "[[ 1. 32.  1.  0.]]\n",
      "[[130.   1.   1.   0.]]\n",
      "[[135.   1.   1.   1.]]\n",
      "[[242.   1.   0.   1.]]\n",
      "[[179.   1.   0.   1.]]\n",
      "[[167.   1.   1.   1.]]\n",
      "[[60.  1.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "## define the domain of the considered parameters\n",
    "n_estimators = tuple(np.arange(1,301,1, dtype= np.int))\n",
    "# print(n_estimators)\n",
    "max_depth = tuple(np.arange(1,110,1, dtype= np.int))\n",
    "# max_features = ('log2', 'sqrt', None)\n",
    "max_features = (0, 1)\n",
    "# criterion = ('gini', 'entropy')\n",
    "criterion = (0, 1)\n",
    "\n",
    "\n",
    "# define the dictionary for GPyOpt\n",
    "domain = [{'n_estimators': 'var_1',  'type': 'discrete',     'domain': n_estimators},\n",
    "          {'max_depth': 'var_2',     'type': 'discrete',     'domain': max_depth},\n",
    "          {'max_features': 'var_3',  'type': 'categorical',  'domain': max_features},\n",
    "          {'criterion': 'var_4',     'type': 'categorical',  'domain': criterion}]\n",
    "\n",
    "\n",
    "## we have to define the function we want to maximize --> validation accuracy, \n",
    "## note it should take a 2D ndarray but it is ok that it assumes only one point\n",
    "## in this setting\n",
    "def objective_function(x): \n",
    "    print(x)\n",
    "    # we have to handle the categorical variables that is convert 0/1 to labels\n",
    "    # log2/sqrt and gini/entropy\n",
    "    \n",
    "    param = x[0]\n",
    "    \n",
    "    if param[2] == 0:\n",
    "        var_3 = \"log2\"\n",
    "    else:\n",
    "        var_3 = \"sqrt\"\n",
    "    \n",
    "    if param[3] == 0:\n",
    "        var_4 = \"gini\"\n",
    "    else:\n",
    "        var_4 = \"entropy\"\n",
    "        \n",
    "        \n",
    "#fit the model\n",
    "    model = RandomForestClassifier(n_estimators = int(param[0]), criterion = var_4, max_depth = int(param[1]), max_features = var_3)\n",
    "    model.fit(Xcattrain, ytrain)\n",
    "    forestPreds = model.predict(Xcattest)\n",
    "    accuracy = len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "opt = GPyOpt.methods.BayesianOptimization(f = objective_function,   # function to optimize\n",
    "                                              domain = domain,         # box-constrains of the problem\n",
    "                                              acquisition_type = \"EI\",      # Select acquisition function MPI, EI, LCB\n",
    "                                             )\n",
    "opt.acquisition.exploration_weight=.1\n",
    "\n",
    "opt.run_optimization(max_iter = 100) \n",
    "\n",
    "\n",
    "x_best = opt.X[np.argmin(opt.Y)]\n",
    "print(\"The best parameters obtained: n_estimators=\" + str(x_best[0]) + \", max_depth=\" + str(x_best[1]) + \", max_features=\" + str(\n",
    "    x_best[2])  + \", criterion=\" + str(\n",
    "    x_best[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3897 3317]\n",
      " [   0    0]]\n",
      "[[0.54019961 0.45980039]\n",
      " [0.         0.        ]]\n",
      "0.5401996118658164\n",
      "Difference between baseline and neural network: 0.17963395262794224\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def baseline(data):\n",
    "    if len(data[data[\"score_text\"] == 0]) > len(data[data[\"score_text\"] == 1]):\n",
    "        y_pred = [0] * len(data[\"score_text\"])\n",
    "        y_pred = torch.tensor(y_pred)\n",
    "    else:\n",
    "        y_pred = [1] * len(data[\"score_text\"])\n",
    "        y_pred = torch.tensor(y_pred)\n",
    "    return y_pred\n",
    "\n",
    "trues = torch.tensor(data[\"score_text\"])\n",
    "y_pred = baseline(data)\n",
    "conf_baseline = confusion_matrix(y_pred,trues)\n",
    "print(conf_baseline)\n",
    "print(conf_baseline / conf_baseline.astype(np.float).sum())\n",
    "baseline_acc = accuracy_score(y_pred, trues)\n",
    "print(baseline_acc)\n",
    "print(\"Difference between baseline and neural network:\", neural_acc - baseline_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
