{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import GPyOpt\n",
    "s = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "#data = pd.read_csv(\"/home/oskar/Desktop/fagprojekt/compas/compas-scores-raw.csv\")\n",
    "url = \"https://raw.githubusercontent.com/oskarwiese/fagprojekt/master/compas/compas-scores-raw.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/oskarwiese/fagprojekt/master/compas_propublica/compas-scores-two-years.csv\"\n",
    "new_data = pd.read_csv(url)\n",
    "# Til at se p√• dataen \n",
    "#print(data.head)\n",
    "#print(data.columns)\n",
    "\n",
    "# Check if there are any missing values\n",
    "print(np.count_nonzero(data[\"IsDeleted\"] == 1))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'name', 'first', 'last', 'compas_screening_date', 'sex', 'dob',\n",
      "       'age', 'age_cat', 'race', 'juv_fel_count', 'decile_score',\n",
      "       'juv_misd_count', 'juv_other_count', 'priors_count',\n",
      "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
      "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas',\n",
      "       'c_charge_degree', 'c_charge_desc', 'is_recid', 'r_case_number',\n",
      "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
      "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
      "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
      "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
      "       'decile_score.1', 'score_text', 'screening_date',\n",
      "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
      "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
      "       'start', 'end', 'event', 'two_year_recid'],\n",
      "      dtype='object')\n",
      "0        NaN\n",
      "1       (F3)\n",
      "2       (M1)\n",
      "3        NaN\n",
      "4        NaN\n",
      "        ... \n",
      "7209     NaN\n",
      "7210     NaN\n",
      "7211     NaN\n",
      "7212     NaN\n",
      "7213    (M2)\n",
      "Name: r_charge_degree, Length: 7214, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(new_data.columns)\n",
    "print(new_data[\"r_charge_degree\"])\n",
    "def is_plot():\n",
    "    sb.countplot(x = \"score_text\", hue = \"race\", data = new_data)\n",
    "    plt.show()\n",
    "\n",
    "    sb.countplot(x = \"two_year_recid\", hue = \"race\", data = new_data)\n",
    "    plt.show()\n",
    "    sb.countplot(x = \"is_recid\", hue = \"race\", data = new_data)\n",
    "    plt.show()\n",
    "    sb.countplot(x = \"is_violent_recid\", hue = \"race\", data = new_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots():\n",
    "    # Show distribution of different ethnicities and sexes\n",
    "    chart = sb.countplot(x = \"Ethnic_Code_Text\", data = data)\n",
    "    chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    chart.set(xlabel='Ethnicity', ylabel='Count')\n",
    "    plt.show()\n",
    "    \n",
    "    chart = sb.countplot(x = \"Sex_Code_Text\", data = data)\n",
    "    chart.set(xlabel='Sex', ylabel='Count')\n",
    "    plt.show()\n",
    "    \n",
    "    sb.countplot(x = \"Language\", data = data)\n",
    "    plt.show()\n",
    "    \n",
    "    # Showing the distribution of the raw and decile values\n",
    "    plt.xlabel(\"Raw value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Visualization of the values\")\n",
    "    plt.hist(data[\"RawScore\"])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.xlabel(\"Decile value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Visualization of the decile values\")\n",
    "    plt.hist(data[\"DecileScore\"])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #sb.countplot(x = \"RawScore\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Indication that some black people might get higher sentences that white people\n",
    "    sb.countplot(x = \"DecileScore\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    plt.show()\n",
    "    \n",
    "    sb.countplot(x = \"ScoreText\", hue = \"Ethnic_Code_Text\", data = data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"Agency_Text\", \"Sex_Code_Text\", \"Ethnic_Code_Text\", \"ScaleSet_ID\", \"AssessmentReason\", \"Language\", \"LegalStatus\", \"CustodyStatus\", \"MaritalStatus\", \"RecSupervisionLevel\"]\n",
    "new_categoricals = [\"c_charge_degree\", \"race\", \"age_cat\", \"sex\", \"is_recid\"] # \"two_year_recid\"\n",
    "# Changing date of birth into age,as this should work better in a neural network\n",
    "new_numericals = [\"age\", \"priors_count\"] # \"days_b_screening_arrest\"\n",
    "\n",
    "if s == 1:\n",
    "    ages = [None] * len(data[\"DateOfBirth\"])\n",
    "    for i in range(len(data[\"DateOfBirth\"])):\n",
    "        ages[i] = 20 +(100 - int(data[\"DateOfBirth\"][i].split(\"/\")[2]))\n",
    "    data[\"DateOfBirth\"] = ages\n",
    "    numericals = [\"DateOfBirth\"]\n",
    "    s+=1\n",
    "else:\n",
    "    pass\n",
    "\n",
    "outputs = [\"ScoreText\"]\n",
    "new_outputs = [\"score_text\"]\n",
    "\n",
    "data = data.dropna(axis = 0, how = 'any')\n",
    "data[outputs] = data[outputs].replace('Low',0)\n",
    "data[outputs] = data[outputs].replace('Medium',1)\n",
    "data[outputs] = data[outputs].replace('High',1)\n",
    "data[outputs] = data[outputs].astype(\"category\")\n",
    "\n",
    "\n",
    "new_data[new_outputs] = new_data[new_outputs].replace('Low',0)\n",
    "new_data[new_outputs] = new_data[new_outputs].replace('Medium',1)\n",
    "new_data[new_outputs] = new_data[new_outputs].replace('High',1)\n",
    "new_data[new_outputs] = new_data[new_outputs].astype(\"category\")\n",
    "\n",
    "\n",
    "for category in categoricals:\n",
    "    data[category] = data[category].astype(\"category\")\n",
    "    \n",
    "for new_category in new_categoricals:\n",
    "    new_data[new_category] = new_data[new_category].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.8624, -0.7121],\n",
       "         [-0.0754, -0.7121],\n",
       "         [-0.9148,  0.1069],\n",
       "         ...,\n",
       "         [-0.9148, -0.3026],\n",
       "         [ 1.4355, -0.7121],\n",
       "         [-0.4951, -0.7121]]), tensor([[ 0.6365, -0.7075],\n",
       "         [-0.3815, -0.5025],\n",
       "         [-0.1270, -0.7075],\n",
       "         ...,\n",
       "         [ 1.9090, -0.7075],\n",
       "         [-0.1270, -0.0924],\n",
       "         [-0.9753, -0.2974]]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Preparing data for pytorch\n",
    "Xcat = []\n",
    "for i in range(len(categoricals)):\n",
    "    Xcat.append(data[categoricals[i]].cat.codes.values)\n",
    "Xcat = torch.tensor(Xcat , dtype = torch.int64).T\n",
    "\n",
    "\n",
    "new_Xcat = []\n",
    "for i in range(len(new_categoricals)):\n",
    "    new_Xcat.append(new_data[new_categoricals[i]].cat.codes.values)\n",
    "new_Xcat = torch.tensor(new_Xcat , dtype = torch.int64).T\n",
    "\n",
    "#Converting the numerical values to a tensor\n",
    "Xnum = np.stack([data[col].values for col in numericals], 1)\n",
    "Xnum = torch.tensor(Xnum, dtype=torch.float)\n",
    "\n",
    "\n",
    "new_Xnum = np.stack([new_data[col].values for col in new_numericals], 1)\n",
    "new_Xnum = torch.tensor(new_Xnum, dtype=torch.float)\n",
    "\n",
    "# Converting the output to tensor\n",
    "y = torch.tensor(data[outputs].values).flatten()\n",
    "new_y = torch.tensor(new_data[new_outputs].values).flatten()\n",
    "\n",
    "# Calculation of embedding sizes for the categorical values in the format (unique categorical values, embedding size (dimension of encoding))\n",
    "categorical_column_sizes = [len(data[column].cat.categories) for column in categoricals]\n",
    "categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]\n",
    "\n",
    "\n",
    "new_categorical_column_sizes = [len(new_data[column].cat.categories) for column in new_categoricals]\n",
    "new_categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in new_categorical_column_sizes]\n",
    "\n",
    "# Train-test split\n",
    "totalnumber = len(Xnum)\n",
    "testnumber = int(totalnumber * 0.2)\n",
    "\n",
    "\n",
    "\n",
    "new_totalnumber = len(new_Xnum)\n",
    "new_testnumber = int(new_totalnumber * 0.2)\n",
    "\n",
    "Xcattrain = Xcat[:totalnumber - testnumber]\n",
    "Xcattest = Xcat[totalnumber - testnumber:totalnumber]\n",
    "Xnumtrain = Xnum[:totalnumber - testnumber]\n",
    "Xnumtest = Xnum[totalnumber - testnumber:totalnumber]\n",
    "ytrain = y[:totalnumber - testnumber]\n",
    "ytest = y[totalnumber - testnumber:totalnumber]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_Xcattrain = new_Xcat[:new_totalnumber - new_testnumber]\n",
    "new_Xcattest = new_Xcat[new_totalnumber - new_testnumber:new_totalnumber]\n",
    "new_Xnumtrain = new_Xnum[:new_totalnumber - new_testnumber]\n",
    "new_Xnumtest = new_Xnum[new_totalnumber - new_testnumber:new_totalnumber]\n",
    "new_ytrain = new_y[:new_totalnumber - new_testnumber]\n",
    "new_ytest = new_y[new_totalnumber - new_testnumber:new_totalnumber]\n",
    "\n",
    "def normalize(type):\n",
    "    if type == \"minmax\":\n",
    "        for i in range(new_Xnumtrain.size()[1]):\n",
    "            new_Xnumtrain[:,i] = (new_Xnumtrain[:,i]-new_Xnumtrain[:,i].min()) / (new_Xnumtrain[:,i].max()-new_Xnumtrain[:,i].min())\n",
    "            new_Xnumtest[:,i] = (new_Xnumtest[:,i]-new_Xnumtest[:,i].min()) / (new_Xnumtest[:,i].max()-new_Xnumtest[:,i].min())\n",
    "        return new_Xnumtrain, new_Xnumtest\n",
    "    elif type == \"zscore\":\n",
    "        for i in range(new_Xnumtrain.size()[1]):\n",
    "            new_Xnumtrain[:,i] = (new_Xnumtrain[:,i]-new_Xnumtrain[:,i].mean()) / (new_Xnumtrain[:,i].std())\n",
    "            new_Xnumtest[:,i] = (new_Xnumtest[:,i]-new_Xnumtest[:,i].mean()) / (new_Xnumtest[:,i].std())\n",
    "        return new_Xnumtrain, new_Xnumtest\n",
    "    else:\n",
    "        raise ValueError(\"Please choose a correct normalization type\")\n",
    "\n",
    "normalize(\"zscore\")\n",
    "\n",
    "\n",
    "#new_Xnumtrain = torch.tensor(np.vstack([(new_Xnumtrain[:,i]-new_Xnumtrain[:,i].min()) / (new_Xnumtrain[:,i].max()-new_Xnumtrain[:,i].min()) for i in range(new_Xnumtrain.size()[1]) if \"Tue elsker det her\"])).view(-1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols + num_numerical_cols\n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = torch.cat([x, x_numerical], 1)\n",
    "        x = self.layers(x)\n",
    "        return nn.functional.softmax(x, dim = -1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(4, 2)\n",
      "    (1): Embedding(2, 1)\n",
      "    (2): Embedding(9, 5)\n",
      "    (3): Embedding(2, 1)\n",
      "    (4): Embedding(1, 1)\n",
      "    (5): Embedding(2, 1)\n",
      "    (6): Embedding(5, 3)\n",
      "    (7): Embedding(6, 3)\n",
      "    (8): Embedding(7, 4)\n",
      "    (9): Embedding(4, 2)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.6, inplace=False)\n",
      "  (batch_norm_num): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=10, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.6, inplace=False)\n",
      "    (4): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.6, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.6, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.6, inplace=False)\n",
      "    (16): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define and show the model\n",
    "#model = Model(categorical_embedding_sizes, 1, 2, [8,16,32,64,128], p=0.6)\n",
    "model = Model(categorical_embedding_sizes, 1, 2, [10,20,20,10], p=0.6)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimization\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001 , weight_decay = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index out of range: Tried to access index 2 out of table with 1 rows. at C:\\w\\1\\s\\tmp_conda_3.6_014803\\conda\\conda-bld\\pytorch_1565315401686\\work\\aten\\src\\TH/generic/THTensorEvenMoreMath.cpp:237",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-bd5697ab9542>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXcattrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXnumtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0msingle_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0maggregated_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingle_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-103-3e78354cb99f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_categorical, x_numerical)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_categorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m         return F.embedding(\n\u001b[0;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1465\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1467\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: index out of range: Tried to access index 2 out of table with 1 rows. at C:\\w\\1\\s\\tmp_conda_3.6_014803\\conda\\conda-bld\\pytorch_1565315401686\\work\\aten\\src\\TH/generic/THTensorEvenMoreMath.cpp:237"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "aggregated_losses = []\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(Xcattrain, Xnumtrain)\n",
    "    single_loss = loss_function(y_pred, ytrain)\n",
    "    aggregated_losses.append(single_loss)\n",
    "    \n",
    "    if i % 25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "    \n",
    "\n",
    "\n",
    "# Plot the loss over epocs\n",
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val = model(Xcattest, Xnumtest)\n",
    "    loss = loss_function(y_val, ytest)\n",
    "\n",
    "\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "\n",
    "print(confusion_matrix(ytest,y_val))\n",
    "print(classification_report(ytest,y_val))\n",
    "print(accuracy_score(ytest, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(2, 1)\n",
      "    (1): Embedding(6, 3)\n",
      "    (2): Embedding(3, 2)\n",
      "    (3): Embedding(2, 1)\n",
      "    (4): Embedding(2, 1)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (batch_norm_num): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=16, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "    (16): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.5, inplace=False)\n",
      "    (20): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define and show the model\n",
    "#model = Model(categorical_embedding_sizes, 1, 2, [8,16,32,64,128], p=0.6)\n",
    "model = Model(new_categorical_embedding_sizes, 2, 2, [16,32,64,128,64], p=0.5)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimization\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 0.71831161\n",
      "epoch:  26 loss: 0.66727412\n",
      "epoch:  51 loss: 0.64092255\n",
      "epoch:  76 loss: 0.62120104\n",
      "epoch: 101 loss: 0.59940594\n",
      "epoch: 126 loss: 0.58769679\n",
      "epoch: 151 loss: 0.58157176\n",
      "epoch: 176 loss: 0.57496309\n",
      "epoch: 201 loss: 0.56143844\n",
      "epoch: 226 loss: 0.56445652\n",
      "epoch: 251 loss: 0.56118506\n",
      "epoch: 276 loss: 0.56061453\n",
      "epoch: 300 loss: 0.5548986197\n",
      "[[626 144]\n",
      " [215 457]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.81      0.78       770\n",
      "           1       0.76      0.68      0.72       672\n",
      "\n",
      "    accuracy                           0.75      1442\n",
      "   macro avg       0.75      0.75      0.75      1442\n",
      "weighted avg       0.75      0.75      0.75      1442\n",
      "\n",
      "0.7510402219140083\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 300\n",
    "aggregated_losses = []\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(new_Xcattrain, new_Xnumtrain)\n",
    "    single_loss = loss_function(y_pred, new_ytrain)\n",
    "    aggregated_losses.append(single_loss)\n",
    "    \n",
    "    if i % 25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "    \n",
    "\n",
    "\n",
    "# Plot the loss over epocs\n",
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val = model(new_Xcattest, new_Xnumtest)\n",
    "    loss = loss_function(y_val, new_ytest)\n",
    "\n",
    "\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "\n",
    "print(confusion_matrix(new_ytest,y_val))\n",
    "print(classification_report(new_ytest,y_val))\n",
    "print(accuracy_score(new_ytest, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times low scoretext is predicted:  1954\n",
      "Times medium scoretext is predicted:  534\n",
      "Times high scoretext is predicted:  639\n",
      "Accuracy of the random forest model:  0.7547169811320755\n"
     ]
    }
   ],
   "source": [
    "# Define the model and fit it to the data\n",
    "forestModel = RandomForestClassifier(n_estimators = 53, max_depth = 14, max_features = \"log2\", criterion = \"entropy\")\n",
    "forestModel.fit(Xcattrain, ytrain)\n",
    "\n",
    "# Predict on the test set\n",
    "forestPreds = forestModel.predict(Xcattest)\n",
    "forestProbs = forestModel.predict_proba(Xcattest)[:, 1]\n",
    "\n",
    "print(\"Times low scoretext is predicted: \", len(forestPreds[forestPreds == 0]))\n",
    "print(\"Times medium scoretext is predicted: \", len(forestPreds[forestPreds == 1]))\n",
    "print(\"Times high scoretext is predicted: \", len(forestPreds[forestPreds == 2]))\n",
    "\n",
    "print(\"Accuracy of the random forest model: \", len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted no recidivism:  802\n",
      "Predicted recidivism:  640\n",
      "Accuracy of the random forest model:  0.6685159500693482\n"
     ]
    }
   ],
   "source": [
    "# Define the model and fit it to the data\n",
    "forestModel = RandomForestClassifier(n_estimators = 53, max_depth = 14, max_features = \"log2\", criterion = \"entropy\")\n",
    "forestModel.fit(new_Xcattrain, new_ytrain)\n",
    "\n",
    "# Predict on the test set\n",
    "forestPreds = forestModel.predict(new_Xcattest)\n",
    "forestProbs = forestModel.predict_proba(new_Xcattest)[:, 1]\n",
    "\n",
    "print(\"Predicted no recidivism: \", len(forestPreds[forestPreds == 0]))\n",
    "print(\"Predicted recidivism: \", len(forestPreds[forestPreds == 1]))\n",
    "\n",
    "print(\"Accuracy of the random forest model: \", len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == new_ytest]) / len(forestPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baysian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[290.   2.   0.   1.]]\n",
      "[[170.  73.   0.   1.]]\n",
      "[[160.  40.   1.   1.]]\n",
      "[[27. 62.  0.  0.]]\n",
      "[[55. 59.  1.  0.]]\n",
      "[[290.   2.   0.   1.]]\n",
      "[[289.   2.   0.   1.]]\n",
      "[[300.  54.   1.   0.]]\n",
      "[[249.   1.   0.   0.]]\n",
      "[[265.   1.   1.   0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[259.   1.   0.   1.]]\n",
      "[[232.   1.   1.   1.]]\n",
      "[[1. 1. 1. 1.]]\n",
      "[[32.  1.  1.  1.]]\n",
      "[[17.  1.  1.  0.]]\n",
      "[[65.  1.  0.  0.]]\n",
      "[[51.  1.  1.  0.]]\n",
      "[[289.   1.   1.   1.]]\n",
      "[[275.   1.   0.   1.]]\n",
      "[[215.   1.   0.   0.]]\n",
      "[[225.   1.   0.   0.]]\n",
      "[[92.  1.  1.  0.]]\n",
      "[[115.   1.   0.   0.]]\n",
      "[[103.   1.   0.   0.]]\n",
      "[[177.   1.   1.   0.]]\n",
      "[[193.   1.   1.   0.]]\n",
      "[[146.   1.   1.   0.]]\n",
      "[[109.   1.   1.   1.]]\n",
      "[[11.  1.  0.  1.]]\n",
      "[[1. 1. 0. 0.]]\n",
      "[[159.   1.   0.   1.]]\n",
      "[[133.   1.   0.   1.]]\n",
      "[[82.  1.  0.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[68.  1.  1.  1.]]\n",
      "[[184.   1.   0.   1.]]\n",
      "[[150.   1.   0.   0.]]\n",
      "[[204.   1.   1.   1.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[76.  1.  1.  0.]]\n",
      "[[263.   8.   0.   1.]]\n",
      "[[108. 109.   0.   0.]]\n",
      "[[229. 109.   0.   0.]]\n",
      "[[  1. 109.   1.   1.]]\n",
      "[[300. 109.   0.   1.]]\n",
      "[[ 1. 23.  1.  0.]]\n",
      "[[101.  25.   0.   0.]]\n",
      "[[ 53. 109.   0.   1.]]\n",
      "[[300.  13.   0.   1.]]\n",
      "[[201.  21.   1.   0.]]\n",
      "[[164. 109.   1.   1.]]\n",
      "[[51. 16.  0.  1.]]\n",
      "[[262.  82.   1.   0.]]\n",
      "[[139.  13.   0.   0.]]\n",
      "[[103.  71.   1.   1.]]\n",
      "[[125.   1.   1.   0.]]\n",
      "[[213.  72.   1.   1.]]\n",
      "[[167.   1.   1.   0.]]\n",
      "[[42.  1.  0.  1.]]\n",
      "[[39.  1.  1.  0.]]\n",
      "[[25.  1.  0.  0.]]\n",
      "[[255.   1.   1.   0.]]\n",
      "[[ 1. 75.  1.  1.]]\n",
      "[[214.   1.   1.   1.]]\n",
      "[[121.   1.   0.   1.]]\n",
      "[[240.   1.   0.   1.]]\n",
      "[[172.   1.   0.   1.]]\n",
      "[[59.  1.  1.  1.]]\n",
      "[[7. 1. 1. 0.]]\n",
      "[[144.   1.   0.   1.]]\n",
      "[[196. 109.   1.   0.]]\n",
      "[[152.   1.   1.   1.]]\n",
      "[[188.   1.   1.   1.]]\n",
      "[[54.  1.  0.  1.]]\n",
      "[[98.  1.  1.  1.]]\n",
      "[[237.   1.   1.   0.]]\n",
      "[[34.  1.  0.  0.]]\n",
      "[[275.   1.   1.   0.]]\n",
      "[[300.  79.   1.   1.]]\n",
      "[[209.   1.   1.   0.]]\n",
      "[[270.   1.   0.   0.]]\n",
      "[[247.   1.   1.   1.]]\n",
      "[[93.  1.  0.  1.]]\n",
      "[[196.   1.   0.   1.]]\n",
      "[[20.  1.  0.  1.]]\n",
      "[[265. 109.   1.   0.]]\n",
      "[[138.   1.   0.   0.]]\n",
      "[[165.   1.   1.   1.]]\n",
      "[[73.  1.  0.  1.]]\n",
      "[[137.   1.   1.   1.]]\n",
      "[[136.  82.   1.   1.]]\n",
      "[[47.  1.  1.  1.]]\n",
      "[[202.   1.   0.   0.]]\n",
      "[[270.   1.   1.   1.]]\n",
      "[[87.  1.  1.  1.]]\n",
      "[[243.   1.   1.   0.]]\n",
      "[[208.   1.   0.   1.]]\n",
      "[[118.   1.   1.   0.]]\n",
      "[[86.  1.  0.  0.]]\n",
      "[[188.   1.   0.   0.]]\n",
      "[[232.   1.   0.   0.]]\n",
      "[[107.   1.   1.   0.]]\n",
      "[[4. 1. 0. 1.]]\n",
      "[[132.   1.   1.   0.]]\n",
      "[[75. 87.  0.  0.]]\n",
      "[[15.  1.  0.  0.]]\n",
      "[[70.  1.  1.  0.]]\n",
      "[[158.   1.   1.   0.]]\n",
      "[[29. 91.  1.  1.]]\n",
      "[[172.   1.   1.   1.]]\n",
      "[[37.  1.  1.  1.]]\n",
      "[[221.   1.   0.   1.]]\n",
      "[[58.  1.  0.  0.]]\n",
      "[[63.  1.  1.  0.]]\n",
      "[[180.   1.   1.   1.]]\n",
      "[[253.   1.   0.   1.]]\n",
      "[[280.   1.   0.   0.]]\n",
      "[[47.  1.  0.  0.]]\n",
      "[[184.   1.   1.   0.]]\n",
      "[[97.  1.  0.  0.]]\n",
      "[[226.   1.   1.   1.]]\n",
      "[[25.  1.  1.  1.]]\n",
      "[[128.   1.   0.   1.]]\n",
      "[[82.  1.  1.  0.]]\n",
      "[[163.   1.   0.   0.]]\n",
      "[[115.   1.   1.   1.]]\n",
      "[[280.   1.   1.   1.]]\n",
      "[[29.  1.  0.  1.]]\n",
      "[[199.   1.   1.   0.]]\n",
      "[[265.   1.   0.   1.]]\n",
      "[[229.   1.   1.   0.]]\n",
      "[[14.  1.  1.  1.]]\n",
      "[[126.   1.   0.   0.]]\n",
      "[[142.   1.   0.   0.]]\n",
      "[[62.  1.  0.  1.]]\n",
      "[[245.   1.   0.   1.]]\n",
      "[[78.  1.  1.  1.]]\n",
      "[[261.   1.   1.   1.]]\n",
      "[[9. 1. 0. 0.]]\n",
      "[[261.   1.   0.   0.]]\n",
      "[[206.   1.   0.   0.]]\n",
      "[[170.   1.   0.   0.]]\n",
      "[[199.   1.   1.   1.]]\n",
      "[[190.  53.   0.   0.]]\n",
      "[[177.   1.   0.   1.]]\n",
      "[[285.   1.   1.   0.]]\n",
      "[[180.  11.   1.   1.]]\n",
      "[[136.  54.   0.   0.]]\n",
      "[[104.   1.   1.   1.]]\n",
      "[[78.  1.  0.  0.]]\n",
      "[[44.  1.  1.  0.]]\n",
      "[[112.   1.   1.   0.]]\n",
      "[[235.   1.   0.   1.]]\n",
      "[[284.   1.   0.   1.]]\n",
      "[[254.   1.   1.   1.]]\n",
      "[[55.  1.  1.  0.]]\n",
      "[[229.   1.   0.   1.]]\n",
      "[[162.   1.   1.   0.]]\n",
      "[[240.   1.   0.   0.]]\n",
      "[[146.   1.   0.   0.]]\n",
      "[[138. 107.   1.   0.]]\n",
      "[[180.   1.   0.   0.]]\n",
      "[[107.   1.   0.   1.]]\n",
      "[[89.  1.  0.  1.]]\n",
      "[[154.   1.   0.   1.]]\n",
      "[[174.   1.   0.   0.]]\n",
      "[[116.   1.   0.   1.]]\n",
      "[[125.   1.   0.   1.]]\n",
      "[[29.  1.  1.  0.]]\n",
      "[[284.  68.   1.   0.]]\n",
      "[[239.   1.   1.   1.]]\n",
      "[[148.   1.   1.   1.]]\n",
      "[[143.   1.   1.   1.]]\n",
      "[[288.   1.   0.   0.]]\n",
      "[[127.   1.   1.   1.]]\n",
      "[[37.  1.  0.  1.]]\n",
      "[[218.   1.   1.   1.]]\n",
      "[[2. 1. 1. 0.]]\n",
      "[[110.   1.   0.   0.]]\n",
      "[[22.  1.  1.  0.]]\n",
      "[[68.  1.  0.  0.]]\n",
      "[[4. 1. 1. 1.]]\n",
      "[[15.  1.  0.  1.]]\n",
      "[[84. 50.  0.  1.]]\n",
      "[[242.   1.   1.   1.]]\n",
      "[[290.   2.   1.   0.]]\n",
      "[[168.   1.   0.   1.]]\n",
      "[[11.  1.  1.  0.]]\n",
      "[[130.   1.   0.   0.]]\n",
      "[[ 82. 109.   1.   0.]]\n",
      "[[211.   1.   0.   0.]]\n",
      "[[51.  1.  0.  1.]]\n",
      "[[73.  1.  1.  1.]]\n",
      "[[21.  1.  1.  1.]]\n",
      "[[8. 1. 0. 1.]]\n",
      "[[191.   1.   0.   1.]]\n",
      "[[96.  1.  1.  1.]]\n",
      "[[101.   1.   1.   0.]]\n",
      "[[270.   1.   1.   0.]]\n",
      "[[266.   1.   1.   1.]]\n",
      "[[257.   1.   1.   1.]]\n",
      "[[233.  87.   1.   0.]]\n",
      "[[203.   1.   1.   0.]]\n",
      "[[223.   1.   1.   1.]]\n",
      "[[250.   1.   0.   1.]]\n",
      "[[42.  1.  0.  0.]]\n",
      "[[156.   1.   1.   1.]]\n",
      "[[214.   1.   1.   0.]]\n",
      "[[89.  1.  1.  0.]]\n",
      "[[175.   1.   1.   1.]]\n",
      "[[139.   1.   1.   0.]]\n",
      "[[153.   1.   1.   0.]]\n",
      "[[140.   1.   0.   1.]]\n",
      "[[73.  1.  0.  0.]]\n",
      "[[197.   1.   0.   0.]]\n",
      "[[196.   1.   1.   1.]]\n",
      "[[171.   1.   1.   0.]]\n",
      "[[ 24. 109.   1.   0.]]\n",
      "[[258.   1.   1.   0.]]\n",
      "[[42.  1.  1.  1.]]\n",
      "[[196.  87.   1.   0.]]\n",
      "[[45.  1.  0.  1.]]\n",
      "[[119.   1.   1.   1.]]\n",
      "[[32.  1.  0.  0.]]\n",
      "[[250.   1.   1.   0.]]\n",
      "[[20.  3.  0.  1.]]\n",
      "[[ 1. 48.  1.  0.]]\n",
      "[[78. 14.  1.  0.]]\n",
      "[[90.  1.  1.  1.]]\n",
      "[[161.   1.   1.   1.]]\n",
      "[[157.   1.   0.   0.]]\n",
      "[[280.  92.   0.   0.]]\n",
      "[[101.   1.   0.   1.]]\n",
      "[[217.   1.   0.   1.]]\n",
      "[[92.  1.  0.  0.]]\n",
      "[[ 6. 90.  0.  0.]]\n",
      "[[225.   1.   1.   0.]]\n",
      "[[54.  1.  1.  1.]]\n",
      "[[25.  1.  0.  1.]]\n",
      "[[186.   1.   0.   1.]]\n",
      "[[83.  1.  1.  1.]]\n",
      "[[277.   1.   1.   1.]]\n",
      "[[53.  1.  0.  0.]]\n",
      "[[133.   1.   1.   1.]]\n",
      "[[118.   1.   0.   0.]]\n",
      "[[273.   1.   0.   0.]]\n",
      "[[279.   1.   1.   0.]]\n",
      "[[58.  1.  0.  1.]]\n",
      "[[66.  1.  0.  1.]]\n",
      "[[254.   1.   0.   0.]]\n",
      "[[184.   1.   1.   1.]]\n",
      "[[219.  49.   0.   0.]]\n",
      "[[6. 1. 0. 0.]]\n",
      "[[286.   1.   1.   1.]]\n",
      "[[112.   1.   0.   1.]]\n",
      "[[64.  1.  1.  1.]]\n",
      "[[21.  1.  0.  0.]]\n",
      "[[33.  1.  1.  0.]]\n",
      "[[204.   1.   0.   1.]]\n",
      "[[235.   1.   1.   1.]]\n",
      "[[283.   1.   1.   0.]]\n",
      "[[110.  45.   1.   1.]]\n",
      "[[236.   1.   0.   0.]]\n",
      "[[154.  96.   0.   0.]]\n",
      "[[288.   4.   1.   1.]]\n",
      "[[300.  73.   0.   0.]]\n",
      "[[229.  16.   0.   0.]]\n",
      "[[246.  58.   0.   1.]]\n",
      "[[113.   9.   1.   1.]]\n",
      "[[300.   4.   0.   0.]]\n",
      "[[51. 82.  0.  0.]]\n",
      "[[39. 42.  0.  1.]]\n",
      "[[154.  62.   1.   0.]]\n",
      "[[115.  89.   0.   0.]]\n",
      "[[211.   6.   0.   1.]]\n",
      "[[241.  35.   0.   0.]]\n",
      "[[198.  37.   1.   0.]]\n",
      "[[63. 39.  0.  0.]]\n",
      "[[266.  56.   0.   0.]]\n",
      "[[177.  94.   1.   0.]]\n",
      "[[289.   1.   1.   0.]]\n",
      "[[158.   8.   1.   0.]]\n",
      "[[247. 109.   1.   1.]]\n",
      "[[76. 68.  1.  0.]]\n",
      "[[94. 91.  1.  1.]]\n",
      "[[243.   7.   1.   0.]]\n",
      "[[214.  96.   1.   1.]]\n",
      "[[122.  66.   0.   1.]]\n",
      "[[222.  33.   1.   1.]]\n",
      "[[173.  52.   1.   1.]]\n",
      "[[131.  36.   0.   1.]]\n",
      "[[19. 41.  0.  1.]]\n",
      "[[190.  70.   1.   0.]]\n",
      "[[300.  36.   0.   1.]]\n",
      "[[280. 109.   1.   1.]]\n",
      "[[235.  68.   0.   0.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[62.  5.  0.  0.]]\n",
      "[[92.  7.  1.  1.]]\n",
      "[[84. 33.  1.  0.]]\n",
      "[[39. 25.  1.  0.]]\n",
      "[[18. 76.  0.  1.]]\n",
      "[[194.   1.   0.   0.]]\n",
      "[[123. 109.   1.   1.]]\n",
      "[[181.  34.   0.   1.]]\n",
      "[[122.   1.   1.   0.]]\n",
      "[[76.  1.  0.  1.]]\n",
      "[[290.   2.   0.   0.]]\n",
      "[[96.  1.  1.  0.]]\n",
      "[[39. 73.  1.  1.]]\n",
      "[[70.  1.  0.  1.]]\n",
      "[[282.  49.   0.   1.]]\n",
      "[[1. 6. 1. 0.]]\n",
      "[[278.   1.   0.   1.]]\n",
      "[[1. 4. 0. 0.]]\n",
      "[[ 1. 12.  1.  0.]]\n",
      "[[262.  38.   1.   1.]]\n",
      "[[291.   2.   1.   0.]]\n",
      "[[ 1. 34.  1.  1.]]\n",
      "[[40.  7.  1.  1.]]\n",
      "[[154.  79.   1.   1.]]\n",
      "[[249.  92.   0.   0.]]\n",
      "[[180. 109.   1.   0.]]\n",
      "[[ 68. 109.   0.   1.]]\n",
      "[[288.   1.   0.   1.]]\n",
      "[[ 38. 109.   0.   0.]]\n",
      "[[ 9. 61.  1.  1.]]\n",
      "[[212. 109.   1.   1.]]\n",
      "[[300.  94.   1.   0.]]\n",
      "[[52. 30.  0.  0.]]\n",
      "[[205.  57.   1.   1.]]\n",
      "[[211.   1.   1.   1.]]\n",
      "[[98. 56.  0.  0.]]\n",
      "[[250.  73.   1.   1.]]\n",
      "[[117.  30.   1.   1.]]\n",
      "[[277.   1.   0.   0.]]\n",
      "[[129.   5.   1.   1.]]\n",
      "[[146.  31.   0.   0.]]\n",
      "[[193.   5.   0.   0.]]\n",
      "[[6. 1. 1. 1.]]\n",
      "[[267.   1.   0.   0.]]\n",
      "[[135.   1.   1.   0.]]\n",
      "[[234.  49.   1.   1.]]\n",
      "[[60. 94.  1.  1.]]\n",
      "[[37.  1.  0.  0.]]\n",
      "[[213.   1.   0.   1.]]\n",
      "[[88. 76.  0.  1.]]\n",
      "[[163.   1.   0.   1.]]\n",
      "[[63. 74.  0.  1.]]\n",
      "[[97. 39.  1.  0.]]\n",
      "[[284.   1.   1.   1.]]\n",
      "[[224.   1.   0.   1.]]\n",
      "[[169.   1.   1.   1.]]\n",
      "[[69. 53.  1.  0.]]\n",
      "[[222.   1.   0.   0.]]\n",
      "[[1. 7. 1. 1.]]\n",
      "[[19. 27.  1.  0.]]\n",
      "[[279.  34.   0.   0.]]\n",
      "[[270.  71.   0.   1.]]\n",
      "[[139.  68.   0.   0.]]\n",
      "[[ 95. 109.   1.   0.]]\n",
      "[[122.  51.   1.   0.]]\n",
      "[[151. 109.   1.   0.]]\n",
      "[[168.  26.   0.   0.]]\n",
      "[[130.  95.   0.   0.]]\n",
      "[[70. 27.  0.  1.]]\n",
      "[[44. 94.  1.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[1. 1. 1. 0.]]\n",
      "[[81.  1.  1.  0.]]\n",
      "[[52.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[70.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[147.  46.   1.   1.]]\n",
      "[[41. 57.  0.  1.]]\n",
      "[[149.   1.   1.   0.]]\n",
      "[[256.   1.   0.   1.]]\n",
      "[[209.  31.   0.   1.]]\n",
      "[[207.   1.   1.   1.]]\n",
      "[[187.   1.   1.   0.]]\n",
      "[[160.   1.   0.   0.]]\n",
      "[[123.   1.   0.   0.]]\n",
      "[[251.  46.   0.   0.]]\n",
      "[[136.   1.   0.   1.]]\n",
      "[[265.  95.   1.   1.]]\n",
      "[[18.  1.  0.  0.]]\n",
      "[[220.  83.   0.   0.]]\n",
      "[[28.  1.  1.  1.]]\n",
      "[[234.   1.   1.   0.]]\n",
      "[[223.  62.   1.   1.]]\n",
      "[[182.  81.   0.   1.]]\n",
      "[[130.   1.   1.   1.]]\n",
      "[[ 14. 101.   0.   1.]]\n",
      "[[185.   1.   0.   0.]]\n",
      "[[48.  1.  0.  1.]]\n",
      "[[165.  87.   1.   0.]]\n",
      "[[149.   1.   0.   1.]]\n",
      "[[285.  81.   0.   1.]]\n",
      "[[181.   1.   1.   0.]]\n",
      "[[190.   1.   1.   0.]]\n",
      "[[100.   1.   0.   0.]]\n",
      "[[121.  78.   1.   1.]]\n",
      "[[273.   1.   1.   1.]]\n",
      "[[85.  1.  0.  1.]]\n",
      "[[193.   1.   1.   1.]]\n",
      "[[174.   1.   1.   0.]]\n",
      "[[178.   1.   1.   1.]]\n",
      "[[201.   1.   1.   1.]]\n",
      "[[39.  1.  0.  1.]]\n",
      "[[246.   1.   0.   0.]]\n",
      "[[101.   1.   1.   1.]]\n",
      "[[10.  1.  1.  1.]]\n",
      "[[228.   1.   0.   0.]]\n",
      "[[264.   1.   0.   0.]]\n",
      "[[257.   1.   0.   0.]]\n",
      "[[23.  1.  0.  1.]]\n",
      "[[131.   1.   0.   1.]]\n",
      "[[61.  1.  1.  1.]]\n",
      "[[123.   1.   1.   1.]]\n",
      "[[66.  1.  1.  0.]]\n",
      "[[112.   1.   1.   1.]]\n",
      "[[202.  76.   0.   1.]]\n",
      "[[243.   1.   0.   0.]]\n",
      "[[218.   1.   0.   0.]]\n",
      "[[285.   1.   0.   0.]]\n",
      "[[134.   1.   0.   0.]]\n",
      "[[18.  1.  1.  1.]]\n",
      "[[181.   1.   0.   1.]]\n",
      "[[182.   1.   0.   0.]]\n",
      "[[33.  1.  0.  1.]]\n",
      "[[262.   1.   1.   0.]]\n",
      "[[216.   1.   1.   0.]]\n",
      "[[196.   1.   1.   0.]]\n",
      "[[52. 46.  1.  1.]]\n",
      "[[85.  1.  1.  0.]]\n",
      "[[179.  63.   0.   0.]]\n",
      "[[48.  1.  1.  0.]]\n",
      "[[200.   1.   0.   1.]]\n",
      "[[128.   1.   1.   0.]]\n",
      "[[111.  60.   1.   1.]]\n",
      "[[ 1. 63.  0.  0.]]\n",
      "[[96.  1.  0.  1.]]\n",
      "[[3. 1. 0. 0.]]\n",
      "[[252.   1.   1.   0.]]\n",
      "[[61.  1.  0.  0.]]\n",
      "[[237.  99.   0.   1.]]\n",
      "[[262.   1.   0.   1.]]\n",
      "[[165.   1.   1.   0.]]\n",
      "[[231.   1.   1.   0.]]\n",
      "[[83.  1.  0.  0.]]\n",
      "[[264.   1.   1.   1.]]\n",
      "[[246.   1.   1.   0.]]\n",
      "[[19.  1.  1.  0.]]\n",
      "[[28.  1.  0.  0.]]\n",
      "[[251.   1.   1.   1.]]\n",
      "[[28. 49.  1.  1.]]\n",
      "[[104.   1.   0.   1.]]\n",
      "[[106.   1.   0.   0.]]\n",
      "[[29. 34.  1.  0.]]\n",
      "[[44.  1.  0.  0.]]\n",
      "[[272.  44.   0.   0.]]\n",
      "[[81. 97.  1.  1.]]\n",
      "[[251.   1.   0.   0.]]\n",
      "[[190.  98.   0.   1.]]\n",
      "[[26.  1.  1.  0.]]\n",
      "[[143.   1.   1.   0.]]\n",
      "[[190.  26.   1.   0.]]\n",
      "[[153.   1.   0.   0.]]\n",
      "[[166.   1.   0.   0.]]\n",
      "[[94.  1.  0.  0.]]\n",
      "[[238.   1.   0.   1.]]\n",
      "[[59.  1.  1.  0.]]\n",
      "[[206.   1.   1.   0.]]\n",
      "[[247.   1.   0.   1.]]\n",
      "[[75.  1.  1.  1.]]\n",
      "[[88. 63.  0.  1.]]\n",
      "[[191.   1.   0.   0.]]\n",
      "[[269.   1.   0.   1.]]\n",
      "[[140.   1.   1.   1.]]\n",
      "[[44.  1.  1.  1.]]\n",
      "[[105.  98.   1.   1.]]\n",
      "[[10. 33.  1.  0.]]\n",
      "[[212.   1.   1.   0.]]\n",
      "[[79.  1.  0.  1.]]\n",
      "[[108.   1.   0.   0.]]\n",
      "[[151.   1.   0.   1.]]\n",
      "[[36.  1.  1.  0.]]\n",
      "[[254.  27.   0.   0.]]\n",
      "[[ 1. 98.  1.  0.]]\n",
      "[[3. 6. 1. 0.]]\n",
      "[[227.   4.   1.   1.]]\n",
      "[[234.  25.   0.   1.]]\n",
      "[[104.  83.   0.   0.]]\n",
      "[[116.   1.   1.   0.]]\n",
      "[[143.  93.   1.   1.]]\n",
      "[[230.   1.   0.   0.]]\n",
      "[[272.   1.   0.   1.]]\n",
      "[[258.  64.   1.   0.]]\n",
      "[[290.  41.   0.   0.]]\n",
      "[[158.   1.   1.   1.]]\n",
      "[[50.  1.  1.  1.]]\n",
      "[[300.  25.   1.   0.]]\n",
      "[[161.  53.   0.   0.]]\n",
      "[[241.   1.   1.   0.]]\n",
      "[[171.   4.   0.   0.]]\n",
      "[[146.   4.   0.   1.]]\n",
      "[[134.  24.   1.   0.]]\n",
      "[[290. 101.   0.   0.]]\n",
      "[[268.   1.   1.   0.]]\n",
      "[[203.  98.   0.   0.]]\n",
      "[[75. 40.  1.  1.]]\n",
      "[[232.   1.   0.   1.]]\n",
      "[[155.   1.   1.   0.]]\n",
      "[[145.   1.   1.   1.]]\n",
      "[[13.  1.  0.  0.]]\n",
      "[[14.  1.  1.  0.]]\n",
      "[[30. 80.  0.  0.]]\n",
      "[[110.   1.   0.   1.]]\n",
      "[[219.  23.   1.   0.]]\n",
      "[[16.  1.  1.  1.]]\n",
      "[[174.   1.   0.   1.]]\n",
      "[[155.  24.   1.   1.]]\n",
      "[[147.   1.   0.   1.]]\n",
      "[[177.   1.   0.   0.]]\n",
      "[[207.  45.   0.   0.]]\n",
      "[[16. 52.  0.  0.]]\n",
      "[[151.   1.   1.   0.]]\n",
      "[[204.   1.   0.   0.]]\n",
      "[[225.  97.   0.   0.]]\n",
      "[[ 11. 109.   1.   0.]]\n",
      "[[222.   1.   1.   0.]]\n",
      "[[99.  1.  0.  1.]]\n",
      "[[168.   1.   0.   0.]]\n",
      "[[73.  1.  1.  0.]]\n",
      "[[193.   1.   0.   1.]]\n",
      "[[191.   1.   1.   1.]]\n",
      "[[57.  1.  1.  1.]]\n",
      "[[35.  1.  1.  1.]]\n",
      "[[206.   1.   0.   1.]]\n",
      "[[186.   1.   1.   1.]]\n",
      "[[155.   1.   0.   0.]]\n",
      "[[291.  60.   0.   1.]]\n",
      "[[104.   1.   1.   0.]]\n",
      "[[189.   1.   0.   1.]]\n",
      "[[18. 88.  1.  0.]]\n",
      "[[89.  1.  0.  0.]]\n",
      "[[160.   1.   1.   0.]]\n",
      "[[171.  40.   0.   0.]]\n",
      "[[220.   1.   1.   1.]]\n",
      "[[29. 18.  1.  0.]]\n",
      "[[239.   1.   1.   0.]]\n",
      "[[17.  1.  0.  1.]]\n",
      "[[51. 70.  0.  1.]]\n",
      "[[107.  34.   0.   1.]]\n",
      "[[230.   1.   1.   1.]]\n",
      "[[165.   1.   0.   1.]]\n",
      "[[41.  1.  1.  0.]]\n",
      "[[71.  1.  0.  0.]]\n",
      "[[245.   1.   1.   1.]]\n",
      "[[120.   1.   0.   0.]]\n",
      "[[56.  1.  0.  0.]]\n",
      "[[274.  82.   1.   0.]]\n",
      "[[87.  1.  0.  1.]]\n",
      "[[267.  28.   0.   0.]]\n",
      "[[93.  1.  1.  1.]]\n",
      "[[113.   1.   0.   0.]]\n",
      "[[142.   1.   0.   1.]]\n",
      "[[227.  75.   0.   1.]]\n",
      "[[255. 102.   1.   1.]]\n",
      "[[210.   1.   0.   1.]]\n",
      "[[86. 23.  0.  1.]]\n",
      "[[114.   1.   1.   0.]]\n",
      "[[161.   1.   0.   1.]]\n",
      "[[11.  1.  0.  0.]]\n",
      "[[242.  80.   0.   0.]]\n",
      "[[80.  1.  1.  1.]]\n",
      "[[1. 1. 0. 1.]]\n",
      "[[282.   1.   0.   0.]]\n",
      "[[167.  99.   0.   1.]]\n",
      "[[81.  1.  0.  0.]]\n",
      "[[166.  63.   1.   1.]]\n",
      "[[2. 1. 0. 1.]]\n",
      "[[209.   1.   1.   1.]]\n",
      "[[118. 100.   1.   0.]]\n",
      "[[28.  4.  1.  0.]]\n",
      "[[ 9. 71.  1.  0.]]\n",
      "[[201.   1.   1.   0.]]\n",
      "[[120.  39.   0.   0.]]\n",
      "[[ 91. 100.   0.   0.]]\n",
      "[[30.  1.  0.  0.]]\n",
      "[[66. 63.  0.  1.]]\n",
      "[[144.   1.   0.   0.]]\n",
      "[[70. 99.  0.  0.]]\n",
      "[[260.   1.   1.   0.]]\n",
      "[[292.  88.   1.   0.]]\n",
      "[[121.   1.   1.   1.]]\n",
      "[[79.  1.  1.  0.]]\n",
      "[[40.  1.  0.  0.]]\n",
      "[[157.   1.   0.   1.]]\n",
      "[[202.  65.   0.   0.]]\n",
      "[[ 31. 100.   0.   0.]]\n",
      "[[ 1. 32.  1.  0.]]\n",
      "[[130.   1.   1.   0.]]\n",
      "[[135.   1.   1.   1.]]\n",
      "[[242.   1.   0.   1.]]\n",
      "[[179.   1.   0.   1.]]\n",
      "[[167.   1.   1.   1.]]\n",
      "[[60.  1.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "## define the domain of the considered parameters\n",
    "n_estimators = tuple(np.arange(1,301,1, dtype= np.int))\n",
    "# print(n_estimators)\n",
    "max_depth = tuple(np.arange(1,110,1, dtype= np.int))\n",
    "# max_features = ('log2', 'sqrt', None)\n",
    "max_features = (0, 1)\n",
    "# criterion = ('gini', 'entropy')\n",
    "criterion = (0, 1)\n",
    "\n",
    "\n",
    "# define the dictionary for GPyOpt\n",
    "domain = [{'n_estimators': 'var_1',  'type': 'discrete',     'domain': n_estimators},\n",
    "          {'max_depth': 'var_2',     'type': 'discrete',     'domain': max_depth},\n",
    "          {'max_features': 'var_3',  'type': 'categorical',  'domain': max_features},\n",
    "          {'criterion': 'var_4',     'type': 'categorical',  'domain': criterion}]\n",
    "\n",
    "\n",
    "## we have to define the function we want to maximize --> validation accuracy, \n",
    "## note it should take a 2D ndarray but it is ok that it assumes only one point\n",
    "## in this setting\n",
    "def objective_function(x): \n",
    "    print(x)\n",
    "    # we have to handle the categorical variables that is convert 0/1 to labels\n",
    "    # log2/sqrt and gini/entropy\n",
    "    \n",
    "    param = x[0]\n",
    "    \n",
    "    if param[2] == 0:\n",
    "        var_3 = \"log2\"\n",
    "    else:\n",
    "        var_3 = \"sqrt\"\n",
    "    \n",
    "    if param[3] == 0:\n",
    "        var_4 = \"gini\"\n",
    "    else:\n",
    "        var_4 = \"entropy\"\n",
    "        \n",
    "        \n",
    "#fit the model\n",
    "    model = RandomForestClassifier(n_estimators = int(param[0]), criterion = var_4, max_depth = int(param[1]), max_features = var_3)\n",
    "    model.fit(Xcattrain, ytrain)\n",
    "    forestPreds = model.predict(Xcattest)\n",
    "    accuracy = len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "opt = GPyOpt.methods.BayesianOptimization(f = objective_function,   # function to optimize\n",
    "                                              domain = domain,         # box-constrains of the problem\n",
    "                                              acquisition_type = \"EI\",      # Select acquisition function MPI, EI, LCB\n",
    "                                             )\n",
    "opt.acquisition.exploration_weight=.1\n",
    "\n",
    "opt.run_optimization(max_iter = 100) \n",
    "\n",
    "\n",
    "x_best = opt.X[np.argmin(opt.Y)]\n",
    "print(\"The best parameters obtained: n_estimators=\" + str(x_best[0]) + \", max_depth=\" + str(x_best[1]) + \", max_features=\" + str(\n",
    "    x_best[2])  + \", criterion=\" + str(\n",
    "    x_best[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
