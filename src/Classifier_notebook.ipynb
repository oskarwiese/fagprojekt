{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import GPyOpt\n",
    "s = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, norm_type):\n",
    "    if norm_type == \"minmax\":\n",
    "        for i in range(data.size()[1]):\n",
    "            data[:,i] = (data[:,i]-data[:,i].min()) / (data[:,i].max()-data[:,i].min())\n",
    "        return data\n",
    "    elif norm_type == \"zscore\":\n",
    "        for i in range(data.size()[1]):\n",
    "            data[:,i] = (data[:,i]-data[:,i].mean()) / (data[:,i].std())\n",
    "        return data\n",
    "    elif norm_type == None:\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(\"Please choose a correct normalization type\")\n",
    "#Xnumtrain = torch.tensor(np.vstack([(Xnumtrain[:,i]-Xnumtrain[:,i].min()) / (Xnumtrain[:,i].max()-Xnumtrain[:,i].min()) for i in range(Xnumtrain.size()[1]) if \"Tue elsker det her\"])).view(-1,2)\n",
    "\n",
    "def dataprep(data, catname, numname, norm_type = \"zscore\"):\n",
    "    for category in categoricals:\n",
    "        data[category] = data[category].astype(\"category\")\n",
    "\n",
    "    catname = []\n",
    "    for i in range(len(categoricals)):\n",
    "        catname.append(data[categoricals[i]].cat.codes.values)\n",
    "    catname = torch.tensor(catname, dtype = torch.int64).T\n",
    "\n",
    "    numname = np.stack([data[col].values for col in numericals], 1)\n",
    "    numname = torch.tensor(numname, dtype=torch.float)\n",
    "\n",
    "\n",
    "    normalize(numname, norm_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/oskarwiese/fagprojekt/master/compas_propublica/compas-scores-two-years.csv\"\n",
    "data = pd.read_csv(url)\n",
    "# Til at se p√• dataen \n",
    "#print(data.head)\n",
    "#print(data.columns)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_plot():\n",
    "    sb.countplot(x = \"score_text\", hue = \"race\", data = data)\n",
    "    plt.show()\n",
    "\n",
    "    sb.countplot(x = \"two_year_recid\", hue = \"race\", data = data)\n",
    "    plt.show()\n",
    "    sb.countplot(x = \"is_recid\", hue = \"race\", data = data)\n",
    "    plt.show()\n",
    "    sb.countplot(x = \"is_violent_recid\", hue = \"race\", data = data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"c_charge_degree\", \"race\", \"age_cat\", \"sex\", \"is_recid\", \"is_violent_recid\", \"c_charge_degree\"] # \"r_charge_degree\"    \"two_year_recid\"\n",
    "numericals = [\"age\", \"priors_count\", \"juv_fel_count\", \"juv_misd_count\"] # \"days_b_screening_arrest\"\n",
    "outputs = [\"score_text\"]\n",
    "\n",
    "# Making the output binary\n",
    "data[outputs] = data[outputs].replace('Low',0)\n",
    "data[outputs] = data[outputs].replace('Medium',1)\n",
    "data[outputs] = data[outputs].replace('High',1)\n",
    "data[outputs] = data[outputs].astype(\"category\")\n",
    "\n",
    "# Changing the categorical values to categories\n",
    "for category in categoricals:\n",
    "    data[category] = data[category].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for pytorch\n",
    "\n",
    "# Converting the categorical values to a tensor\n",
    "Xcat = []\n",
    "for i in range(len(categoricals)):\n",
    "    Xcat.append(data[categoricals[i]].cat.codes.values)\n",
    "Xcat = torch.tensor(Xcat , dtype = torch.int64).T\n",
    "\n",
    "#Converting the numerical values to a tensor\n",
    "Xnum = np.stack([data[col].values for col in numericals], 1)\n",
    "Xnum = torch.tensor(Xnum, dtype=torch.float)\n",
    "\n",
    "# Converting the output to tensor\n",
    "y = torch.tensor(data[outputs].values).flatten()\n",
    "\n",
    "# Calculation of embedding sizes for the categorical values in the format (unique categorical values, embedding size (dimension of encoding))\n",
    "categorical_column_sizes = [len(data[column].cat.categories) for column in categoricals]\n",
    "categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]\n",
    "\n",
    "# Train-test split\n",
    "totalnumber = len(Xnum)\n",
    "testnumber = int(totalnumber * 0.2)\n",
    "\n",
    "Xcattrain = Xcat[:totalnumber - testnumber]\n",
    "Xcattest = Xcat[totalnumber - testnumber:totalnumber]\n",
    "Xnumtrain = Xnum[:totalnumber - testnumber]\n",
    "Xnumtest = Xnum[totalnumber - testnumber:totalnumber]\n",
    "ytrain = y[:totalnumber - testnumber]\n",
    "ytest = y[totalnumber - testnumber:totalnumber]\n",
    "\n",
    "\n",
    "# Make sure that we dont validate on training data to compare if the algorithm is biased\n",
    "\n",
    "\n",
    "normalize(Xnumtrain, \"zscore\");\n",
    "normalize(Xnumtest, \"zscore\");\n",
    "\n",
    "df = data[totalnumber - testnumber:totalnumber]\n",
    "black_data = df[df[\"race\"]==\"African-American\"]\n",
    "white_data = df[df[\"race\"]==\"Caucasian\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols + num_numerical_cols\n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = torch.cat([x, x_numerical], 1)\n",
    "        x = self.layers(x)\n",
    "        return nn.functional.softmax(x, dim = -1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(2, 1)\n",
      "    (1): Embedding(6, 3)\n",
      "    (2): Embedding(3, 2)\n",
      "    (3): Embedding(2, 1)\n",
      "    (4): Embedding(2, 1)\n",
      "    (5): Embedding(2, 1)\n",
      "    (6): Embedding(2, 1)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (batch_norm_num): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=14, out_features=16, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "    (16): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.5, inplace=False)\n",
      "    (20): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define and show the model\n",
    "model = Model(categorical_embedding_sizes, 4, 2, [16,32,64,128,64], p=0.5)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimization\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 0.59362102\n",
      "epoch:  26 loss: 0.58039081\n",
      "epoch:  51 loss: 0.57079703\n",
      "epoch:  76 loss: 0.56935853\n",
      "epoch: 101 loss: 0.56094056\n",
      "epoch: 126 loss: 0.55647403\n",
      "epoch: 151 loss: 0.55608046\n",
      "epoch: 176 loss: 0.55893248\n",
      "epoch: 201 loss: 0.55833155\n",
      "epoch: 226 loss: 0.55334526\n",
      "epoch: 251 loss: 0.54979426\n",
      "epoch: 276 loss: 0.54961973\n",
      "epoch: 300 loss: 0.5516906977\n",
      "[[635 135]\n",
      " [229 443]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.82      0.78       770\n",
      "           1       0.77      0.66      0.71       672\n",
      "\n",
      "    accuracy                           0.75      1442\n",
      "   macro avg       0.75      0.74      0.74      1442\n",
      "weighted avg       0.75      0.75      0.75      1442\n",
      "\n",
      "0.7475728155339806\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 300\n",
    "aggregated_losses = []\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(Xcattrain, Xnumtrain)\n",
    "    single_loss = loss_function(y_pred, ytrain)\n",
    "    aggregated_losses.append(single_loss)\n",
    "    \n",
    "    if i % 25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "    \n",
    "\n",
    "\n",
    "# Plot the loss over epocs\n",
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val = model(Xcattest, Xnumtest)\n",
    "    loss = loss_function(y_val, ytest)\n",
    "\n",
    "\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "\n",
    "print(confusion_matrix(ytest,y_val))\n",
    "print(classification_report(ytest,y_val))\n",
    "print(accuracy_score(ytest, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep black and white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categoricals:\n",
    "    white_data[category] = white_data[category].astype(\"category\")\n",
    "    black_data[category] = black_data[category].astype(\"category\")\n",
    "\n",
    "Xcat_white, Xcat_black = [] ,[]\n",
    "for i in range(len(categoricals)):\n",
    "    Xcat_white.append(white_data[categoricals[i]].cat.codes.values)\n",
    "    Xcat_black.append(black_data[categoricals[i]].cat.codes.values)\n",
    "Xcat_white = torch.tensor(Xcat_white , dtype = torch.int64).T\n",
    "Xcat_black = torch.tensor(Xcat_black , dtype = torch.int64).T \n",
    "\n",
    "Xnum_white = np.stack([white_data[col].values for col in numericals], 1)\n",
    "Xnum_white = torch.tensor(Xnum_white, dtype=torch.float)\n",
    "Xnum_black = np.stack([black_data[col].values for col in numericals], 1)\n",
    "Xnum_black = torch.tensor(Xnum_black, dtype=torch.float)\n",
    "\n",
    "\n",
    "normalize(Xnum_white, \"zscore\");\n",
    "normalize(Xnum_black, \"zscore\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix for black/white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confussion matrix for the white:\n",
      "[[217  80]\n",
      " [ 63 115]]\n",
      "[[0.73063973 0.4494382 ]\n",
      " [0.21212121 0.64606742]]\n",
      "\n",
      "Confussion matrix for the black:\n",
      "[[275  74]\n",
      " [175 220]]\n",
      "[[0.78796562 0.18734177]\n",
      " [0.50143266 0.55696203]]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_val_white = model(Xcat_white, Xnum_white)\n",
    "y_val_white = np.argmax(y_val_white.detach().numpy(), axis = 1)\n",
    "y_val_black = model(Xcat_black, Xnum_black)\n",
    "y_val_black = np.argmax(y_val_black.detach().numpy(), axis = 1)\n",
    "\n",
    "# Ground truth of recidivism from dataset\n",
    "y_white = torch.tensor(white_data[\"two_year_recid\"].values).flatten()\n",
    "y_black = torch.tensor(black_data[\"two_year_recid\"].values).flatten()\n",
    "\n",
    "print(\"Confussion matrix for the white:\")\n",
    "conf_white = confusion_matrix( y_white, y_val_white)\n",
    "print(conf_white)\n",
    "print(conf_white / conf_white.astype(np.float).sum(axis=1))\n",
    "print()\n",
    "print(\"Confussion matrix for the black:\")\n",
    "conf_black = confusion_matrix( y_black, y_val_black)\n",
    "print(conf_black)\n",
    "print(conf_black / conf_black.astype(np.float).sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and fit it to the data\n",
    "def RandomForest(datatrain, datatest, ytrain, ytest, n_estimators = 53, max_depth = 14, max_features = \"log2\", criterion = \"entropy\", show_acc = True):\n",
    "    forestModel = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, max_features = max_features, criterion = criterion)\n",
    "    forestModel.fit(datatrain, ytrain)\n",
    "\n",
    "    # Predict on the test set\n",
    "    forestPreds = forestModel.predict(datatest)\n",
    "\n",
    "    forestProbs = forestModel.predict_proba(datatest)[:, 1]\n",
    "\n",
    "    if show_acc:\n",
    "        print(\"Predicted no recidivism: \", len(forestPreds[forestPreds == 0]))\n",
    "        print(\"Predicted recidivism: \", len(forestPreds[forestPreds == 1]))\n",
    "\n",
    "        print(\"Accuracy of the random forest model: \", len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds))\n",
    "\n",
    "RandomForest(Xcattrain, Xcattest, ytrain, ytest, n_estimators = 53, max_depth = 14, max_features = \"log2\", criterion = \"entropy\", show_acc = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baysian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[290.   2.   0.   1.]]\n",
      "[[170.  73.   0.   1.]]\n",
      "[[160.  40.   1.   1.]]\n",
      "[[27. 62.  0.  0.]]\n",
      "[[55. 59.  1.  0.]]\n",
      "[[290.   2.   0.   1.]]\n",
      "[[289.   2.   0.   1.]]\n",
      "[[300.  54.   1.   0.]]\n",
      "[[249.   1.   0.   0.]]\n",
      "[[265.   1.   1.   0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[259.   1.   0.   1.]]\n",
      "[[232.   1.   1.   1.]]\n",
      "[[1. 1. 1. 1.]]\n",
      "[[32.  1.  1.  1.]]\n",
      "[[17.  1.  1.  0.]]\n",
      "[[65.  1.  0.  0.]]\n",
      "[[51.  1.  1.  0.]]\n",
      "[[289.   1.   1.   1.]]\n",
      "[[275.   1.   0.   1.]]\n",
      "[[215.   1.   0.   0.]]\n",
      "[[225.   1.   0.   0.]]\n",
      "[[92.  1.  1.  0.]]\n",
      "[[115.   1.   0.   0.]]\n",
      "[[103.   1.   0.   0.]]\n",
      "[[177.   1.   1.   0.]]\n",
      "[[193.   1.   1.   0.]]\n",
      "[[146.   1.   1.   0.]]\n",
      "[[109.   1.   1.   1.]]\n",
      "[[11.  1.  0.  1.]]\n",
      "[[1. 1. 0. 0.]]\n",
      "[[159.   1.   0.   1.]]\n",
      "[[133.   1.   0.   1.]]\n",
      "[[82.  1.  0.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[68.  1.  1.  1.]]\n",
      "[[184.   1.   0.   1.]]\n",
      "[[150.   1.   0.   0.]]\n",
      "[[204.   1.   1.   1.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[76.  1.  1.  0.]]\n",
      "[[263.   8.   0.   1.]]\n",
      "[[108. 109.   0.   0.]]\n",
      "[[229. 109.   0.   0.]]\n",
      "[[  1. 109.   1.   1.]]\n",
      "[[300. 109.   0.   1.]]\n",
      "[[ 1. 23.  1.  0.]]\n",
      "[[101.  25.   0.   0.]]\n",
      "[[ 53. 109.   0.   1.]]\n",
      "[[300.  13.   0.   1.]]\n",
      "[[201.  21.   1.   0.]]\n",
      "[[164. 109.   1.   1.]]\n",
      "[[51. 16.  0.  1.]]\n",
      "[[262.  82.   1.   0.]]\n",
      "[[139.  13.   0.   0.]]\n",
      "[[103.  71.   1.   1.]]\n",
      "[[125.   1.   1.   0.]]\n",
      "[[213.  72.   1.   1.]]\n",
      "[[167.   1.   1.   0.]]\n",
      "[[42.  1.  0.  1.]]\n",
      "[[39.  1.  1.  0.]]\n",
      "[[25.  1.  0.  0.]]\n",
      "[[255.   1.   1.   0.]]\n",
      "[[ 1. 75.  1.  1.]]\n",
      "[[214.   1.   1.   1.]]\n",
      "[[121.   1.   0.   1.]]\n",
      "[[240.   1.   0.   1.]]\n",
      "[[172.   1.   0.   1.]]\n",
      "[[59.  1.  1.  1.]]\n",
      "[[7. 1. 1. 0.]]\n",
      "[[144.   1.   0.   1.]]\n",
      "[[196. 109.   1.   0.]]\n",
      "[[152.   1.   1.   1.]]\n",
      "[[188.   1.   1.   1.]]\n",
      "[[54.  1.  0.  1.]]\n",
      "[[98.  1.  1.  1.]]\n",
      "[[237.   1.   1.   0.]]\n",
      "[[34.  1.  0.  0.]]\n",
      "[[275.   1.   1.   0.]]\n",
      "[[300.  79.   1.   1.]]\n",
      "[[209.   1.   1.   0.]]\n",
      "[[270.   1.   0.   0.]]\n",
      "[[247.   1.   1.   1.]]\n",
      "[[93.  1.  0.  1.]]\n",
      "[[196.   1.   0.   1.]]\n",
      "[[20.  1.  0.  1.]]\n",
      "[[265. 109.   1.   0.]]\n",
      "[[138.   1.   0.   0.]]\n",
      "[[165.   1.   1.   1.]]\n",
      "[[73.  1.  0.  1.]]\n",
      "[[137.   1.   1.   1.]]\n",
      "[[136.  82.   1.   1.]]\n",
      "[[47.  1.  1.  1.]]\n",
      "[[202.   1.   0.   0.]]\n",
      "[[270.   1.   1.   1.]]\n",
      "[[87.  1.  1.  1.]]\n",
      "[[243.   1.   1.   0.]]\n",
      "[[208.   1.   0.   1.]]\n",
      "[[118.   1.   1.   0.]]\n",
      "[[86.  1.  0.  0.]]\n",
      "[[188.   1.   0.   0.]]\n",
      "[[232.   1.   0.   0.]]\n",
      "[[107.   1.   1.   0.]]\n",
      "[[4. 1. 0. 1.]]\n",
      "[[132.   1.   1.   0.]]\n",
      "[[75. 87.  0.  0.]]\n",
      "[[15.  1.  0.  0.]]\n",
      "[[70.  1.  1.  0.]]\n",
      "[[158.   1.   1.   0.]]\n",
      "[[29. 91.  1.  1.]]\n",
      "[[172.   1.   1.   1.]]\n",
      "[[37.  1.  1.  1.]]\n",
      "[[221.   1.   0.   1.]]\n",
      "[[58.  1.  0.  0.]]\n",
      "[[63.  1.  1.  0.]]\n",
      "[[180.   1.   1.   1.]]\n",
      "[[253.   1.   0.   1.]]\n",
      "[[280.   1.   0.   0.]]\n",
      "[[47.  1.  0.  0.]]\n",
      "[[184.   1.   1.   0.]]\n",
      "[[97.  1.  0.  0.]]\n",
      "[[226.   1.   1.   1.]]\n",
      "[[25.  1.  1.  1.]]\n",
      "[[128.   1.   0.   1.]]\n",
      "[[82.  1.  1.  0.]]\n",
      "[[163.   1.   0.   0.]]\n",
      "[[115.   1.   1.   1.]]\n",
      "[[280.   1.   1.   1.]]\n",
      "[[29.  1.  0.  1.]]\n",
      "[[199.   1.   1.   0.]]\n",
      "[[265.   1.   0.   1.]]\n",
      "[[229.   1.   1.   0.]]\n",
      "[[14.  1.  1.  1.]]\n",
      "[[126.   1.   0.   0.]]\n",
      "[[142.   1.   0.   0.]]\n",
      "[[62.  1.  0.  1.]]\n",
      "[[245.   1.   0.   1.]]\n",
      "[[78.  1.  1.  1.]]\n",
      "[[261.   1.   1.   1.]]\n",
      "[[9. 1. 0. 0.]]\n",
      "[[261.   1.   0.   0.]]\n",
      "[[206.   1.   0.   0.]]\n",
      "[[170.   1.   0.   0.]]\n",
      "[[199.   1.   1.   1.]]\n",
      "[[190.  53.   0.   0.]]\n",
      "[[177.   1.   0.   1.]]\n",
      "[[285.   1.   1.   0.]]\n",
      "[[180.  11.   1.   1.]]\n",
      "[[136.  54.   0.   0.]]\n",
      "[[104.   1.   1.   1.]]\n",
      "[[78.  1.  0.  0.]]\n",
      "[[44.  1.  1.  0.]]\n",
      "[[112.   1.   1.   0.]]\n",
      "[[235.   1.   0.   1.]]\n",
      "[[284.   1.   0.   1.]]\n",
      "[[254.   1.   1.   1.]]\n",
      "[[55.  1.  1.  0.]]\n",
      "[[229.   1.   0.   1.]]\n",
      "[[162.   1.   1.   0.]]\n",
      "[[240.   1.   0.   0.]]\n",
      "[[146.   1.   0.   0.]]\n",
      "[[138. 107.   1.   0.]]\n",
      "[[180.   1.   0.   0.]]\n",
      "[[107.   1.   0.   1.]]\n",
      "[[89.  1.  0.  1.]]\n",
      "[[154.   1.   0.   1.]]\n",
      "[[174.   1.   0.   0.]]\n",
      "[[116.   1.   0.   1.]]\n",
      "[[125.   1.   0.   1.]]\n",
      "[[29.  1.  1.  0.]]\n",
      "[[284.  68.   1.   0.]]\n",
      "[[239.   1.   1.   1.]]\n",
      "[[148.   1.   1.   1.]]\n",
      "[[143.   1.   1.   1.]]\n",
      "[[288.   1.   0.   0.]]\n",
      "[[127.   1.   1.   1.]]\n",
      "[[37.  1.  0.  1.]]\n",
      "[[218.   1.   1.   1.]]\n",
      "[[2. 1. 1. 0.]]\n",
      "[[110.   1.   0.   0.]]\n",
      "[[22.  1.  1.  0.]]\n",
      "[[68.  1.  0.  0.]]\n",
      "[[4. 1. 1. 1.]]\n",
      "[[15.  1.  0.  1.]]\n",
      "[[84. 50.  0.  1.]]\n",
      "[[242.   1.   1.   1.]]\n",
      "[[290.   2.   1.   0.]]\n",
      "[[168.   1.   0.   1.]]\n",
      "[[11.  1.  1.  0.]]\n",
      "[[130.   1.   0.   0.]]\n",
      "[[ 82. 109.   1.   0.]]\n",
      "[[211.   1.   0.   0.]]\n",
      "[[51.  1.  0.  1.]]\n",
      "[[73.  1.  1.  1.]]\n",
      "[[21.  1.  1.  1.]]\n",
      "[[8. 1. 0. 1.]]\n",
      "[[191.   1.   0.   1.]]\n",
      "[[96.  1.  1.  1.]]\n",
      "[[101.   1.   1.   0.]]\n",
      "[[270.   1.   1.   0.]]\n",
      "[[266.   1.   1.   1.]]\n",
      "[[257.   1.   1.   1.]]\n",
      "[[233.  87.   1.   0.]]\n",
      "[[203.   1.   1.   0.]]\n",
      "[[223.   1.   1.   1.]]\n",
      "[[250.   1.   0.   1.]]\n",
      "[[42.  1.  0.  0.]]\n",
      "[[156.   1.   1.   1.]]\n",
      "[[214.   1.   1.   0.]]\n",
      "[[89.  1.  1.  0.]]\n",
      "[[175.   1.   1.   1.]]\n",
      "[[139.   1.   1.   0.]]\n",
      "[[153.   1.   1.   0.]]\n",
      "[[140.   1.   0.   1.]]\n",
      "[[73.  1.  0.  0.]]\n",
      "[[197.   1.   0.   0.]]\n",
      "[[196.   1.   1.   1.]]\n",
      "[[171.   1.   1.   0.]]\n",
      "[[ 24. 109.   1.   0.]]\n",
      "[[258.   1.   1.   0.]]\n",
      "[[42.  1.  1.  1.]]\n",
      "[[196.  87.   1.   0.]]\n",
      "[[45.  1.  0.  1.]]\n",
      "[[119.   1.   1.   1.]]\n",
      "[[32.  1.  0.  0.]]\n",
      "[[250.   1.   1.   0.]]\n",
      "[[20.  3.  0.  1.]]\n",
      "[[ 1. 48.  1.  0.]]\n",
      "[[78. 14.  1.  0.]]\n",
      "[[90.  1.  1.  1.]]\n",
      "[[161.   1.   1.   1.]]\n",
      "[[157.   1.   0.   0.]]\n",
      "[[280.  92.   0.   0.]]\n",
      "[[101.   1.   0.   1.]]\n",
      "[[217.   1.   0.   1.]]\n",
      "[[92.  1.  0.  0.]]\n",
      "[[ 6. 90.  0.  0.]]\n",
      "[[225.   1.   1.   0.]]\n",
      "[[54.  1.  1.  1.]]\n",
      "[[25.  1.  0.  1.]]\n",
      "[[186.   1.   0.   1.]]\n",
      "[[83.  1.  1.  1.]]\n",
      "[[277.   1.   1.   1.]]\n",
      "[[53.  1.  0.  0.]]\n",
      "[[133.   1.   1.   1.]]\n",
      "[[118.   1.   0.   0.]]\n",
      "[[273.   1.   0.   0.]]\n",
      "[[279.   1.   1.   0.]]\n",
      "[[58.  1.  0.  1.]]\n",
      "[[66.  1.  0.  1.]]\n",
      "[[254.   1.   0.   0.]]\n",
      "[[184.   1.   1.   1.]]\n",
      "[[219.  49.   0.   0.]]\n",
      "[[6. 1. 0. 0.]]\n",
      "[[286.   1.   1.   1.]]\n",
      "[[112.   1.   0.   1.]]\n",
      "[[64.  1.  1.  1.]]\n",
      "[[21.  1.  0.  0.]]\n",
      "[[33.  1.  1.  0.]]\n",
      "[[204.   1.   0.   1.]]\n",
      "[[235.   1.   1.   1.]]\n",
      "[[283.   1.   1.   0.]]\n",
      "[[110.  45.   1.   1.]]\n",
      "[[236.   1.   0.   0.]]\n",
      "[[154.  96.   0.   0.]]\n",
      "[[288.   4.   1.   1.]]\n",
      "[[300.  73.   0.   0.]]\n",
      "[[229.  16.   0.   0.]]\n",
      "[[246.  58.   0.   1.]]\n",
      "[[113.   9.   1.   1.]]\n",
      "[[300.   4.   0.   0.]]\n",
      "[[51. 82.  0.  0.]]\n",
      "[[39. 42.  0.  1.]]\n",
      "[[154.  62.   1.   0.]]\n",
      "[[115.  89.   0.   0.]]\n",
      "[[211.   6.   0.   1.]]\n",
      "[[241.  35.   0.   0.]]\n",
      "[[198.  37.   1.   0.]]\n",
      "[[63. 39.  0.  0.]]\n",
      "[[266.  56.   0.   0.]]\n",
      "[[177.  94.   1.   0.]]\n",
      "[[289.   1.   1.   0.]]\n",
      "[[158.   8.   1.   0.]]\n",
      "[[247. 109.   1.   1.]]\n",
      "[[76. 68.  1.  0.]]\n",
      "[[94. 91.  1.  1.]]\n",
      "[[243.   7.   1.   0.]]\n",
      "[[214.  96.   1.   1.]]\n",
      "[[122.  66.   0.   1.]]\n",
      "[[222.  33.   1.   1.]]\n",
      "[[173.  52.   1.   1.]]\n",
      "[[131.  36.   0.   1.]]\n",
      "[[19. 41.  0.  1.]]\n",
      "[[190.  70.   1.   0.]]\n",
      "[[300.  36.   0.   1.]]\n",
      "[[280. 109.   1.   1.]]\n",
      "[[235.  68.   0.   0.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[62.  5.  0.  0.]]\n",
      "[[92.  7.  1.  1.]]\n",
      "[[84. 33.  1.  0.]]\n",
      "[[39. 25.  1.  0.]]\n",
      "[[18. 76.  0.  1.]]\n",
      "[[194.   1.   0.   0.]]\n",
      "[[123. 109.   1.   1.]]\n",
      "[[181.  34.   0.   1.]]\n",
      "[[122.   1.   1.   0.]]\n",
      "[[76.  1.  0.  1.]]\n",
      "[[290.   2.   0.   0.]]\n",
      "[[96.  1.  1.  0.]]\n",
      "[[39. 73.  1.  1.]]\n",
      "[[70.  1.  0.  1.]]\n",
      "[[282.  49.   0.   1.]]\n",
      "[[1. 6. 1. 0.]]\n",
      "[[278.   1.   0.   1.]]\n",
      "[[1. 4. 0. 0.]]\n",
      "[[ 1. 12.  1.  0.]]\n",
      "[[262.  38.   1.   1.]]\n",
      "[[291.   2.   1.   0.]]\n",
      "[[ 1. 34.  1.  1.]]\n",
      "[[40.  7.  1.  1.]]\n",
      "[[154.  79.   1.   1.]]\n",
      "[[249.  92.   0.   0.]]\n",
      "[[180. 109.   1.   0.]]\n",
      "[[ 68. 109.   0.   1.]]\n",
      "[[288.   1.   0.   1.]]\n",
      "[[ 38. 109.   0.   0.]]\n",
      "[[ 9. 61.  1.  1.]]\n",
      "[[212. 109.   1.   1.]]\n",
      "[[300.  94.   1.   0.]]\n",
      "[[52. 30.  0.  0.]]\n",
      "[[205.  57.   1.   1.]]\n",
      "[[211.   1.   1.   1.]]\n",
      "[[98. 56.  0.  0.]]\n",
      "[[250.  73.   1.   1.]]\n",
      "[[117.  30.   1.   1.]]\n",
      "[[277.   1.   0.   0.]]\n",
      "[[129.   5.   1.   1.]]\n",
      "[[146.  31.   0.   0.]]\n",
      "[[193.   5.   0.   0.]]\n",
      "[[6. 1. 1. 1.]]\n",
      "[[267.   1.   0.   0.]]\n",
      "[[135.   1.   1.   0.]]\n",
      "[[234.  49.   1.   1.]]\n",
      "[[60. 94.  1.  1.]]\n",
      "[[37.  1.  0.  0.]]\n",
      "[[213.   1.   0.   1.]]\n",
      "[[88. 76.  0.  1.]]\n",
      "[[163.   1.   0.   1.]]\n",
      "[[63. 74.  0.  1.]]\n",
      "[[97. 39.  1.  0.]]\n",
      "[[284.   1.   1.   1.]]\n",
      "[[224.   1.   0.   1.]]\n",
      "[[169.   1.   1.   1.]]\n",
      "[[69. 53.  1.  0.]]\n",
      "[[222.   1.   0.   0.]]\n",
      "[[1. 7. 1. 1.]]\n",
      "[[19. 27.  1.  0.]]\n",
      "[[279.  34.   0.   0.]]\n",
      "[[270.  71.   0.   1.]]\n",
      "[[139.  68.   0.   0.]]\n",
      "[[ 95. 109.   1.   0.]]\n",
      "[[122.  51.   1.   0.]]\n",
      "[[151. 109.   1.   0.]]\n",
      "[[168.  26.   0.   0.]]\n",
      "[[130.  95.   0.   0.]]\n",
      "[[70. 27.  0.  1.]]\n",
      "[[44. 94.  1.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[1. 1. 1. 0.]]\n",
      "[[81.  1.  1.  0.]]\n",
      "[[52.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[70.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[147.  46.   1.   1.]]\n",
      "[[41. 57.  0.  1.]]\n",
      "[[149.   1.   1.   0.]]\n",
      "[[256.   1.   0.   1.]]\n",
      "[[209.  31.   0.   1.]]\n",
      "[[207.   1.   1.   1.]]\n",
      "[[187.   1.   1.   0.]]\n",
      "[[160.   1.   0.   0.]]\n",
      "[[123.   1.   0.   0.]]\n",
      "[[251.  46.   0.   0.]]\n",
      "[[136.   1.   0.   1.]]\n",
      "[[265.  95.   1.   1.]]\n",
      "[[18.  1.  0.  0.]]\n",
      "[[220.  83.   0.   0.]]\n",
      "[[28.  1.  1.  1.]]\n",
      "[[234.   1.   1.   0.]]\n",
      "[[223.  62.   1.   1.]]\n",
      "[[182.  81.   0.   1.]]\n",
      "[[130.   1.   1.   1.]]\n",
      "[[ 14. 101.   0.   1.]]\n",
      "[[185.   1.   0.   0.]]\n",
      "[[48.  1.  0.  1.]]\n",
      "[[165.  87.   1.   0.]]\n",
      "[[149.   1.   0.   1.]]\n",
      "[[285.  81.   0.   1.]]\n",
      "[[181.   1.   1.   0.]]\n",
      "[[190.   1.   1.   0.]]\n",
      "[[100.   1.   0.   0.]]\n",
      "[[121.  78.   1.   1.]]\n",
      "[[273.   1.   1.   1.]]\n",
      "[[85.  1.  0.  1.]]\n",
      "[[193.   1.   1.   1.]]\n",
      "[[174.   1.   1.   0.]]\n",
      "[[178.   1.   1.   1.]]\n",
      "[[201.   1.   1.   1.]]\n",
      "[[39.  1.  0.  1.]]\n",
      "[[246.   1.   0.   0.]]\n",
      "[[101.   1.   1.   1.]]\n",
      "[[10.  1.  1.  1.]]\n",
      "[[228.   1.   0.   0.]]\n",
      "[[264.   1.   0.   0.]]\n",
      "[[257.   1.   0.   0.]]\n",
      "[[23.  1.  0.  1.]]\n",
      "[[131.   1.   0.   1.]]\n",
      "[[61.  1.  1.  1.]]\n",
      "[[123.   1.   1.   1.]]\n",
      "[[66.  1.  1.  0.]]\n",
      "[[112.   1.   1.   1.]]\n",
      "[[202.  76.   0.   1.]]\n",
      "[[243.   1.   0.   0.]]\n",
      "[[218.   1.   0.   0.]]\n",
      "[[285.   1.   0.   0.]]\n",
      "[[134.   1.   0.   0.]]\n",
      "[[18.  1.  1.  1.]]\n",
      "[[181.   1.   0.   1.]]\n",
      "[[182.   1.   0.   0.]]\n",
      "[[33.  1.  0.  1.]]\n",
      "[[262.   1.   1.   0.]]\n",
      "[[216.   1.   1.   0.]]\n",
      "[[196.   1.   1.   0.]]\n",
      "[[52. 46.  1.  1.]]\n",
      "[[85.  1.  1.  0.]]\n",
      "[[179.  63.   0.   0.]]\n",
      "[[48.  1.  1.  0.]]\n",
      "[[200.   1.   0.   1.]]\n",
      "[[128.   1.   1.   0.]]\n",
      "[[111.  60.   1.   1.]]\n",
      "[[ 1. 63.  0.  0.]]\n",
      "[[96.  1.  0.  1.]]\n",
      "[[3. 1. 0. 0.]]\n",
      "[[252.   1.   1.   0.]]\n",
      "[[61.  1.  0.  0.]]\n",
      "[[237.  99.   0.   1.]]\n",
      "[[262.   1.   0.   1.]]\n",
      "[[165.   1.   1.   0.]]\n",
      "[[231.   1.   1.   0.]]\n",
      "[[83.  1.  0.  0.]]\n",
      "[[264.   1.   1.   1.]]\n",
      "[[246.   1.   1.   0.]]\n",
      "[[19.  1.  1.  0.]]\n",
      "[[28.  1.  0.  0.]]\n",
      "[[251.   1.   1.   1.]]\n",
      "[[28. 49.  1.  1.]]\n",
      "[[104.   1.   0.   1.]]\n",
      "[[106.   1.   0.   0.]]\n",
      "[[29. 34.  1.  0.]]\n",
      "[[44.  1.  0.  0.]]\n",
      "[[272.  44.   0.   0.]]\n",
      "[[81. 97.  1.  1.]]\n",
      "[[251.   1.   0.   0.]]\n",
      "[[190.  98.   0.   1.]]\n",
      "[[26.  1.  1.  0.]]\n",
      "[[143.   1.   1.   0.]]\n",
      "[[190.  26.   1.   0.]]\n",
      "[[153.   1.   0.   0.]]\n",
      "[[166.   1.   0.   0.]]\n",
      "[[94.  1.  0.  0.]]\n",
      "[[238.   1.   0.   1.]]\n",
      "[[59.  1.  1.  0.]]\n",
      "[[206.   1.   1.   0.]]\n",
      "[[247.   1.   0.   1.]]\n",
      "[[75.  1.  1.  1.]]\n",
      "[[88. 63.  0.  1.]]\n",
      "[[191.   1.   0.   0.]]\n",
      "[[269.   1.   0.   1.]]\n",
      "[[140.   1.   1.   1.]]\n",
      "[[44.  1.  1.  1.]]\n",
      "[[105.  98.   1.   1.]]\n",
      "[[10. 33.  1.  0.]]\n",
      "[[212.   1.   1.   0.]]\n",
      "[[79.  1.  0.  1.]]\n",
      "[[108.   1.   0.   0.]]\n",
      "[[151.   1.   0.   1.]]\n",
      "[[36.  1.  1.  0.]]\n",
      "[[254.  27.   0.   0.]]\n",
      "[[ 1. 98.  1.  0.]]\n",
      "[[3. 6. 1. 0.]]\n",
      "[[227.   4.   1.   1.]]\n",
      "[[234.  25.   0.   1.]]\n",
      "[[104.  83.   0.   0.]]\n",
      "[[116.   1.   1.   0.]]\n",
      "[[143.  93.   1.   1.]]\n",
      "[[230.   1.   0.   0.]]\n",
      "[[272.   1.   0.   1.]]\n",
      "[[258.  64.   1.   0.]]\n",
      "[[290.  41.   0.   0.]]\n",
      "[[158.   1.   1.   1.]]\n",
      "[[50.  1.  1.  1.]]\n",
      "[[300.  25.   1.   0.]]\n",
      "[[161.  53.   0.   0.]]\n",
      "[[241.   1.   1.   0.]]\n",
      "[[171.   4.   0.   0.]]\n",
      "[[146.   4.   0.   1.]]\n",
      "[[134.  24.   1.   0.]]\n",
      "[[290. 101.   0.   0.]]\n",
      "[[268.   1.   1.   0.]]\n",
      "[[203.  98.   0.   0.]]\n",
      "[[75. 40.  1.  1.]]\n",
      "[[232.   1.   0.   1.]]\n",
      "[[155.   1.   1.   0.]]\n",
      "[[145.   1.   1.   1.]]\n",
      "[[13.  1.  0.  0.]]\n",
      "[[14.  1.  1.  0.]]\n",
      "[[30. 80.  0.  0.]]\n",
      "[[110.   1.   0.   1.]]\n",
      "[[219.  23.   1.   0.]]\n",
      "[[16.  1.  1.  1.]]\n",
      "[[174.   1.   0.   1.]]\n",
      "[[155.  24.   1.   1.]]\n",
      "[[147.   1.   0.   1.]]\n",
      "[[177.   1.   0.   0.]]\n",
      "[[207.  45.   0.   0.]]\n",
      "[[16. 52.  0.  0.]]\n",
      "[[151.   1.   1.   0.]]\n",
      "[[204.   1.   0.   0.]]\n",
      "[[225.  97.   0.   0.]]\n",
      "[[ 11. 109.   1.   0.]]\n",
      "[[222.   1.   1.   0.]]\n",
      "[[99.  1.  0.  1.]]\n",
      "[[168.   1.   0.   0.]]\n",
      "[[73.  1.  1.  0.]]\n",
      "[[193.   1.   0.   1.]]\n",
      "[[191.   1.   1.   1.]]\n",
      "[[57.  1.  1.  1.]]\n",
      "[[35.  1.  1.  1.]]\n",
      "[[206.   1.   0.   1.]]\n",
      "[[186.   1.   1.   1.]]\n",
      "[[155.   1.   0.   0.]]\n",
      "[[291.  60.   0.   1.]]\n",
      "[[104.   1.   1.   0.]]\n",
      "[[189.   1.   0.   1.]]\n",
      "[[18. 88.  1.  0.]]\n",
      "[[89.  1.  0.  0.]]\n",
      "[[160.   1.   1.   0.]]\n",
      "[[171.  40.   0.   0.]]\n",
      "[[220.   1.   1.   1.]]\n",
      "[[29. 18.  1.  0.]]\n",
      "[[239.   1.   1.   0.]]\n",
      "[[17.  1.  0.  1.]]\n",
      "[[51. 70.  0.  1.]]\n",
      "[[107.  34.   0.   1.]]\n",
      "[[230.   1.   1.   1.]]\n",
      "[[165.   1.   0.   1.]]\n",
      "[[41.  1.  1.  0.]]\n",
      "[[71.  1.  0.  0.]]\n",
      "[[245.   1.   1.   1.]]\n",
      "[[120.   1.   0.   0.]]\n",
      "[[56.  1.  0.  0.]]\n",
      "[[274.  82.   1.   0.]]\n",
      "[[87.  1.  0.  1.]]\n",
      "[[267.  28.   0.   0.]]\n",
      "[[93.  1.  1.  1.]]\n",
      "[[113.   1.   0.   0.]]\n",
      "[[142.   1.   0.   1.]]\n",
      "[[227.  75.   0.   1.]]\n",
      "[[255. 102.   1.   1.]]\n",
      "[[210.   1.   0.   1.]]\n",
      "[[86. 23.  0.  1.]]\n",
      "[[114.   1.   1.   0.]]\n",
      "[[161.   1.   0.   1.]]\n",
      "[[11.  1.  0.  0.]]\n",
      "[[242.  80.   0.   0.]]\n",
      "[[80.  1.  1.  1.]]\n",
      "[[1. 1. 0. 1.]]\n",
      "[[282.   1.   0.   0.]]\n",
      "[[167.  99.   0.   1.]]\n",
      "[[81.  1.  0.  0.]]\n",
      "[[166.  63.   1.   1.]]\n",
      "[[2. 1. 0. 1.]]\n",
      "[[209.   1.   1.   1.]]\n",
      "[[118. 100.   1.   0.]]\n",
      "[[28.  4.  1.  0.]]\n",
      "[[ 9. 71.  1.  0.]]\n",
      "[[201.   1.   1.   0.]]\n",
      "[[120.  39.   0.   0.]]\n",
      "[[ 91. 100.   0.   0.]]\n",
      "[[30.  1.  0.  0.]]\n",
      "[[66. 63.  0.  1.]]\n",
      "[[144.   1.   0.   0.]]\n",
      "[[70. 99.  0.  0.]]\n",
      "[[260.   1.   1.   0.]]\n",
      "[[292.  88.   1.   0.]]\n",
      "[[121.   1.   1.   1.]]\n",
      "[[79.  1.  1.  0.]]\n",
      "[[40.  1.  0.  0.]]\n",
      "[[157.   1.   0.   1.]]\n",
      "[[202.  65.   0.   0.]]\n",
      "[[ 31. 100.   0.   0.]]\n",
      "[[ 1. 32.  1.  0.]]\n",
      "[[130.   1.   1.   0.]]\n",
      "[[135.   1.   1.   1.]]\n",
      "[[242.   1.   0.   1.]]\n",
      "[[179.   1.   0.   1.]]\n",
      "[[167.   1.   1.   1.]]\n",
      "[[60.  1.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "## define the domain of the considered parameters\n",
    "n_estimators = tuple(np.arange(1,301,1, dtype= np.int))\n",
    "# print(n_estimators)\n",
    "max_depth = tuple(np.arange(1,110,1, dtype= np.int))\n",
    "# max_features = ('log2', 'sqrt', None)\n",
    "max_features = (0, 1)\n",
    "# criterion = ('gini', 'entropy')\n",
    "criterion = (0, 1)\n",
    "\n",
    "\n",
    "# define the dictionary for GPyOpt\n",
    "domain = [{'n_estimators': 'var_1',  'type': 'discrete',     'domain': n_estimators},\n",
    "          {'max_depth': 'var_2',     'type': 'discrete',     'domain': max_depth},\n",
    "          {'max_features': 'var_3',  'type': 'categorical',  'domain': max_features},\n",
    "          {'criterion': 'var_4',     'type': 'categorical',  'domain': criterion}]\n",
    "\n",
    "\n",
    "## we have to define the function we want to maximize --> validation accuracy, \n",
    "## note it should take a 2D ndarray but it is ok that it assumes only one point\n",
    "## in this setting\n",
    "def objective_function(x): \n",
    "    print(x)\n",
    "    # we have to handle the categorical variables that is convert 0/1 to labels\n",
    "    # log2/sqrt and gini/entropy\n",
    "    \n",
    "    param = x[0]\n",
    "    \n",
    "    if param[2] == 0:\n",
    "        var_3 = \"log2\"\n",
    "    else:\n",
    "        var_3 = \"sqrt\"\n",
    "    \n",
    "    if param[3] == 0:\n",
    "        var_4 = \"gini\"\n",
    "    else:\n",
    "        var_4 = \"entropy\"\n",
    "        \n",
    "        \n",
    "#fit the model\n",
    "    model = RandomForestClassifier(n_estimators = int(param[0]), criterion = var_4, max_depth = int(param[1]), max_features = var_3)\n",
    "    model.fit(Xcattrain, ytrain)\n",
    "    forestPreds = model.predict(Xcattest)\n",
    "    accuracy = len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "opt = GPyOpt.methods.BayesianOptimization(f = objective_function,   # function to optimize\n",
    "                                              domain = domain,         # box-constrains of the problem\n",
    "                                              acquisition_type = \"EI\",      # Select acquisition function MPI, EI, LCB\n",
    "                                             )\n",
    "opt.acquisition.exploration_weight=.1\n",
    "\n",
    "opt.run_optimization(max_iter = 100) \n",
    "\n",
    "\n",
    "x_best = opt.X[np.argmin(opt.Y)]\n",
    "print(\"The best parameters obtained: n_estimators=\" + str(x_best[0]) + \", max_depth=\" + str(x_best[1]) + \", max_features=\" + str(\n",
    "    x_best[2])  + \", criterion=\" + str(\n",
    "    x_best[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
