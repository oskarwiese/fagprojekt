{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import GPyOpt\n",
    "s = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, norm_type):\n",
    "    if norm_type == \"minmax\":\n",
    "        for i in range(data.size()[1]):\n",
    "            data[:,i] = (data[:,i]-data[:,i].min()) / (data[:,i].max()-data[:,i].min())\n",
    "        return data\n",
    "    elif norm_type == \"zscore\":\n",
    "        for i in range(data.size()[1]):\n",
    "            data[:,i] = (data[:,i]-data[:,i].mean()) / (data[:,i].std())\n",
    "        return data\n",
    "    elif norm_type == None:\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(\"Please choose a correct normalization type\")\n",
    "#Xnumtrain = torch.tensor(np.vstack([(Xnumtrain[:,i]-Xnumtrain[:,i].min()) / (Xnumtrain[:,i].max()-Xnumtrain[:,i].min()) for i in range(Xnumtrain.size()[1]) if \"Tue elsker det her\"])).view(-1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/oskarwiese/fagprojekt/master/compas_propublica/compas-scores-two-years.csv\"\n",
    "data = pd.read_csv(url)\n",
    "# Til at se p√• dataen \n",
    "#print(data.head)\n",
    "#print(data.columns)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_plot():\n",
    "    sb.countplot(x = \"score_text\", hue = \"race\", data = data)\n",
    "    plt.show()\n",
    "\n",
    "    sb.countplot(x = \"two_year_recid\", hue = \"race\", data = data)\n",
    "    plt.show()\n",
    "    sb.countplot(x = \"is_recid\", hue = \"race\", data = data)\n",
    "    plt.show()\n",
    "    sb.countplot(x = \"is_violent_recid\", hue = \"race\", data = data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"c_charge_degree\", \"race\", \"age_cat\", \"sex\", \"is_recid\", \"is_violent_recid\", \"c_charge_degree\"] # \"r_charge_degree\"    \"two_year_recid\"\n",
    "numericals = [\"age\", \"priors_count\", \"juv_fel_count\", \"juv_misd_count\"] # \"days_b_screening_arrest\"\n",
    "outputs = [\"score_text\"]\n",
    "\n",
    "# Making the output binary\n",
    "data[outputs] = data[outputs].replace('Low',0)\n",
    "data[outputs] = data[outputs].replace('Medium',1)\n",
    "data[outputs] = data[outputs].replace('High',1)\n",
    "data[outputs] = data[outputs].astype(\"category\")\n",
    "\n",
    "# Changing the categorical values to categories\n",
    "for category in categoricals:\n",
    "    data[category] = data[category].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for pytorch\n",
    "\n",
    "# Converting the categorical values to a tensor\n",
    "Xcat = []\n",
    "for i in range(len(categoricals)):\n",
    "    Xcat.append(data[categoricals[i]].cat.codes.values)\n",
    "Xcat = torch.tensor(Xcat , dtype = torch.int64).T\n",
    "\n",
    "#Converting the numerical values to a tensor\n",
    "Xnum = np.stack([data[col].values for col in numericals], 1)\n",
    "Xnum = torch.tensor(Xnum, dtype=torch.float)\n",
    "\n",
    "# Converting the output to tensor\n",
    "y = torch.tensor(data[outputs].values).flatten()\n",
    "\n",
    "# Calculation of embedding sizes for the categorical values in the format (unique categorical values, embedding size (dimension of encoding))\n",
    "categorical_column_sizes = [len(data[column].cat.categories) for column in categoricals]\n",
    "categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]\n",
    "\n",
    "# Train-test split\n",
    "totalnumber = len(Xnum)\n",
    "testnumber = int(totalnumber * 0.2)\n",
    "\n",
    "Xcattrain = Xcat[:totalnumber - testnumber]\n",
    "Xcattest = Xcat[totalnumber - testnumber:totalnumber]\n",
    "Xnumtrain = Xnum[:totalnumber - testnumber]\n",
    "Xnumtest = Xnum[totalnumber - testnumber:totalnumber]\n",
    "ytrain = y[:totalnumber - testnumber]\n",
    "ytest = y[totalnumber - testnumber:totalnumber]\n",
    "\n",
    "\n",
    "# Make sure that we dont validate on training data to compare if the algorithm is biased\n",
    "df = df[totalnumber - testnumber:totalnumber]\n",
    "black_data = df[df[\"race\"]==\"African-American\"]\n",
    "white_data = df[df[\"race\"]==\"Caucasian\"]\n",
    "\n",
    "normalize(Xnumtrain, \"zscore\");\n",
    "normalize(Xnumtest, \"zscore\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols + num_numerical_cols\n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = torch.cat([x, x_numerical], 1)\n",
    "        x = self.layers(x)\n",
    "        return nn.functional.softmax(x, dim = -1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(2, 1)\n",
      "    (1): Embedding(6, 3)\n",
      "    (2): Embedding(3, 2)\n",
      "    (3): Embedding(2, 1)\n",
      "    (4): Embedding(2, 1)\n",
      "    (5): Embedding(2, 1)\n",
      "    (6): Embedding(2, 1)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (batch_norm_num): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=14, out_features=16, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "    (16): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.5, inplace=False)\n",
      "    (20): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define and show the model\n",
    "model = Model(categorical_embedding_sizes, 4, 2, [16,32,64,128,64], p=0.5)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimization\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 0.70677227\n",
      "epoch:  26 loss: 0.66798919\n",
      "epoch:  51 loss: 0.63298637\n",
      "epoch:  76 loss: 0.61146748\n",
      "epoch: 100 loss: 0.5983138680\n",
      "[[674  96]\n",
      " [304 368]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.88      0.77       770\n",
      "           1       0.79      0.55      0.65       672\n",
      "\n",
      "    accuracy                           0.72      1442\n",
      "   macro avg       0.74      0.71      0.71      1442\n",
      "weighted avg       0.74      0.72      0.71      1442\n",
      "\n",
      "0.7226074895977809\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 100\n",
    "aggregated_losses = []\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(Xcattrain, Xnumtrain)\n",
    "    single_loss = loss_function(y_pred, ytrain)\n",
    "    aggregated_losses.append(single_loss)\n",
    "    \n",
    "    if i % 25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "    \n",
    "\n",
    "\n",
    "# Plot the loss over epocs\n",
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val = model(Xcattest, Xnumtest)\n",
    "    loss = loss_function(y_val, ytest)\n",
    "\n",
    "\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "\n",
    "print(confusion_matrix(ytest,y_val))\n",
    "print(classification_report(ytest,y_val))\n",
    "print(accuracy_score(ytest, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep black and white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confussion matrix for the white:\n",
      "[[276  21]\n",
      " [ 91  87]]\n",
      "[[0.92929293 0.11797753]\n",
      " [0.30639731 0.48876404]]\n",
      "\n",
      "Confussion matrix for the black:\n",
      "[[309  40]\n",
      " [150 245]]\n",
      "[[0.88538682 0.10126582]\n",
      " [0.42979943 0.62025316]]\n"
     ]
    }
   ],
   "source": [
    "for category in categoricals:\n",
    "    white_data[category] = white_data[category].astype(\"category\")\n",
    "    black_data[category] = black_data[category].astype(\"category\")\n",
    "\n",
    "Xcat_white, Xcat_black = [] ,[]\n",
    "for i in range(len(categoricals)):\n",
    "    Xcat_white.append(white_data[categoricals[i]].cat.codes.values)\n",
    "    Xcat_black.append(black_data[categoricals[i]].cat.codes.values)\n",
    "Xcat_white = torch.tensor(Xcat_white , dtype = torch.int64).T\n",
    "Xcat_black = torch.tensor(Xcat_black , dtype = torch.int64).T \n",
    "\n",
    "Xnum_white = np.stack([white_data[col].values for col in numericals], 1)\n",
    "Xnum_white = torch.tensor(Xnum_white, dtype=torch.float)\n",
    "Xnum_black = np.stack([black_data[col].values for col in numericals], 1)\n",
    "Xnum_black = torch.tensor(Xnum_black, dtype=torch.float)\n",
    "\n",
    "\n",
    "normalize(Xnum_white, \"zscore\")\n",
    "normalize(Xnum_black, \"zscore\")\n",
    "model.eval()\n",
    "y_val_white = model(Xcat_white, Xnum_white)\n",
    "y_val_white = np.argmax(y_val_white.detach().numpy(), axis = 1)\n",
    "y_val_black = model(Xcat_black, Xnum_black)\n",
    "y_val_black = np.argmax(y_val_black.detach().numpy(), axis = 1)\n",
    "\n",
    "# Ground truth of recidivism from dataset\n",
    "y_white = torch.tensor(white_data[\"two_year_recid\"].values).flatten()\n",
    "y_black = torch.tensor(black_data[\"two_year_recid\"].values).flatten()\n",
    "\n",
    "print(\"Confussion matrix for the white:\")\n",
    "conf_white = confusion_matrix( y_white, y_val_white)\n",
    "print(conf_white)\n",
    "print(conf_white / conf_white.astype(np.float).sum(axis=1))\n",
    "print()\n",
    "print(\"Confussion matrix for the black:\")\n",
    "conf_black = confusion_matrix( y_black, y_val_black)\n",
    "print(conf_black)\n",
    "print(conf_black / conf_black.astype(np.float).sum(axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and fit it to the data\n",
    "def RandomForest(datatrain, datatest, ytrain, ytest, n_estimators = 53, max_depth = 14, max_features = \"log2\", criterion = \"entropy\", show_acc = True):\n",
    "    forestModel = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, max_features = max_features, criterion = criterion)\n",
    "    forestModel.fit(datatrain, ytrain)\n",
    "\n",
    "    # Predict on the test set\n",
    "    forestPreds = forestModel.predict(datatest)\n",
    "\n",
    "    forestProbs = forestModel.predict_proba(datatest)[:, 1]\n",
    "\n",
    "    if show_acc:\n",
    "        print(\"Predicted no recidivism: \", len(forestPreds[forestPreds == 0]))\n",
    "        print(\"Predicted recidivism: \", len(forestPreds[forestPreds == 1]))\n",
    "\n",
    "        print(\"Accuracy of the random forest model: \", len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds))\n",
    "\n",
    "RandomForest(Xcattrain, Xcattest, ytrain, ytest, n_estimators = 53, max_depth = 14, max_features = \"log2\", criterion = \"entropy\", show_acc = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baysian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[290.   2.   0.   1.]]\n",
      "[[170.  73.   0.   1.]]\n",
      "[[160.  40.   1.   1.]]\n",
      "[[27. 62.  0.  0.]]\n",
      "[[55. 59.  1.  0.]]\n",
      "[[290.   2.   0.   1.]]\n",
      "[[289.   2.   0.   1.]]\n",
      "[[300.  54.   1.   0.]]\n",
      "[[249.   1.   0.   0.]]\n",
      "[[265.   1.   1.   0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[259.   1.   0.   1.]]\n",
      "[[232.   1.   1.   1.]]\n",
      "[[1. 1. 1. 1.]]\n",
      "[[32.  1.  1.  1.]]\n",
      "[[17.  1.  1.  0.]]\n",
      "[[65.  1.  0.  0.]]\n",
      "[[51.  1.  1.  0.]]\n",
      "[[289.   1.   1.   1.]]\n",
      "[[275.   1.   0.   1.]]\n",
      "[[215.   1.   0.   0.]]\n",
      "[[225.   1.   0.   0.]]\n",
      "[[92.  1.  1.  0.]]\n",
      "[[115.   1.   0.   0.]]\n",
      "[[103.   1.   0.   0.]]\n",
      "[[177.   1.   1.   0.]]\n",
      "[[193.   1.   1.   0.]]\n",
      "[[146.   1.   1.   0.]]\n",
      "[[109.   1.   1.   1.]]\n",
      "[[11.  1.  0.  1.]]\n",
      "[[1. 1. 0. 0.]]\n",
      "[[159.   1.   0.   1.]]\n",
      "[[133.   1.   0.   1.]]\n",
      "[[82.  1.  0.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[68.  1.  1.  1.]]\n",
      "[[184.   1.   0.   1.]]\n",
      "[[150.   1.   0.   0.]]\n",
      "[[204.   1.   1.   1.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[76.  1.  1.  0.]]\n",
      "[[263.   8.   0.   1.]]\n",
      "[[108. 109.   0.   0.]]\n",
      "[[229. 109.   0.   0.]]\n",
      "[[  1. 109.   1.   1.]]\n",
      "[[300. 109.   0.   1.]]\n",
      "[[ 1. 23.  1.  0.]]\n",
      "[[101.  25.   0.   0.]]\n",
      "[[ 53. 109.   0.   1.]]\n",
      "[[300.  13.   0.   1.]]\n",
      "[[201.  21.   1.   0.]]\n",
      "[[164. 109.   1.   1.]]\n",
      "[[51. 16.  0.  1.]]\n",
      "[[262.  82.   1.   0.]]\n",
      "[[139.  13.   0.   0.]]\n",
      "[[103.  71.   1.   1.]]\n",
      "[[125.   1.   1.   0.]]\n",
      "[[213.  72.   1.   1.]]\n",
      "[[167.   1.   1.   0.]]\n",
      "[[42.  1.  0.  1.]]\n",
      "[[39.  1.  1.  0.]]\n",
      "[[25.  1.  0.  0.]]\n",
      "[[255.   1.   1.   0.]]\n",
      "[[ 1. 75.  1.  1.]]\n",
      "[[214.   1.   1.   1.]]\n",
      "[[121.   1.   0.   1.]]\n",
      "[[240.   1.   0.   1.]]\n",
      "[[172.   1.   0.   1.]]\n",
      "[[59.  1.  1.  1.]]\n",
      "[[7. 1. 1. 0.]]\n",
      "[[144.   1.   0.   1.]]\n",
      "[[196. 109.   1.   0.]]\n",
      "[[152.   1.   1.   1.]]\n",
      "[[188.   1.   1.   1.]]\n",
      "[[54.  1.  0.  1.]]\n",
      "[[98.  1.  1.  1.]]\n",
      "[[237.   1.   1.   0.]]\n",
      "[[34.  1.  0.  0.]]\n",
      "[[275.   1.   1.   0.]]\n",
      "[[300.  79.   1.   1.]]\n",
      "[[209.   1.   1.   0.]]\n",
      "[[270.   1.   0.   0.]]\n",
      "[[247.   1.   1.   1.]]\n",
      "[[93.  1.  0.  1.]]\n",
      "[[196.   1.   0.   1.]]\n",
      "[[20.  1.  0.  1.]]\n",
      "[[265. 109.   1.   0.]]\n",
      "[[138.   1.   0.   0.]]\n",
      "[[165.   1.   1.   1.]]\n",
      "[[73.  1.  0.  1.]]\n",
      "[[137.   1.   1.   1.]]\n",
      "[[136.  82.   1.   1.]]\n",
      "[[47.  1.  1.  1.]]\n",
      "[[202.   1.   0.   0.]]\n",
      "[[270.   1.   1.   1.]]\n",
      "[[87.  1.  1.  1.]]\n",
      "[[243.   1.   1.   0.]]\n",
      "[[208.   1.   0.   1.]]\n",
      "[[118.   1.   1.   0.]]\n",
      "[[86.  1.  0.  0.]]\n",
      "[[188.   1.   0.   0.]]\n",
      "[[232.   1.   0.   0.]]\n",
      "[[107.   1.   1.   0.]]\n",
      "[[4. 1. 0. 1.]]\n",
      "[[132.   1.   1.   0.]]\n",
      "[[75. 87.  0.  0.]]\n",
      "[[15.  1.  0.  0.]]\n",
      "[[70.  1.  1.  0.]]\n",
      "[[158.   1.   1.   0.]]\n",
      "[[29. 91.  1.  1.]]\n",
      "[[172.   1.   1.   1.]]\n",
      "[[37.  1.  1.  1.]]\n",
      "[[221.   1.   0.   1.]]\n",
      "[[58.  1.  0.  0.]]\n",
      "[[63.  1.  1.  0.]]\n",
      "[[180.   1.   1.   1.]]\n",
      "[[253.   1.   0.   1.]]\n",
      "[[280.   1.   0.   0.]]\n",
      "[[47.  1.  0.  0.]]\n",
      "[[184.   1.   1.   0.]]\n",
      "[[97.  1.  0.  0.]]\n",
      "[[226.   1.   1.   1.]]\n",
      "[[25.  1.  1.  1.]]\n",
      "[[128.   1.   0.   1.]]\n",
      "[[82.  1.  1.  0.]]\n",
      "[[163.   1.   0.   0.]]\n",
      "[[115.   1.   1.   1.]]\n",
      "[[280.   1.   1.   1.]]\n",
      "[[29.  1.  0.  1.]]\n",
      "[[199.   1.   1.   0.]]\n",
      "[[265.   1.   0.   1.]]\n",
      "[[229.   1.   1.   0.]]\n",
      "[[14.  1.  1.  1.]]\n",
      "[[126.   1.   0.   0.]]\n",
      "[[142.   1.   0.   0.]]\n",
      "[[62.  1.  0.  1.]]\n",
      "[[245.   1.   0.   1.]]\n",
      "[[78.  1.  1.  1.]]\n",
      "[[261.   1.   1.   1.]]\n",
      "[[9. 1. 0. 0.]]\n",
      "[[261.   1.   0.   0.]]\n",
      "[[206.   1.   0.   0.]]\n",
      "[[170.   1.   0.   0.]]\n",
      "[[199.   1.   1.   1.]]\n",
      "[[190.  53.   0.   0.]]\n",
      "[[177.   1.   0.   1.]]\n",
      "[[285.   1.   1.   0.]]\n",
      "[[180.  11.   1.   1.]]\n",
      "[[136.  54.   0.   0.]]\n",
      "[[104.   1.   1.   1.]]\n",
      "[[78.  1.  0.  0.]]\n",
      "[[44.  1.  1.  0.]]\n",
      "[[112.   1.   1.   0.]]\n",
      "[[235.   1.   0.   1.]]\n",
      "[[284.   1.   0.   1.]]\n",
      "[[254.   1.   1.   1.]]\n",
      "[[55.  1.  1.  0.]]\n",
      "[[229.   1.   0.   1.]]\n",
      "[[162.   1.   1.   0.]]\n",
      "[[240.   1.   0.   0.]]\n",
      "[[146.   1.   0.   0.]]\n",
      "[[138. 107.   1.   0.]]\n",
      "[[180.   1.   0.   0.]]\n",
      "[[107.   1.   0.   1.]]\n",
      "[[89.  1.  0.  1.]]\n",
      "[[154.   1.   0.   1.]]\n",
      "[[174.   1.   0.   0.]]\n",
      "[[116.   1.   0.   1.]]\n",
      "[[125.   1.   0.   1.]]\n",
      "[[29.  1.  1.  0.]]\n",
      "[[284.  68.   1.   0.]]\n",
      "[[239.   1.   1.   1.]]\n",
      "[[148.   1.   1.   1.]]\n",
      "[[143.   1.   1.   1.]]\n",
      "[[288.   1.   0.   0.]]\n",
      "[[127.   1.   1.   1.]]\n",
      "[[37.  1.  0.  1.]]\n",
      "[[218.   1.   1.   1.]]\n",
      "[[2. 1. 1. 0.]]\n",
      "[[110.   1.   0.   0.]]\n",
      "[[22.  1.  1.  0.]]\n",
      "[[68.  1.  0.  0.]]\n",
      "[[4. 1. 1. 1.]]\n",
      "[[15.  1.  0.  1.]]\n",
      "[[84. 50.  0.  1.]]\n",
      "[[242.   1.   1.   1.]]\n",
      "[[290.   2.   1.   0.]]\n",
      "[[168.   1.   0.   1.]]\n",
      "[[11.  1.  1.  0.]]\n",
      "[[130.   1.   0.   0.]]\n",
      "[[ 82. 109.   1.   0.]]\n",
      "[[211.   1.   0.   0.]]\n",
      "[[51.  1.  0.  1.]]\n",
      "[[73.  1.  1.  1.]]\n",
      "[[21.  1.  1.  1.]]\n",
      "[[8. 1. 0. 1.]]\n",
      "[[191.   1.   0.   1.]]\n",
      "[[96.  1.  1.  1.]]\n",
      "[[101.   1.   1.   0.]]\n",
      "[[270.   1.   1.   0.]]\n",
      "[[266.   1.   1.   1.]]\n",
      "[[257.   1.   1.   1.]]\n",
      "[[233.  87.   1.   0.]]\n",
      "[[203.   1.   1.   0.]]\n",
      "[[223.   1.   1.   1.]]\n",
      "[[250.   1.   0.   1.]]\n",
      "[[42.  1.  0.  0.]]\n",
      "[[156.   1.   1.   1.]]\n",
      "[[214.   1.   1.   0.]]\n",
      "[[89.  1.  1.  0.]]\n",
      "[[175.   1.   1.   1.]]\n",
      "[[139.   1.   1.   0.]]\n",
      "[[153.   1.   1.   0.]]\n",
      "[[140.   1.   0.   1.]]\n",
      "[[73.  1.  0.  0.]]\n",
      "[[197.   1.   0.   0.]]\n",
      "[[196.   1.   1.   1.]]\n",
      "[[171.   1.   1.   0.]]\n",
      "[[ 24. 109.   1.   0.]]\n",
      "[[258.   1.   1.   0.]]\n",
      "[[42.  1.  1.  1.]]\n",
      "[[196.  87.   1.   0.]]\n",
      "[[45.  1.  0.  1.]]\n",
      "[[119.   1.   1.   1.]]\n",
      "[[32.  1.  0.  0.]]\n",
      "[[250.   1.   1.   0.]]\n",
      "[[20.  3.  0.  1.]]\n",
      "[[ 1. 48.  1.  0.]]\n",
      "[[78. 14.  1.  0.]]\n",
      "[[90.  1.  1.  1.]]\n",
      "[[161.   1.   1.   1.]]\n",
      "[[157.   1.   0.   0.]]\n",
      "[[280.  92.   0.   0.]]\n",
      "[[101.   1.   0.   1.]]\n",
      "[[217.   1.   0.   1.]]\n",
      "[[92.  1.  0.  0.]]\n",
      "[[ 6. 90.  0.  0.]]\n",
      "[[225.   1.   1.   0.]]\n",
      "[[54.  1.  1.  1.]]\n",
      "[[25.  1.  0.  1.]]\n",
      "[[186.   1.   0.   1.]]\n",
      "[[83.  1.  1.  1.]]\n",
      "[[277.   1.   1.   1.]]\n",
      "[[53.  1.  0.  0.]]\n",
      "[[133.   1.   1.   1.]]\n",
      "[[118.   1.   0.   0.]]\n",
      "[[273.   1.   0.   0.]]\n",
      "[[279.   1.   1.   0.]]\n",
      "[[58.  1.  0.  1.]]\n",
      "[[66.  1.  0.  1.]]\n",
      "[[254.   1.   0.   0.]]\n",
      "[[184.   1.   1.   1.]]\n",
      "[[219.  49.   0.   0.]]\n",
      "[[6. 1. 0. 0.]]\n",
      "[[286.   1.   1.   1.]]\n",
      "[[112.   1.   0.   1.]]\n",
      "[[64.  1.  1.  1.]]\n",
      "[[21.  1.  0.  0.]]\n",
      "[[33.  1.  1.  0.]]\n",
      "[[204.   1.   0.   1.]]\n",
      "[[235.   1.   1.   1.]]\n",
      "[[283.   1.   1.   0.]]\n",
      "[[110.  45.   1.   1.]]\n",
      "[[236.   1.   0.   0.]]\n",
      "[[154.  96.   0.   0.]]\n",
      "[[288.   4.   1.   1.]]\n",
      "[[300.  73.   0.   0.]]\n",
      "[[229.  16.   0.   0.]]\n",
      "[[246.  58.   0.   1.]]\n",
      "[[113.   9.   1.   1.]]\n",
      "[[300.   4.   0.   0.]]\n",
      "[[51. 82.  0.  0.]]\n",
      "[[39. 42.  0.  1.]]\n",
      "[[154.  62.   1.   0.]]\n",
      "[[115.  89.   0.   0.]]\n",
      "[[211.   6.   0.   1.]]\n",
      "[[241.  35.   0.   0.]]\n",
      "[[198.  37.   1.   0.]]\n",
      "[[63. 39.  0.  0.]]\n",
      "[[266.  56.   0.   0.]]\n",
      "[[177.  94.   1.   0.]]\n",
      "[[289.   1.   1.   0.]]\n",
      "[[158.   8.   1.   0.]]\n",
      "[[247. 109.   1.   1.]]\n",
      "[[76. 68.  1.  0.]]\n",
      "[[94. 91.  1.  1.]]\n",
      "[[243.   7.   1.   0.]]\n",
      "[[214.  96.   1.   1.]]\n",
      "[[122.  66.   0.   1.]]\n",
      "[[222.  33.   1.   1.]]\n",
      "[[173.  52.   1.   1.]]\n",
      "[[131.  36.   0.   1.]]\n",
      "[[19. 41.  0.  1.]]\n",
      "[[190.  70.   1.   0.]]\n",
      "[[300.  36.   0.   1.]]\n",
      "[[280. 109.   1.   1.]]\n",
      "[[235.  68.   0.   0.]]\n",
      "[[290.   1.   0.   1.]]\n",
      "[[62.  5.  0.  0.]]\n",
      "[[92.  7.  1.  1.]]\n",
      "[[84. 33.  1.  0.]]\n",
      "[[39. 25.  1.  0.]]\n",
      "[[18. 76.  0.  1.]]\n",
      "[[194.   1.   0.   0.]]\n",
      "[[123. 109.   1.   1.]]\n",
      "[[181.  34.   0.   1.]]\n",
      "[[122.   1.   1.   0.]]\n",
      "[[76.  1.  0.  1.]]\n",
      "[[290.   2.   0.   0.]]\n",
      "[[96.  1.  1.  0.]]\n",
      "[[39. 73.  1.  1.]]\n",
      "[[70.  1.  0.  1.]]\n",
      "[[282.  49.   0.   1.]]\n",
      "[[1. 6. 1. 0.]]\n",
      "[[278.   1.   0.   1.]]\n",
      "[[1. 4. 0. 0.]]\n",
      "[[ 1. 12.  1.  0.]]\n",
      "[[262.  38.   1.   1.]]\n",
      "[[291.   2.   1.   0.]]\n",
      "[[ 1. 34.  1.  1.]]\n",
      "[[40.  7.  1.  1.]]\n",
      "[[154.  79.   1.   1.]]\n",
      "[[249.  92.   0.   0.]]\n",
      "[[180. 109.   1.   0.]]\n",
      "[[ 68. 109.   0.   1.]]\n",
      "[[288.   1.   0.   1.]]\n",
      "[[ 38. 109.   0.   0.]]\n",
      "[[ 9. 61.  1.  1.]]\n",
      "[[212. 109.   1.   1.]]\n",
      "[[300.  94.   1.   0.]]\n",
      "[[52. 30.  0.  0.]]\n",
      "[[205.  57.   1.   1.]]\n",
      "[[211.   1.   1.   1.]]\n",
      "[[98. 56.  0.  0.]]\n",
      "[[250.  73.   1.   1.]]\n",
      "[[117.  30.   1.   1.]]\n",
      "[[277.   1.   0.   0.]]\n",
      "[[129.   5.   1.   1.]]\n",
      "[[146.  31.   0.   0.]]\n",
      "[[193.   5.   0.   0.]]\n",
      "[[6. 1. 1. 1.]]\n",
      "[[267.   1.   0.   0.]]\n",
      "[[135.   1.   1.   0.]]\n",
      "[[234.  49.   1.   1.]]\n",
      "[[60. 94.  1.  1.]]\n",
      "[[37.  1.  0.  0.]]\n",
      "[[213.   1.   0.   1.]]\n",
      "[[88. 76.  0.  1.]]\n",
      "[[163.   1.   0.   1.]]\n",
      "[[63. 74.  0.  1.]]\n",
      "[[97. 39.  1.  0.]]\n",
      "[[284.   1.   1.   1.]]\n",
      "[[224.   1.   0.   1.]]\n",
      "[[169.   1.   1.   1.]]\n",
      "[[69. 53.  1.  0.]]\n",
      "[[222.   1.   0.   0.]]\n",
      "[[1. 7. 1. 1.]]\n",
      "[[19. 27.  1.  0.]]\n",
      "[[279.  34.   0.   0.]]\n",
      "[[270.  71.   0.   1.]]\n",
      "[[139.  68.   0.   0.]]\n",
      "[[ 95. 109.   1.   0.]]\n",
      "[[122.  51.   1.   0.]]\n",
      "[[151. 109.   1.   0.]]\n",
      "[[168.  26.   0.   0.]]\n",
      "[[130.  95.   0.   0.]]\n",
      "[[70. 27.  0.  1.]]\n",
      "[[44. 94.  1.  1.]]\n",
      "[[219.   1.   1.   0.]]\n",
      "[[1. 1. 1. 0.]]\n",
      "[[81.  1.  1.  0.]]\n",
      "[[52.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[70.  1.  1.  1.]]\n",
      "[[4. 1. 1. 0.]]\n",
      "[[290.   1.   0.   0.]]\n",
      "[[147.  46.   1.   1.]]\n",
      "[[41. 57.  0.  1.]]\n",
      "[[149.   1.   1.   0.]]\n",
      "[[256.   1.   0.   1.]]\n",
      "[[209.  31.   0.   1.]]\n",
      "[[207.   1.   1.   1.]]\n",
      "[[187.   1.   1.   0.]]\n",
      "[[160.   1.   0.   0.]]\n",
      "[[123.   1.   0.   0.]]\n",
      "[[251.  46.   0.   0.]]\n",
      "[[136.   1.   0.   1.]]\n",
      "[[265.  95.   1.   1.]]\n",
      "[[18.  1.  0.  0.]]\n",
      "[[220.  83.   0.   0.]]\n",
      "[[28.  1.  1.  1.]]\n",
      "[[234.   1.   1.   0.]]\n",
      "[[223.  62.   1.   1.]]\n",
      "[[182.  81.   0.   1.]]\n",
      "[[130.   1.   1.   1.]]\n",
      "[[ 14. 101.   0.   1.]]\n",
      "[[185.   1.   0.   0.]]\n",
      "[[48.  1.  0.  1.]]\n",
      "[[165.  87.   1.   0.]]\n",
      "[[149.   1.   0.   1.]]\n",
      "[[285.  81.   0.   1.]]\n",
      "[[181.   1.   1.   0.]]\n",
      "[[190.   1.   1.   0.]]\n",
      "[[100.   1.   0.   0.]]\n",
      "[[121.  78.   1.   1.]]\n",
      "[[273.   1.   1.   1.]]\n",
      "[[85.  1.  0.  1.]]\n",
      "[[193.   1.   1.   1.]]\n",
      "[[174.   1.   1.   0.]]\n",
      "[[178.   1.   1.   1.]]\n",
      "[[201.   1.   1.   1.]]\n",
      "[[39.  1.  0.  1.]]\n",
      "[[246.   1.   0.   0.]]\n",
      "[[101.   1.   1.   1.]]\n",
      "[[10.  1.  1.  1.]]\n",
      "[[228.   1.   0.   0.]]\n",
      "[[264.   1.   0.   0.]]\n",
      "[[257.   1.   0.   0.]]\n",
      "[[23.  1.  0.  1.]]\n",
      "[[131.   1.   0.   1.]]\n",
      "[[61.  1.  1.  1.]]\n",
      "[[123.   1.   1.   1.]]\n",
      "[[66.  1.  1.  0.]]\n",
      "[[112.   1.   1.   1.]]\n",
      "[[202.  76.   0.   1.]]\n",
      "[[243.   1.   0.   0.]]\n",
      "[[218.   1.   0.   0.]]\n",
      "[[285.   1.   0.   0.]]\n",
      "[[134.   1.   0.   0.]]\n",
      "[[18.  1.  1.  1.]]\n",
      "[[181.   1.   0.   1.]]\n",
      "[[182.   1.   0.   0.]]\n",
      "[[33.  1.  0.  1.]]\n",
      "[[262.   1.   1.   0.]]\n",
      "[[216.   1.   1.   0.]]\n",
      "[[196.   1.   1.   0.]]\n",
      "[[52. 46.  1.  1.]]\n",
      "[[85.  1.  1.  0.]]\n",
      "[[179.  63.   0.   0.]]\n",
      "[[48.  1.  1.  0.]]\n",
      "[[200.   1.   0.   1.]]\n",
      "[[128.   1.   1.   0.]]\n",
      "[[111.  60.   1.   1.]]\n",
      "[[ 1. 63.  0.  0.]]\n",
      "[[96.  1.  0.  1.]]\n",
      "[[3. 1. 0. 0.]]\n",
      "[[252.   1.   1.   0.]]\n",
      "[[61.  1.  0.  0.]]\n",
      "[[237.  99.   0.   1.]]\n",
      "[[262.   1.   0.   1.]]\n",
      "[[165.   1.   1.   0.]]\n",
      "[[231.   1.   1.   0.]]\n",
      "[[83.  1.  0.  0.]]\n",
      "[[264.   1.   1.   1.]]\n",
      "[[246.   1.   1.   0.]]\n",
      "[[19.  1.  1.  0.]]\n",
      "[[28.  1.  0.  0.]]\n",
      "[[251.   1.   1.   1.]]\n",
      "[[28. 49.  1.  1.]]\n",
      "[[104.   1.   0.   1.]]\n",
      "[[106.   1.   0.   0.]]\n",
      "[[29. 34.  1.  0.]]\n",
      "[[44.  1.  0.  0.]]\n",
      "[[272.  44.   0.   0.]]\n",
      "[[81. 97.  1.  1.]]\n",
      "[[251.   1.   0.   0.]]\n",
      "[[190.  98.   0.   1.]]\n",
      "[[26.  1.  1.  0.]]\n",
      "[[143.   1.   1.   0.]]\n",
      "[[190.  26.   1.   0.]]\n",
      "[[153.   1.   0.   0.]]\n",
      "[[166.   1.   0.   0.]]\n",
      "[[94.  1.  0.  0.]]\n",
      "[[238.   1.   0.   1.]]\n",
      "[[59.  1.  1.  0.]]\n",
      "[[206.   1.   1.   0.]]\n",
      "[[247.   1.   0.   1.]]\n",
      "[[75.  1.  1.  1.]]\n",
      "[[88. 63.  0.  1.]]\n",
      "[[191.   1.   0.   0.]]\n",
      "[[269.   1.   0.   1.]]\n",
      "[[140.   1.   1.   1.]]\n",
      "[[44.  1.  1.  1.]]\n",
      "[[105.  98.   1.   1.]]\n",
      "[[10. 33.  1.  0.]]\n",
      "[[212.   1.   1.   0.]]\n",
      "[[79.  1.  0.  1.]]\n",
      "[[108.   1.   0.   0.]]\n",
      "[[151.   1.   0.   1.]]\n",
      "[[36.  1.  1.  0.]]\n",
      "[[254.  27.   0.   0.]]\n",
      "[[ 1. 98.  1.  0.]]\n",
      "[[3. 6. 1. 0.]]\n",
      "[[227.   4.   1.   1.]]\n",
      "[[234.  25.   0.   1.]]\n",
      "[[104.  83.   0.   0.]]\n",
      "[[116.   1.   1.   0.]]\n",
      "[[143.  93.   1.   1.]]\n",
      "[[230.   1.   0.   0.]]\n",
      "[[272.   1.   0.   1.]]\n",
      "[[258.  64.   1.   0.]]\n",
      "[[290.  41.   0.   0.]]\n",
      "[[158.   1.   1.   1.]]\n",
      "[[50.  1.  1.  1.]]\n",
      "[[300.  25.   1.   0.]]\n",
      "[[161.  53.   0.   0.]]\n",
      "[[241.   1.   1.   0.]]\n",
      "[[171.   4.   0.   0.]]\n",
      "[[146.   4.   0.   1.]]\n",
      "[[134.  24.   1.   0.]]\n",
      "[[290. 101.   0.   0.]]\n",
      "[[268.   1.   1.   0.]]\n",
      "[[203.  98.   0.   0.]]\n",
      "[[75. 40.  1.  1.]]\n",
      "[[232.   1.   0.   1.]]\n",
      "[[155.   1.   1.   0.]]\n",
      "[[145.   1.   1.   1.]]\n",
      "[[13.  1.  0.  0.]]\n",
      "[[14.  1.  1.  0.]]\n",
      "[[30. 80.  0.  0.]]\n",
      "[[110.   1.   0.   1.]]\n",
      "[[219.  23.   1.   0.]]\n",
      "[[16.  1.  1.  1.]]\n",
      "[[174.   1.   0.   1.]]\n",
      "[[155.  24.   1.   1.]]\n",
      "[[147.   1.   0.   1.]]\n",
      "[[177.   1.   0.   0.]]\n",
      "[[207.  45.   0.   0.]]\n",
      "[[16. 52.  0.  0.]]\n",
      "[[151.   1.   1.   0.]]\n",
      "[[204.   1.   0.   0.]]\n",
      "[[225.  97.   0.   0.]]\n",
      "[[ 11. 109.   1.   0.]]\n",
      "[[222.   1.   1.   0.]]\n",
      "[[99.  1.  0.  1.]]\n",
      "[[168.   1.   0.   0.]]\n",
      "[[73.  1.  1.  0.]]\n",
      "[[193.   1.   0.   1.]]\n",
      "[[191.   1.   1.   1.]]\n",
      "[[57.  1.  1.  1.]]\n",
      "[[35.  1.  1.  1.]]\n",
      "[[206.   1.   0.   1.]]\n",
      "[[186.   1.   1.   1.]]\n",
      "[[155.   1.   0.   0.]]\n",
      "[[291.  60.   0.   1.]]\n",
      "[[104.   1.   1.   0.]]\n",
      "[[189.   1.   0.   1.]]\n",
      "[[18. 88.  1.  0.]]\n",
      "[[89.  1.  0.  0.]]\n",
      "[[160.   1.   1.   0.]]\n",
      "[[171.  40.   0.   0.]]\n",
      "[[220.   1.   1.   1.]]\n",
      "[[29. 18.  1.  0.]]\n",
      "[[239.   1.   1.   0.]]\n",
      "[[17.  1.  0.  1.]]\n",
      "[[51. 70.  0.  1.]]\n",
      "[[107.  34.   0.   1.]]\n",
      "[[230.   1.   1.   1.]]\n",
      "[[165.   1.   0.   1.]]\n",
      "[[41.  1.  1.  0.]]\n",
      "[[71.  1.  0.  0.]]\n",
      "[[245.   1.   1.   1.]]\n",
      "[[120.   1.   0.   0.]]\n",
      "[[56.  1.  0.  0.]]\n",
      "[[274.  82.   1.   0.]]\n",
      "[[87.  1.  0.  1.]]\n",
      "[[267.  28.   0.   0.]]\n",
      "[[93.  1.  1.  1.]]\n",
      "[[113.   1.   0.   0.]]\n",
      "[[142.   1.   0.   1.]]\n",
      "[[227.  75.   0.   1.]]\n",
      "[[255. 102.   1.   1.]]\n",
      "[[210.   1.   0.   1.]]\n",
      "[[86. 23.  0.  1.]]\n",
      "[[114.   1.   1.   0.]]\n",
      "[[161.   1.   0.   1.]]\n",
      "[[11.  1.  0.  0.]]\n",
      "[[242.  80.   0.   0.]]\n",
      "[[80.  1.  1.  1.]]\n",
      "[[1. 1. 0. 1.]]\n",
      "[[282.   1.   0.   0.]]\n",
      "[[167.  99.   0.   1.]]\n",
      "[[81.  1.  0.  0.]]\n",
      "[[166.  63.   1.   1.]]\n",
      "[[2. 1. 0. 1.]]\n",
      "[[209.   1.   1.   1.]]\n",
      "[[118. 100.   1.   0.]]\n",
      "[[28.  4.  1.  0.]]\n",
      "[[ 9. 71.  1.  0.]]\n",
      "[[201.   1.   1.   0.]]\n",
      "[[120.  39.   0.   0.]]\n",
      "[[ 91. 100.   0.   0.]]\n",
      "[[30.  1.  0.  0.]]\n",
      "[[66. 63.  0.  1.]]\n",
      "[[144.   1.   0.   0.]]\n",
      "[[70. 99.  0.  0.]]\n",
      "[[260.   1.   1.   0.]]\n",
      "[[292.  88.   1.   0.]]\n",
      "[[121.   1.   1.   1.]]\n",
      "[[79.  1.  1.  0.]]\n",
      "[[40.  1.  0.  0.]]\n",
      "[[157.   1.   0.   1.]]\n",
      "[[202.  65.   0.   0.]]\n",
      "[[ 31. 100.   0.   0.]]\n",
      "[[ 1. 32.  1.  0.]]\n",
      "[[130.   1.   1.   0.]]\n",
      "[[135.   1.   1.   1.]]\n",
      "[[242.   1.   0.   1.]]\n",
      "[[179.   1.   0.   1.]]\n",
      "[[167.   1.   1.   1.]]\n",
      "[[60.  1.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "## define the domain of the considered parameters\n",
    "n_estimators = tuple(np.arange(1,301,1, dtype= np.int))\n",
    "# print(n_estimators)\n",
    "max_depth = tuple(np.arange(1,110,1, dtype= np.int))\n",
    "# max_features = ('log2', 'sqrt', None)\n",
    "max_features = (0, 1)\n",
    "# criterion = ('gini', 'entropy')\n",
    "criterion = (0, 1)\n",
    "\n",
    "\n",
    "# define the dictionary for GPyOpt\n",
    "domain = [{'n_estimators': 'var_1',  'type': 'discrete',     'domain': n_estimators},\n",
    "          {'max_depth': 'var_2',     'type': 'discrete',     'domain': max_depth},\n",
    "          {'max_features': 'var_3',  'type': 'categorical',  'domain': max_features},\n",
    "          {'criterion': 'var_4',     'type': 'categorical',  'domain': criterion}]\n",
    "\n",
    "\n",
    "## we have to define the function we want to maximize --> validation accuracy, \n",
    "## note it should take a 2D ndarray but it is ok that it assumes only one point\n",
    "## in this setting\n",
    "def objective_function(x): \n",
    "    print(x)\n",
    "    # we have to handle the categorical variables that is convert 0/1 to labels\n",
    "    # log2/sqrt and gini/entropy\n",
    "    \n",
    "    param = x[0]\n",
    "    \n",
    "    if param[2] == 0:\n",
    "        var_3 = \"log2\"\n",
    "    else:\n",
    "        var_3 = \"sqrt\"\n",
    "    \n",
    "    if param[3] == 0:\n",
    "        var_4 = \"gini\"\n",
    "    else:\n",
    "        var_4 = \"entropy\"\n",
    "        \n",
    "        \n",
    "#fit the model\n",
    "    model = RandomForestClassifier(n_estimators = int(param[0]), criterion = var_4, max_depth = int(param[1]), max_features = var_3)\n",
    "    model.fit(Xcattrain, ytrain)\n",
    "    forestPreds = model.predict(Xcattest)\n",
    "    accuracy = len(forestPreds[torch.tensor(forestPreds, dtype = torch.int64) == ytest]) / len(forestPreds)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "opt = GPyOpt.methods.BayesianOptimization(f = objective_function,   # function to optimize\n",
    "                                              domain = domain,         # box-constrains of the problem\n",
    "                                              acquisition_type = \"EI\",      # Select acquisition function MPI, EI, LCB\n",
    "                                             )\n",
    "opt.acquisition.exploration_weight=.1\n",
    "\n",
    "opt.run_optimization(max_iter = 100) \n",
    "\n",
    "\n",
    "x_best = opt.X[np.argmin(opt.Y)]\n",
    "print(\"The best parameters obtained: n_estimators=\" + str(x_best[0]) + \", max_depth=\" + str(x_best[1]) + \", max_features=\" + str(\n",
    "    x_best[2])  + \", criterion=\" + str(\n",
    "    x_best[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
